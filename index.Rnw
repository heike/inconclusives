\documentclass[doubleblind]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\usepackage{amsmath,amsthm}
\usepackage{amstext}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{float}
% \usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{csquotes}% display/block quotes
\usepackage{fullpage}

\usepackage[numbers]{natbib}

\newenvironment{definition}[1]% environment name 
{% begin code 
  \par\vspace{.75\baselineskip}\noindent 
  \textbf{Definition (#1)}\begin{itshape}% 
  \par\vspace{.5\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\journal{Forensic Science International}
%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
% \bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
% \bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num-names}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\noindent

\begin{frontmatter}

\title{Treatment of Inconclusives in the AFTE Range of Identifications\tnoteref{t1}}

\tnotetext[t1]{This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement \#70NANB15H176 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, University of California Irvine, and University of Virginia.}


%% Group authors per affiliation:
% \author{Elsevier\fnref{myfootnote}}
% \address{Radarweg 29, Amsterdam}
% \fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
% \author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
% \ead[url]{www.elsevier.com}
% 
% \author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
% \cortext[mycorrespondingauthor]{Corresponding author}
% \ead{support@elsevier.com}
% 
% \address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
% \address[mysecondaryaddress]{360 Park Avenue South, New York}

\author[isu,csafe]{Heike Hofmann}
\author[isu,csafe]{Alicia Carriquiry Hofmann}
\cortext[corauthor]{Corresponding author}\ead{srvander@iastate.edu}
\author[unl]{Susan Vanderplas\corref{corauthor}}
\address[isu]{Statistics Department, Iowa State University\\2438 Osborne Dr, Ames, IA 50011}
\address[csafe]{Center for Statistical Applications in Forensic Evidence, Iowa State University\\613 Morrill Rd, Ames, IA 50011}
\address[unl]{University of Nebraska-Lincoln}   

\begin{abstract}
The treatment of inconclusives in the assessment of errors has been a long-standing issue with far-reaching implications in the legal system. 
\end{abstract}

\begin{keyword}
forensic science, black box studies, proficeincy testing
\end{keyword}

\end{frontmatter}



\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\svp}[1]{{\textcolor{teal}{#1}}}
\noindent


<<echo = FALSE, message = FALSE, warning = FALSE>>=
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  dpi = 600,
  fig.align = "center",
  out.width = "\\textwidth",
  cache = TRUE,
  fig.path = "figures/",
  echo = FALSE
)
options(knitr.table.format = "latex")

library(stringr)

library(tidyverse)
library(scales)
library(multidplyr) # install_github("hadley/multidplyr")
library(bulletxtrctr) # install_github("csafe-isu/bulletxtrctr")
library(gridExtra)
library(kableExtra)
@


Examiners visually classify similarity of toolmark and firearm evidence according to the AFTE theory of identification \citep{identification} as one of identification, inconclusive or exclusion. Exact guidelines for this classification vary from lab to lab; some labs will exclude only on the basis of non-matching class characteristics, such as direction of the twist in rifling, land length or number of lands, or type of rifling. In other labs, CMS (consecutively matching striae) as defined by \citeauthor{biasotti} \citep{biasotti} is used as a measure to quantify the similarity of two lands. In virtually all labs, individual characteristics used to identify matching bullets are derived from visual assessment; some class characteristics may be directly measured, but these are not sufficient for individualization. 
% Identification using 3D Scanning Technology


\begin{table}[hbt]
\noindent\fbox{%
    \parbox{\textwidth}{%
\begin{enumerate}
\item Identification\hfill\newline
Agreement of a combination of individual characteristics and all discernible class characteristics where the extent of agreement exceeds that which can occur in the comparison of toolmarks made by different tools and is consistent with the agreement demonstrated by toolmarks known to have been produced by the same tool.


\item Inconclusive 
\begin{enumerate}
\item Some agreement of individual characteristics and all discernible class characteristics, but insufficient for an identification.
\item Agreement of all discernible class characteristics without agreement or disagreement of individual characteristics due to an absence, insufficiency, or lack of reproducibility.
\item Agreement of all discernible class characteristics and disagreement of individual characteristics, but insufficient for an elimination.
\end{enumerate}

\item Elimination \hfill\newline
Significant disagreement of discernible class characteristics and/or individual characteristics.

\item Unsuitable \hfill\newline
Unsuitable for examination.
\end{enumerate}
    }%
}
\caption{\label{tab:afte} AFTE Rules of Toolmark Identifications \citep{identification}.}
\end{table}
\citet{Dror:2018fp} have started the discussion to treat inconclusive results as an examiner's conscious decision to not decide on an identification or elimination. 
This view has drawn some criticism \citep{Biedermann:2018hr}.

In this paper we want to highlight the treatment of inconclusives in the assessment of error rates from a statistical point of view. % We believe that this -- rather than further dividing the different opinions -- provides a unifying framework.

% \section{Background}

The identification process -- i.e. the assessment of whether two samples come from the same source (were made by the same tool, the same shoe, the same finger, shot through the same barrel) or from different sources -- is quite complex. As a result, studies which attempt to gain insight into the identification process must also navigate these complexities, ideally without becoming too complicated to reasonably execute. 

There are several considerations when designing a study of the identification process:
\begin{description}
\item [Closed vs. Open Set] A closed set study is one in which all unknown samples come from the same source as a known which is also included in the test set. Conversely, an open set study is one in which unknown samples may originate from sources which are not part of the known samples provided in the test set.
\item [White box vs. Black Box] A white-box study is one which makes an attempt to understand \emph{why} an examiner came to the decision, in contrast to a black box study, which only evaluates the correctness of the decision (and not the reasoning behind it). 
\item [Blind testing] A blind (or blinded) study is one in which the participant (in this case the examiner) does not know that they are being tested; that is, a study which appears to be part of a case, rather than research. \citet{pcast2016} recommended that the use of blind testing be increased, as error rates derived from blind studies better generalize to casework.
\item [Number of knowns from different sources] Some studies \citep{keisler} provide only one known exemplar (or a set of multiple exemplars from the same source). Other studies, such as \citep{hamby,Hamby:2018hu} and the Houston FSC and Phoenix studies from \citet{case-validation}, include multiple different sources as knowns. %Typically, these studies can be completed with the assumption that once an unknown matches one known, it will not match any other knowns. \svp{Add in the issues with this later}
\item [Study length] Most crime labs are understaffed and maintain a fairly large backlog of cases; as a consequence, examiner time is limited. While examiners should participate in these studies, because they benefit the discipline as a whole, they must balance the competing demands of a large and consequential workload. As such, studies which require examiners to make a large number of comparisons may be less likely to have sufficient participants to generalize to the wider discipline.
\end{description}



In order to assess how well this process is functioning, studies with known ground truth have to be employed because casework does not allow for an assessment of correctness. 

There are different kinds of studies: closed set studies, open set studies, blind testing. 

Ground truth and error rates: 

Ground truth is the knowledge of whether two samples come from the same source or come in fact from two different sources. This knowledge is only available to the body setting up the test.   We distinguish between known matches (same source samples) and known non-matches (samples from different sources).
In particular, ground truth in case work is not known.

Once a study is run and test samples have been assessed by forensic examiners, error rates can be calculated. Differences between reported results and ground truth are considered to be errors in the identification process. A detailed discussion on the the exact procedure on how to deal with inconclusive results in calculating error rates is provided in section XXX.

The {\bf true positive rate}, also known as {\bf sensitivity}, is the probability that two samples from the same source are identified in the examination process as a match, $P(\text{identification mde} \mid \text{same source samples})$. 

The {\bf true negative rate}, also known as {\bf specificity}, is the probability that two samples from different sources result in an elimination in the examination process, $P(\text{elimination mde} \mid \text{different sources samples})$. 





\section{Error Rate Calculation}

National Research Council 2009:

\begin{displayquote} much forensic evidence—including, for example, bitemarks and firearm and toolmark identifications—is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline.\end{displayquote}

PCAST (2015) identified two important gaps:

\begin{displayquote} (1) the need for clarity on the scientific meaning of "reliable principles and methods" and "scientific validity" in the context of certain forensic disciplines, and (2) the need to evaluate specific forensic methods to determine whether they have been scientifically established to be valid and reliable.\end{displayquote}

PCAST Report (2016):

\begin{displayquote} In an online experiment, researchers asked mock jurors to estimate the frequency that a qualified, experienced forensic scientist would mistakenly conclude that two samples of specified types came from the same person when they actually came from two different people. The mock jurors believed such errors are likely to occur about 1 in 5.5 million for fingerprint analysis comparison; 1 in 1 million for bitemark comparison; 1 in 1 million for hair comparison; and 1 in 100 thousand for handwriting comparison.\end{displayquote}




\hh{In the absence of knowing the truth we have to resort to working within a probabilistic framework to assess the likelihood of certain events. Some (XXX? quantify?) evidence is based on questions of pattern matching: does the shoe print from the crime scene match a particular shoe? was the bullet found at a crime scene fired from a particular weapon? does the breech face impression on a cartridge case from the crime scene match the breech face of a particular weapon frame?}

\hh{
In these situations the fundamental quantity in question is the probability that two pieces of evidence come from the same source or from different sources. In a legal setting the opinion provided by the expert guides this assessment. So given expert testimony, how does or should the evaluation of the above probability change?  }

The AFTE theory of firearms and toolmarks identification as shown in \autoref{tab:afte} allows an expert to come to three main conclusions: make an identification, i.e. two pieces of evidence are thought to come from the same source, make an elimination, i.e. two pieces of evidence are tought to come from different sources, or an inconclusive, i.e. the evidence does not allow either an identification or an elimination. 

Given these three outcomes, we are interested for each the probability of same source and different sources, resulting in a total of six ($3 \times 2$) conditional probabilities. In a perfect world, we would expect the following probabilities:

\begin{eqnarray*}
\arraycolsep=1.4pt\def\arraystretch{1.1}
\begin{array}{rcr}
P (\text{same source} \mid \text{identification made}) &=&  1 \\
P (\text{different source} \mid \text{identification made}) &=&  0 \\[10pt]
P (\text{same source} \mid \text{inconclusive}) &=& 0.5 \\
P (\text{different source} \mid \text{inconclusive}) &=& 0.5 \\[10pt]
P (\text{same source} \mid \text{elimination made}) &=& 0\\
P (\text{different source} \mid \text{elimination made}) &=&  1 \\
\end{array}
\end{eqnarray*}



%The probability to make an identification  could be either reported directly or calculated as a composite of true positive and false positive rates:

% \[
% P(\text{identification made}) = TPR \times P(\text{same source}) + FPR \times P(\text{different source}) 
% \]

In the presence of studies, we can use existing data to evaluate the conditional probability of same source versus different sources given an expert's testimony directly.



\section{Studies}

Baldwin , Keisler , Lyons \cite{lyons}, Hare, Fadul \cite{fadul}, Brundage-Hamby, Duez \cite{Duez:2017kha}, Chumbley \cite{Macziewski:2016jw}

confidence interval used in Baldwin: Pearson-Clopper, applied to Keisler sensitivity and specificity:

<<r functions, echo=FALSE>>=
clopper <- function(alpha, success, trials) {
  lower <- qbeta(alpha/2, success, trials-success+1)
  upper <- qbeta(1-alpha/2, success+1, trials-success)
  c(lower, upper)
}

# Keisler sensitivity
clopper(0.05, 1508, 1512)
# Keisler specificity
clopper(0.05, 805, 1008)
@

\subsection{Baldwin Study \cite{Baldwin:2014bb}}

Each kit consists of 15 sets of 3 known cartridge cases and 1 questioned. For all participants 5 of the sets were from same-source and 10 of the sets were from different sources. {\bf ``We instructed all participants to refrain from sharing or discussing the contents or results of their sample sets and answers to minimize the risk of revealing the design".} \hh{this is a weakness of the design,  but it also fixes the frequencies of same source and different source occurrences and with it the probabilities for the events:}

A total of 25 firearms was used for the study, such that within each kit no firearm was re-used for either knowns or questioned cartridge cases, i.e. no additional information could be gained by comparing any cartridge cases across sets.

\[
P(\text{same source}) = \frac{1}{3},  \ \ \ P(\text{different source}) = \frac{2}{3}
\]

218 participants provided evaluations of the kits: \hfill\newline

\begin{tabular}{lrrrr}\hline
& \# Pairs & Elimination & \multirow{2}{1.5in}{Inconclusives or no response} & Identification\\
\\ \hline
Known Matches & 1090 & 4 & 11 & 1075\\
Known Non-Matches & 2180 & 1421 & 735+2 & 22\\ \hline
\end{tabular}\hfill\newline


Each of the above evaluations is actually based on a comparison on three known same source  cartridge cases to one questioned.

For 3234 comparisons, FTEs evaluated how many of the known cartridge cases were usable for an evaluation: all three specimens were used in 3018 cases, two were used in 207 cases, and only one was used in nine cases.


The set of conditional probabilities is calculated as:

\begin{eqnarray*}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{rcccr}
P (\text{same source} \mid \text{identification made}) &=&  \frac{1075}{1075+ 22} &=& 0.9799 \\
P (\text{different source} \mid \text{identification made}) &=&  \frac{22}{1075+ 22} &=& 0.0201 \\[10pt]
P (\text{same source} \mid \text{inconclusive}) &=& \frac{11}{737+11} &=& 0.0147 \\
P (\text{different source} \mid \text{inconclusive}) &=& \frac{737}{737+11} &=& 0.9853 \\[10pt]
P (\text{same source} \mid \text{elimination made}) &=& \frac{4}{1421+4} &=& 0.0028 \\
P (\text{different source} \mid \text{elimination made}) &=&  \frac{1421}{1421+4} &=& 0.9972 \\
\end{array}
\end{eqnarray*}


It seems that firearms examiners are more willing to bet on identifications than on exclusions. In fact, even if inconclusives and eliminations are combined, the error rate of making a false elimination is 0.0069, only a third of the error of a false identification:

\begin{eqnarray*}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{rcccr}
P (\text{same source} \mid \text{inconclusive or elimination}) &=& \frac{11+4}{748+1425} &=& 0.0069 \\
P (\text{different source} \mid \text{inconclusive or elimination}) &=& \frac{737+1421}{748+1425} &=& 0.9931 
\end{array}
\end{eqnarray*}



\subsection{Keisler \cite{keisler}}

Each test kit consisted of sets of 20 pairs of cartridge cases,  12 of the pairs were from the same source and 8 pairs were from different sources.

126 participants reported results:

\begin{tabular}{lrrrr}\hline
& \# Pairs & Elimination & \multirow{2}{1.5in}{Inconclusives} & Identification\\
\\ \hline
Known Matches & 1512 & 0 & 4 & 1508\\
Known Non-Matches & 1008 & 805 & 203 & 0\\ \hline
\end{tabular}\hfill\newline

The set of conditional probabilities is calculated as:

\begin{eqnarray*}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{rcccr}
P (\text{same source} \mid \text{identification made}) &=&  \frac{1508}{1508} &=& 1 \\
P (\text{different source} \mid \text{identification made}) &=&  \frac{0}{1508} &=& 0 \\[10pt]
P (\text{same source} \mid \text{inconclusive}) &=& \frac{4}{203+4} &=& 0.0193 \\
P (\text{different source} \mid \text{inconclusive}) &=& \frac{203}{203+4} &=& 0.9807 \\[10pt]
P (\text{same source} \mid \text{elimination made}) &=& \frac{0}{205} &=& 0 \\
P (\text{different source} \mid \text{elimination made}) &=&  \frac{205}{205} &=& 1 \\
\end{array}
\end{eqnarray*}


\subsection{Brundage-Hamby  \citet{Hamby:2018hu}}

\begin{tabular}{lrrrr}
Test Series	& \# Examiners Participating in Test &	\# Examiners Reporting Inconclusives	& \#Inconclusively Identified Bullets	& \#Incorrectly Identified Bullets \\
Brundage &	67 &	1 &	1 &	0\\
Hamby &	630 &	4 &	7 &	0\\
Totals: &	697 &	5 &	8 &	0\\
\end{tabular}


\section{Discussion and Conclusions}

number of firearms very small 

\section{Other Thoughts/Interesting finds}
\begin{itemize}
\item Full pipeline errors
  \begin{itemize}
    \item \svp{\citet{edmondGuideInterpretingForensic2014} is a fingerprint study that discusses the potential for errors at any stage in the process and the limitations of error rate studies as they are currently performed.}
    \item \svp{\citet{hozoDecisionMakingWhenData2008} calls for what seems to be full pipeline analysis of errors, with included components tailored to the goals of the research. Focused primarily on clinical trials, so a good portion of the paper doesn't apply...}
  \end{itemize}
  
\item \svp{It may be worth comparing/contrasting AFTE guidelines to SWGTREAD and other disciplines (SWGTREAD has a few extra categories that make it interesting \citep{swgtread-conclusions-2013}) and Uniform Language Testimony and Reporting guidelines - e.g. \citet{ULTR-latent} and \citet{ULTR-firearms-pattern} (all of the ULTR guidelines are at \url{https://www.justice.gov/olp/uniform-language-testimony-and-reports}).}
\item \svp{Inconclusives also show up in diagnostic medicine (x-rays, mammograms, scans). Is the statistical treatment there any different given that typically there are better defined populations and some available statistical information on those populations? It's also worth noting that you can recall the patient and do additional (informative) testing with people; with forensic evidence, you can't get better data than what you had initially. }

\item \svp{From my read of \citet{tukeyConclusionsVsDecisions}, it seems to be an argument for inconclusives (e.g. doing nothing). It may be worth differentiating the error rate calculation focused on the examiner's decision from action taken by the legal system, which is a full-pipeline thing that includes systematic errors during evidence recording, collection, etc. (things the examiner doesn't control). What defendents care about is the full-scope conclusion, which includes consideration of all of the errors earlier in the pipeline. }
\end{itemize}
<<output-all-figs, eval = F, include = F>>=
thisdoc <- readLines("index.Rnw")

# First, get figures that aren't in code chunks
# Haven't really tested this part - regex may need work
raw_figures <- tibble(
  line_start = which(str_detect(thisdoc, "\\begin\\{figure\\}")), 
  line_end = which(str_detect(thisdoc, "\\end\\{figure\\}")),
  subfloats = purrr::map2(line_start, line_end, 
                          ~str_extract(thisdoc[.x:.y], "\\includegraphics(?:\\[.*\\])?\\{(.*)\\}") %>%
                            str_replace("\\includegraphics(?:\\[.*\\])?\\{(.*)\\}", "\\1"))
)

# Code chunk figures
chunk_figures <- tibble(
  line_start = which(str_detect(thisdoc, "^<<.*>>=$")),
  line_end = which(str_detect(thisdoc, "^@$")),
  chunk_name = str_replace(thisdoc[line_start], "<<([0-9A-z-]*?),?((?:.*=.*){1,})>>=", "\\1")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, chunk_name, ~thisdoc[.x] %>%
                              str_extract("fig\\.subcap ?= ?.*?\\),") %>%
                              str_replace("fig\\.subcap ?= ?", "") %>%
                              str_replace(",$", "") %>%
                              parse(text=.) %>% eval %>% length() %>% 
                              seq(1, ., by = 1) %>%
                              sprintf("figures/%s-%d.pdf", .y, .))
  )

# Knitr included graphics figures
chunk_incl_figures <- tibble(
  line_start = which(str_detect(thisdoc, "^<<.*>>=$")),
  line_end = which(str_detect(thisdoc, "^@$")),
  chunk_name = str_replace(thisdoc[line_start], "<<([0-9A-z-]*?),?((?:.*=.*){1,})>>=", "\\1")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, line_end, function(.x, .y) {
      thisdoc[.x:.y] %>%
        paste(collapse = " ") %>%
        str_extract(., "include_graphics\\(c?\\(?.*?\\)?\\)") %>%
        str_replace("include_graphics\\(", "") %>%
        str_extract_all('\\"[\\.A-z0-9 /-]*?\\"') %>%
        parse(text = .) %>% eval() %>%
        str_remove_all("\\\\|\"")
    })
  )

# All figures
figs <- bind_rows(raw_figures, chunk_figures, chunk_incl_figures) %>%
  arrange(line_start) %>%
  mutate(subfloats = purrr::map(subfloats, function(x) data_frame(file = x, exists = file.exists(x)))) %>%
  tidyr::unnest() %>%
  filter(exists) %>%
  mutate(fig_num = group_indices(., line_start, line_end)) %>%
  group_by(fig_num) %>%
  mutate(rn = row_number(),
         maxrn = n(),
         fig_label = letters[rn],
         fig_label = ifelse(maxrn > 1, fig_label, ""),
         fileext = tools::file_ext(file),
         fig_label = sprintf("Figure_%d%s.%s", fig_num, fig_label, fileext))

if (!dir.exists("separate_fig_files")) dir.create("separate_fig_files")
file.copy(figs$file, file.path("separate_fig_files", figs$fig_label), overwrite = T)

@
\section{References}

\bibliography{bibfile}

\end{document}

% <!--include marginal distributions of training data and compare to test data-->

<< echo=FALSE, eval=FALSE>>=
training <- read.csv("data/features-hamby.csv")
gtrain <- training %>% 
  select(profile1_id, profile2_id, study1, study2, match, ccf:sum_peaks) %>%
  gather(feature, value, ccf:sum_peaks)
# now compare to test data
@
