\documentclass[doubleblind]{elsarticle}

\modulolinenumbers[5]

\usepackage{lineno,hyperref}

\usepackage{amsmath,amsthm,amstext}
\usepackage{graphicx,psfrag,epsf,wrapfig,subfig,float}
\usepackage[dvipsnames]{xcolor}
\usepackage{booktabs,longtable,multirow,multicol,tabu,tabularx,threeparttable,colortbl,array}
\usepackage{adjustbox}
\usepackage{pdflscape,fullpage}
\usepackage{pbox}
\usepackage[numbers]{natbib}
\usepackage{enumerate}

\usepackage{csquotes}% display/block quotes

\setlength{\parindent}{0cm} % remove paragraph indent
\usepackage[skip=\baselineskip]{parskip}

\newenvironment{definition}[1]% environment name 
{% begin code 
  \par\vspace{.75\baselineskip}\noindent 
  \textbf{Definition (#1)}\begin{itshape}% 
  \par\vspace{.5\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\journal{Forensic Science International}
%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
% \bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
% \bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num-names}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\noindent

\begin{frontmatter}

\title{Treatment of Inconclusives in the AFTE Range of Identifications\tnoteref{t1}}

\tnotetext[t1]{This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement \#70NANB15H176 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, University of California Irvine, and University of Virginia.}


%% Group authors per affiliation:
% \author{Elsevier\fnref{myfootnote}}
% \address{Radarweg 29, Amsterdam}
% \fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
% \author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
% \ead[url]{www.elsevier.com}
% 
% \author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
% \cortext[mycorrespondingauthor]{Corresponding author}
% \ead{support@elsevier.com}
% 
% \address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
% \address[mysecondaryaddress]{360 Park Avenue South, New York}

\author[isu,csafe]{Heike Hofmann}
\author[isu,csafe]{Alicia Carriquiry}
\cortext[corauthor]{Corresponding author}\ead{srvander@iastate.edu}
\author[unl]{Susan Vanderplas\corref{corauthor}}
\address[isu]{Statistics Department, Iowa State University\\2438 Osborne Dr, Ames, IA 50011}
\address[csafe]{Center for Statistical Applications in Forensic Evidence, Iowa State University\\613 Morrill Rd, Ames, IA 50011}
\address[unl]{University of Nebraska-Lincoln}   

\begin{abstract}
The treatment of inconclusives in the assessment of errors has been a long-standing issue with far-reaching implications in the legal system. 
\end{abstract}

\begin{keyword}
forensic science, black box studies, proficeincy testing
\end{keyword}

\end{frontmatter}



\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\svp}[1]{{\textcolor{teal}{#1}}}
% boxes for illustrations

\noindent


<<echo = FALSE, message = FALSE, warning = FALSE>>=
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  dpi = 600,
  fig.align = "center",
  out.width = "\\textwidth",
  cache = TRUE,
  fig.path = "figures/",
  echo = FALSE
)
options(knitr.table.format = "latex")

library(stringr)

library(tidyverse)
library(scales)
library(multidplyr) # install_github("hadley/multidplyr")
library(bulletxtrctr) # install_github("csafe-isu/bulletxtrctr")
library(gridExtra)
library(kableExtra)
@


Examiners visually classify similarity of toolmark and firearm evidence according to the AFTE theory of identification \citep{identification} as one of identification, inconclusive or elimination. Exact guidelines for this classification vary from lab to lab; some labs will exclude only on the basis of non-matching class characteristics, such as direction of the twist in rifling, land length or number of lands, or type of rifling. In other labs, CMS (consecutively matching striae) as defined by \citeauthor{biasotti} \citep{biasotti} is used as a measure to quantify the similarity of two lands. In virtually all labs, individual characteristics used to identify matching bullets are derived from visual assessment; some class characteristics may be directly measured, but these are not sufficient for individualization. 
% Identification using 3D Scanning Technology


\begin{table}[hbt]
\noindent\fbox{%
    \parbox{\textwidth}{%
\begin{enumerate}
\item Identification\hfill\newline
Agreement of a combination of individual characteristics and all discernible class characteristics where the extent of agreement exceeds that which can occur in the comparison of toolmarks made by different tools and is consistent with the agreement demonstrated by toolmarks known to have been produced by the same tool.


\item Inconclusive 
\begin{enumerate}
\item Some agreement of individual characteristics and all discernible class characteristics, but insufficient for an identification.
\item Agreement of all discernible class characteristics without agreement or disagreement of individual characteristics due to an absence, insufficiency, or lack of reproducibility.
\item Agreement of all discernible class characteristics and disagreement of individual characteristics, but insufficient for an elimination.
\end{enumerate}

\item Elimination \hfill\newline
Significant disagreement of discernible class characteristics and/or individual characteristics.

\item Unsuitable \hfill\newline
Unsuitable for examination.
\end{enumerate}
    }%
}
\caption{\label{tab:afte} AFTE Rules of Toolmark Identifications \citep{identification}.}
\end{table}
\citet{Dror:2018fp} have started the discussion to treat inconclusive results as an examiner's conscious decision to not decide on an identification or elimination. 
This view has drawn some criticism \citep{Biedermann:2018hr}.

In this paper we want to highlight the treatment of inconclusives in the assessment of error rates from a statistical point of view. % We believe that this -- rather than further dividing the different opinions -- provides a unifying framework.

% \section{Background} % Identification

The identification process -- i.e. the assessment of whether two samples come from the same source (were made by the same tool, the same shoe, the same finger, shot through the same barrel) or from different sources -- is quite complex. 
In order to assess how well this process is functioning, studies with known ground truth have to be employed because casework does not allow for an assessment of correctness.  

As a result, studies which attempt to gain insight into the identification process must also navigate these complexities, ideally without becoming too complicated to reasonably execute. \svp{This isn't quite right phrasing-wise, but I don't know where to put it just yet.}

There are several considerations when designing a study of the identification process:
\begin{description}
\item [Closed vs. Open Set] A closed set study is one in which all questioned samples share the same source as one set of  knowns in the test set. Conversely, an open set study is one in which questioned samples may originate from sources which are not part of the known samples provided in the test set.
\item [White box vs. Black Box] A white-box study is one which makes an attempt to understand \emph{why} an examiner came to the decision, in contrast to a black box study, which only evaluates the correctness of the decision (and not the reasoning behind it). 
\item [Blind testing] A blind (or blinded) study is one in which the participant (in this case the examiner) does not know that they are being tested; that is, a study which appears to be part of a case, rather than research. \citet{pcast2016} recommended that the use of blind testing be increased, as error rates derived from blind studies better generalize to casework.
\item [Number of knowns from different sources] Some studies \citep{keisler} provide only one known exemplar (or a set of multiple exemplars from the same source). Other studies, such as \citep{brundage, hamby,Hamby:2018hu} and the Houston FSC and Phoenix studies from \citet{case-validation}, include multiple different sources as knowns. %Typically, these studies can be completed with the assumption that once an unknown matches one known, it will not match any other knowns. \svp{Add in the issues with this later}
\item [Study length] Most crime labs are understaffed and maintain a fairly large backlog of cases; as a consequence, examiner time is limited. While examiners should participate in these studies, because they benefit the discipline as a whole, they must balance the competing demands of a large and consequential workload. As such, studies which require examiners to make a large number of comparisons may be less likely to have sufficient participants to generalize to the wider discipline.
\end{description}




Ground truth and error rates: 

Ground truth is the knowledge of whether two samples come from the same source or come in fact from two different sources. This knowledge is only available to the body setting up the test.   We distinguish between known matches (same source samples) and known non-matches (samples from different sources).
In particular, ground truth in case work is not known. \svp{move this up a bit?}

%% Transition to error rates
Once a study is run and test samples have been assessed by forensic examiners, error rates can be calculated. Differences between reported results and ground truth are considered to be errors in the identification process. %A detailed discussion on the the exact procedure on how to deal with inconclusive results in calculating error rates is provided in section XXX.



\section{Error Rate Calculation}

The need for scientific validation and experimentally-determined error rates has been identified in several reports evaluating the discipline of forensic science in the United States as critical for addressing problems within the field and improving the overall performance of the justice system.
National Research Council 2009:

\begin{displayquote} much forensic evidence—including, for example, bitemarks and firearm and toolmark identifications—is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline.\end{displayquote}

PCAST (2015) identified two important gaps:

\begin{displayquote} (1) the need for clarity on the scientific meaning of "reliable principles and methods" and "scientific validity" in the context of certain forensic disciplines, and (2) the need to evaluate specific forensic methods to determine whether they have been scientifically established to be valid and reliable.\end{displayquote}

Indeed, when jurors are left to form their own conclusions about the error rates of forensic pattern disciplines, they often come up with estimates which are far lower than empirical studies, suggesting that jurors consider forensic evidence (in the absence of error rates determined from scientific studies) as of more determinative value than is warranted by the evidence. The PCAST Report (2016) summarizes the effect:

\begin{displayquote} In an online experiment, researchers asked mock jurors to estimate the frequency that a qualified, experienced forensic scientist would mistakenly conclude that two samples of specified types came from the same person when they actually came from two different people. The mock jurors believed such errors are likely to occur about 1 in 5.5 million for fingerprint analysis comparison; 1 in 1 million for bitemark comparison; 1 in 1 million for hair comparison; and 1 in 100 thousand for handwriting comparison.\end{displayquote}

Unfortunately, the way in which error rates are computed and reported in the (relatively sparse) literature involving estimating these error rates experimentally is highly variable. In this paper, we will evaluate several studies to assess the technical accuracy and practical utility of the reported error rates. Before approaching the actual studies, it may be useful to explain the logic behind the probability calculations and define some basic notation to facilitate comparison of studies with different structure. 

\subsection{Fundamentals of Probability}


When discussing error rates, it is helpful to think in terms of conditional probability, that is, the probability of an event occurring, taking into account additional information. This additional information may be assumed for the sake of logical reasoning about the likelihood of different conditional events, or it may be known and used to update assessments of probabilities computed before the event was known. \svp{This is confusing and needs a lot of work}

Mathematically, we would write the probability of an event $E$ conditional on another event $A$ as $P(E | A)$. 
Mathematically, conditional probability is a function of the probability of the both events: 
\begin{align}P(E \text{ and } A) &= P(E|A) P(A), \text{ so}\nonumber\\
P(E|A) &= \frac{P(E \text{ and } A)}{P(A)}.\label{eqn:cond-prob}
\end{align}

That is, the probability of both $E$ and $A$ occurring is equal to the probability of $E$ given that $A$ has occurred, multiplied by the probability that $A$ has occurred. 
If there are only two possible outcomes, $A$ and $B$, then, using basic logic (or the law of total probability), we can write the probability of an event $E$ as $$P(E) = P(E|A) P(A) + P(E|B) P(B).$$ That is, if we know the probabilities of $A$ and $B$, and the probability of $E$ conditioned on $A$ and $B$ (separately), we can calculate the probability of $E$ directly. 

% There is one last piece of this probability puzzle that is necessary before we begin a discussion of specific error rates, and that is known as Bayes theorem \svp{cite?}. 
% \svp{Bayes rule}

During the analysis of the outcomes of examiner evaluations in a (well-designed) study, we might reasonably be interested in several different quantities which are all connected to the calculation and reporting of error rates. 
During the study design, experimenters determine the probability of a same-source evaluation (and the complementary probability of a different-source evaluation); this is a function of the study design. In our example above, a same-source evaluation might correspond to event $A$ and a different-source evaluation to event $B$. \svp{This may need to come later during the calculation stage?}

The study data would consist of evaluations reported in the form of a table which might look like the generic table shown in \autoref{tab:generic-results}.

\begin{table}
\centering
\caption{An example results table for a generic experiment where comparisons can be broken down as either from the same source or from different sources, and examiners classify each comparison as an identification, an inconclusive, or an elimination, as in the AFTE rules of identification. Define $S = a + b + c$ as the total number of same source comparisons, $D = d + e + f$ as the total number of different-source comparisons, and $N = S + D$ as the total number of comparisons. Then the probability of the occurrence of any cell in the interior of the table (a, b, c, d, e, or f) can be founc by dividing the respective letter by $N$, the total number of comparisons.
%The total comparisons ($S = a + b + c$ for same source, $D = e + f + g$ for different source) are set by the experimenters (and thus, are fixed at the beginning of the experiment). \svp{I don't like how the table looks with this definition played out, so I'm not defining it for now...}
}\label{tab:generic-results}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc||c|}\hline
& \multicolumn{3}{c||}{Examiner Conclusion} &\\\cline{2-4}
Reality & Identification & Inconclusive & Elimination & Comparison Totals \\\hline
Same source & $a$ & $b$ & $c$ & $S = a + b + c$ \\
Different source & $d$ & $e$ & $f$ & $D = d + e + f$ \\\hline\hline
Conclusion totals & $ a + d$ & $b + e$ & $c + f$ & $N = S + D$\\\hline
\end{tabu}}
\end{table}

The \emph{joint probability} of any reality condition and any examiner conclusion can be found by taking the corresponding cell in \autoref{tab:generic-results} and dividing by $N$, the total number of comparisons evaluated in the study.

\subsection{Conditional Error Rates}\label{sec:conditional-prob}
Using \autoref{tab:generic-results}, we can then calculate the following conditional probabilities which relate to the results of an evaluation of two items:

\newcommand{\vbar}{$ | $~}
\begin{description}
\item [P(same source \vbar examiner makes an identification)] The probability that, assuming the examiner makes an identification, the two items came from the same source. We will refer to this using the shorthand $P(SS|Identification)$. This would be one of two possible correct decisions made during an evaluation and is known as a \emph{true positive}; the rate or probability of this event is known as the \emph{true positive rate} or the \emph{sensitivity}.
\item [P(different source \vbar examiner makes an identification)] The probability that the two items come from different sources, given that the examiner makes an identification. We will refer to this using the shorthand  $P(DS|Identification)$. This is one of two errors an examiner might make during an evaluation; the error is sometimes called a \emph{false positive}, with the probability or error rate termed the \emph{false positive rate}. \svp{something to make this relatable}\vspace{1em}
\item [P(same source \vbar examiner reports an inconclusive)] The probability that two items come from the same source given that the examiner reports the comparison as inconclusive. We will refer to this using the shorthand $P(SS|Inconclusive)$. 
\item [P(different source \vbar examiner makes an identification)] The probability that the two items come from different sources, given that the examiner reports the comparison as inconclusive. We will refer to this using the shorthand  $P(DS|Inconclusive)$. \vspace{1em}
\item [P(same source \vbar examiner makes an elimination)] The probability that two items come from the same source given that the examiner makes an elimination. We will refer to this using the shorthand $P(SS|Elimination)$. This is the second error that an examiner could make during an evaluation, and is termed a \emph{false negative} or a \emph{miss}. The probability or rate of such errors is termed the \emph{false negative rate} or \emph{miss rate}. 
\item [P(different source \vbar examiner makes an elimination)] The probability that the two items come from different sources, given that the examiner makes an elimination. We will refer to this using the shorthand  $P(DS|Elimination)$. This is the second of two correct decisions which could be made during an evaluation and is known as a \emph{true negative} and the rate at which it occurs is known as the \emph{true negative rate} or the \emph{specificity}. 
\end{description}

Generic formulas for these probabilities can each be calculated from the quantities in \autoref{tab:generic-results}. A similar table is shown in \autoref{tab:generic-probabilities}, but instead of raw quantities describing evaluations, we have instead provided the conditional probabilities described at the beginning of this subsection (\autoref{sec:conditional-prob}). 

\begin{table}\centering
\caption{Probabilities of interest, calculated using the quantities introduced in \autoref{tab:generic-results}. In each cell in the main body of the table, probabilities shown are of $P(X | Y)$ where $X$ is the known type of comparison (e.g. reality), either same-source or different-source, and $Y$ is the examiner's decision (one of Identification, Inconclusive, or Elimination. In the last row in the table, the marginal probabilities $P(Y)$ are shown, where $N$ is the total number of comparisons evaluated in the study. The joint probabilities (e.g. $P(X \text{ and } Y)$) described in \autoref{tab:generic-results} can be recovered from this table by multiplying any interior cell by the corresponding cell in the last row of the table. That is, $P(X \text{ and } Y) = P(X|Y) P(Y)$, which follows from \autoref{eqn:cond-prob}. }\label{tab:generic-probabilities}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
\multirow{2}{*}{Same source} & $P(SS|Identification)$ & $P(SS|Inconclusive)$ & $P(SS | Elimination)$ \\
& $=\displaystyle\frac{a}{a + d}$ & $=\displaystyle\frac{b}{b + e}$ & $=\displaystyle\frac{c}{c + f}$ \\\hline
\multirow{2}{*}{Different source} & $P(DS|Identification)$ & $P(DS|Inconclusive)$ & $P(DS | Elimination)$\\ 
 & $=\displaystyle\frac{d}{a + d}$ & $=\displaystyle\frac{e}{b + e}$ & $=\displaystyle\frac{f}{c + f}$ \\\hline\hline
\multirow{2}{*}{$P(Y)$} & $P(Identification)$ & $P(Inconclusive)$& $P(Elimination)$\\
& $=\displaystyle\frac{a + d}{N}$ & $=\displaystyle\frac{b+e}{N}$ & $=\displaystyle\frac{c+f}{N}$ \\\hline
\end{tabu}
}
\end{table}

\svp{A google sheets workbook for performing these calculations is available at \url{http://bit.ly/FTE-error-rate}.}

Given this series of different conditional probabilities, we now must determine which quantities are useful when evaluating the performance of examiner

The set of conditional probabilities in \autoref{tab:generic-probabilities} are all equally important in the assessment of the process of evidence examination. However, standard evaluations of classification algorithms (or human decision making), such as false positive and false negative rates, or sensitivity and specificity \svp{XXX cite...?} generally involve classification states which precisely mirror reality; these problems do not usually involve inconclusive classifications. More precisely, in most classification problems, the number of real classes is the same as the number of possible decisions; in forensics, there is an asymmetry caused by the addition of the inconclusive decision. This difference between forensic evaluation and classification problems which are more commonly evaluated provides an unfortunate opportunity for confusion when using the same methods for assessing forensic decision making. 

\autoref{tab:overall-err-rate} shows several different  error rate calculations, using the general structure of \autoref{tab:generic-results}. Each of these calculations has a  different meaning and interpretation; as a result, it is important to consider which error rate best suits the purpose of a  study \hh{and its interpretation in the larger framework of pattern evidence}.




\newcommand{\redsq}[2][1.15em]{\protect\fcolorbox{black}{red!50}{\makebox[#1][c]{#2\vphantom{dp}}}}
\newcommand{\graysq}[2][1.15em]{\protect\fcolorbox{black}{black!30}{\makebox[#1][c]{#2\vphantom{dp}}}}
\newcommand{\whitesq}[2][1.15em]{\protect\fcolorbox{black}{white!100}{\makebox[#1][c]{#2\vphantom{dp}}}}
% \newcommand{\blacksq}{\fcolorbox{black}{black!100}{\phantom{x}}}
\newcommand{\xsq}{\protect\fcolorbox{black}{white!100}{\makebox[1.25em][c]{x\vphantom{dp}}}}

\begin{table}\protect\caption{Different ways to calculate the overall error rate. An arrangement of two rows of three boxes then resembles the interior cells of \autoref{tab:generic-results}, where \pbox{10em}{\mbox{\whitesq{a}\whitesq{b}\whitesq{c}}\\\mbox{\whitesq{d}\whitesq{e}\whitesq{f}}} are the six conditional quantities under discussion. Let \pbox{1.65em}{\whitesq{}} ~correspond to a correct decision, indicating that the box is to be included in the denominator of the calculation, \pbox{1.65em}{\graysq{}} indicate that the box is to be conditionally excluded from the calculation via \autoref{eqn:cond-prob}, and \pbox{1.65em}{\redsq{}} correspond to an incorrect decision, indicating that the box is to be included in the numerator and denominator of the calculation. 
}
\begin{tabularx}{\linewidth}{llcl}
\# & Type & Illustration & $P(Error) = $\\
0 & No Inconcl. & \pbox{5em}{\whitesq{a}\redsq{c}\\\redsq{d}\whitesq{f}} & $P(DS\text{ and }Ident.) + P(SS\text{ and }Excl.) = \displaystyle\frac{c + d}{a + c + d + f}$\\\\
1 & \pbox[l]{8em}{Condition on\\Not Inconcl} & \pbox{5em}{\whitesq{a}\graysq{b}\redsq{c}\\\redsq{d}\graysq{e}\whitesq{f}} & $\displaystyle\frac{P(DS\text{ and }Ident.) + P(SS \text{ and } Excl.)}{P(Ident.) + P(Excl.)} = \displaystyle\frac{c + d}{a + d + c + f}$\\\\
2 & \pbox[l]{8em}{Inconclusives\\as Correct} & \pbox{5em}{\whitesq{a}\whitesq{b}\redsq{c}\\\redsq{d}\whitesq{e}\whitesq{f}} & $P(DS\text{ and }Ident.) + P(SS\text{ and }Excl.) = \displaystyle\frac{c+d}{N}$\\\\
3 & \pbox[l]{8em}{Inconclusives\\as Incorrect} & \pbox{5em}{\whitesq{a}\redsq{b}\redsq{c}\\\redsq{d}\redsq{e}\whitesq{f}} & $P(DS\text{ and }Ident.) + P(SS\text{ and }Excl.) + P(Inconcl.) = \displaystyle\frac{b + c + d + e}{N}$\\
\end{tabularx}\label{tab:overall-err-rate}
\end{table}

In \autoref{tab:overall-err-rate}, option 0 corresponds to a definition of the overall error rate of a binary, symmetric decision process consisting of two actual states and two possible decisions. For  simplicity we will assume that the two actual states are SS (same source) and DS (different source) and the two decisions are Identification and Elimination. In this case, the overall error rate is the sum of the false positives and the false negatives, divided by the number of overall comparisons. 

\hh{The AFTE Rules of Identification introduce inconclusives as a  third decision into the process. Inconclusives do not correspond to a real state, we therefore} 
%However, if an inconclusive category is introduced which does not correspond to a real state, then we 
have to decide how to handle them \hh{when assessing error rates}. 


One reasonable solution to the  would be to calculate the conditional probability of an error given that the classification was not inconclusive, as in option 1 of \autoref{tab:overall-err-rate}. Note that if we condition on the decision being either identification or elimination, the probability calculations for scenarios 0 and 1 in \autoref{tab:overall-err-rate} are identical; the only difference between the two is the setup of the scenario: In \autoref{eqn:overall-error-conditional} we allow for inconclusives as a decision category but restrict them from the calculation of the error rate entirely. This solution does entirely ignore inconclusives, though, and as they are part of the process and can be included in examiner testimony (usually phrased as ``could not be excluded"), this elimination from consideration is not appropriate if the goal of a study is to assess the error rate of the entire evaluation process. \svp{\citet{koehlerProficiencyTestsEstimate2013} recommends using the option 1 approach for calculating error rates while  also tracking rates of inconclusive identifications ``for other purposes", perhaps including reporting during testimony.}\svp{The last clause needs work here XXX}

Under AFTE's Theory of Identification, however, inconclusive results are acceptable outcomes and not considered errors; to give a very extreme example,  an examiner could report inconclusives for the rest of their career and never make an error. Option 2 shows this way of treating inconclusives as correct regardless of the actual state.
%In order to successfully accommodate this, we would likely calculate $P(Elimination | SS) = c/S$ as the false negative rate, and $P(Identification|DS) = d/D$ as the false positive rate (Note that the conditioning in these probabilities is flipped; instead of considering each column in the table as a separate group, these probabilities are conditioned on rows). Combining these two rates to get an overall error rate produces the equation for Option 1 in \autoref{tab:overall-err-rate}. 
What this means in real terms, is that inconclusives are effectively counted as identifications if they are actually from the same source, but counted as eliminations if they are actually from a different source. When the goal is to assess the error rate of the examiner under the prevailing guidelines of the AFTE Theory of Identification, this is a reasonable option for assessing error rates of firearm and toolmark examiners, with the slightly odd side-effect that an examiner who marks every comparison as inconclusive does not provide any useful actionable information, but effectively has a zero error rate. 

When viewed within the framework of the legal system, error rates calculated using option 2  are  not considering any error introduced by the evidence itself. If e.g. a firearm does not mark well, a firearms examiner might not be able to make an identification or an elimination through no fault of theirs. However, 
from the wider perspective of the \emph{system}, it is  important to look at the overall error rate of the entire process. This requires that we consider the errors from the perspective of the real-world state: two pieces of evidence are  either from the same source or originate from different sources; an inability to distinguish between those options is an error in the \emph{process} even it if is not an error made on the part of the examiner. 

Option 3 of \autoref{tab:overall-err-rate} reflects this global view by counting  inconclusives as errors. Practically, option 3 reflects the error of the examination process rather than the examiner as an individual. To illustrate why this is the case, consider that an examiner may, upon inspection of a fired bullet, find that there are insufficient individualizing characteristics to make an identification. This does not reflective negatively on the examiner's skill; it is merely a consequence of the contact made between the bullet and the barrel during the firing process. Nonetheless, even though a pair of evidence originated from the same physical source, it is not possible to evaluate the evidence and make an identification. The inconclusives are representative of the errors involved in the process of evidence recording, rather than the evaluation. 


\subsection{Positive Predictive Value}
In \autoref{sec:conditional-prob}, we briefly discussed two different sets of conditional probabilities. The difference between $P(Identification | SS)$ and $P(SS|Identification)$ is subtle but important: $P(Identification | SS)$ is the probability that an examiner will make an identification if the evidence is from the same source, that is, $P(Identification|SS) = a/S$. In contrast, $P(SS|Identification)$ is the probability that if the examiner makes an identification, the evidence is actually from the same source. More generally, $P(Identification|SS)$ is predicting the examiner's behavior given the experimental stimuli, while $P(SS|Identification)$ is predicting the state of reality given the examiner's decision. 

In legal settings, it is the state of reality that is under debate; the examiner's opinion is testimony and is information we already have. This probability is also known as \emph{positive predictive value (PPV)} and is used to characterize the performance of a process\citep{med-pos-pred-value}. In medical contexts, this is often the performance of a test for a disease; in forensics, the process includes evidence creation as well as the examination. The medical context is perhaps more approachable: a given test will have a sensitivity (true positive rate) and a specificity (true negative rate) that combined would yield the accuracy rate. However, if you as a patient are sent for a test which yields a positive result, what you care about is not the accuracy rate (because when testing for rare diseases the true negatives are much more common than true positives); it is the probability that, given your positive result, you actually have the disease. In tests for rare diseases which are highly accurate, the positive predictive value can be fairly small, because false positives are more common than the incidence rate of the disease.

In the next section, we will examine the results from a number of studies of the error rate of firearms examinations, identifying the results which are reported, converting these results (where possible) into the format used in \autoref{tab:generic-results}, and calculating the conditional probabilities in the format of \autoref{tab:generic-probabilities}. Prior to that, however, it may be useful to develop one more idea: in a perfect world with examiners who are all-knowing, what should the results of these studies look like?

\subsection{Ideal study probabilities}


\hh{In the absence of knowing the truth we have to resort to working within a probabilistic framework to assess the likelihood of certain events. Some (XXX? quantify?) evidence is based on questions of pattern matching: does the shoe print from the crime scene match a particular shoe? was the bullet found at a crime scene fired from a particular weapon? does the breech face impression on a cartridge case from the crime scene match the breech face of a particular weapon frame?}

\hh{
In these situations the fundamental quantity in question is the probability that two pieces of evidence come from the same source or from different sources. In a legal setting the opinion provided by the expert guides this assessment. So given expert testimony, how does or should the evaluation of the above probability change?  }

The AFTE theory of firearms and toolmarks identification as shown in \autoref{tab:afte} allows an expert to come to three main conclusions: make an identification, i.e. two pieces of evidence are thought to come from the same source, make an elimination, i.e. two pieces of evidence are tought to come from different sources, or an inconclusive, i.e. the evidence does not allow either an identification or an elimination. 

Given these three outcomes, we are interested for each the probability of same source and different sources, resulting in a total of six ($3 \times 2$) conditional probabilities. In a perfect world, we would expect the probabilities shown in \autoref{tab:ideal-probabilities}:


\begin{table}
\caption{Expected conditional probabilities in a perfect world.}\label{tab:ideal-probabilities}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & $P(SS|Identification) = 1 $& $P(SS|Inconclusive) = P(SS)$ & $P(SS | Elimination) = 0$ \\
Different source & $P(DS|Identification) = 0$ & $P(DS|Inconclusive) = P(DS)$ & $P(DS | Elimination)= 1$ \\\hline
\end{tabu}
}
\end{table}

Most notably, we would expect inconclusive results to not show any dependence on the type of comparison unless there is some evidence provided by experts that one type of conclusion is more difficult to make than another.

In actual case work we are not able to assess the probabilities for same source or different source.  However, those probabilities are easily accessible in studies or in blinded proficiency testing,\svp{ as $P(SS)$ and $P(DS)$ in these studies are determined by the experimental design.}

\hh{reporting of eliminations differs from lab to lab. Some labs do only allow firearm examiners to make eliminations based on differences in class characteristics. XXX quantify: baldwin, }


%The probability to make an identification  could be either reported directly or calculated as a composite of true positive and false positive rates:

% \[
% P(\text{identification made}) = TPR \times P(\text{same source}) + FPR \times P(\text{different source}) 
% \]

\svp{By examining these studies}, we can use existing data to evaluate the conditional probability of same source versus different sources given an expert's testimony directly.



\section{Studies}

Baldwin , Keisler , Lyons \cite{lyons}, Hare, Fadul \cite{fadul}, Brundage-Hamby, Duez , Chumbley - can't find any examiner evaluations for Chumbley

confidence interval used in Baldwin: Pearson-Clopper, applied to Keisler sensitivity and specificity:

<<r functions, echo=FALSE>>=
clopper <- function(alpha, success, trials) {
  lower <- qbeta(alpha/2, success, trials-success+1)
  upper <- qbeta(1-alpha/2, success+1, trials-success)
  c(lower, upper)
}

# Keisler sensitivity
clopper(0.05, 1508, 1512)
# Keisler specificity
clopper(0.05, 805, 1008)
@

\subsection{Baldwin Study \cite{Baldwin:2014bb}}

Each kit consists of 15 sets of 3 known cartridge cases and 1 questioned. For all participants 5 of the sets were from same-source and 10 of the sets were from different sources. {\bf ``We instructed all participants to refrain from sharing or discussing the contents or results of their sample sets and answers to minimize the risk of revealing the design".} \hh{this is a weakness of the design,  but it also fixes the frequencies of same source and different source occurrences and with it the probabilities for the events:}

A total of 25 firearms was used for the study, such that within each kit no firearm was re-used for either knowns or questioned cartridge cases, i.e. no additional information could be gained by comparing any cartridge cases across sets.

218 participants provided evaluations of the kits, evaluation results are shown in \autoref{tab:baldwin-totals}. Each of these evaluations is  based on a comparison of one questioned cartridge case to three known same source  cartridge cases.


\begin{table}[hbt]
\centering
\caption{Reported number of evaluations  from Baldwin Study.}\label{tab:baldwin-totals}
{\tabulinesep=1.2mm
\begin{tabu}{|c|rrr|r|}\hline
 & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
 & Identification & Inconclusive  & Elimination &  \\
$X = $Reality&&or no response&&Totals\\\hline
Same source & 1075  & 11 & 4 & 1090\\
Different source & 22 & 735+2 & 1421 & 2180\\\hline
Percent & 33.5\% & 22.9\% & 43.6\% & 100.0\\\hline
\end{tabu}
}
\end{table}




\begin{table}[hbt]
\centering
\caption{Error rate assessment for Baldwin Study.}\label{tab:baldwin-errors}
{\tabulinesep=1.2mm
\begin{tabu}{|l|rr|}\hline
 & Missed Identification & Missed Elimination  \\ \hline
Option 2 (FTE error) & 0.0037  & 0.0101\\
Option 3 (Process error) & 0.0138 & 0.3482\\ \hline
\end{tabu}
}
\end{table}


The resulting set of conditional probabilities is shown in \autoref{tab:baldwin}

\begin{table}[hbt]
\centering
\caption{Predictive probabilities from Baldwin Study.}\label{tab:baldwin}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & 0.9799 & 0.0147 & 0.0028 \\
Different source & 0.0201 & 0.9853 & 0.9972 \\\hline
\end{tabu}
}
\end{table}

It seems that firearms examiners are more willing to bet on identifications than on eliminations. Based on the set up of the study, we would expect the probabilities for same and different source given an inconclusive result is reported to be: 
\begin{eqnarray*}
P(\text{same source} \mid \text{Inconclusive}) &=& P(\text{same source}) = \frac{1}{3},  \\
P(\text{different source} \mid \text{Inconclusive}) &=& P(\text{different source}) = \frac{2}{3}
\end{eqnarray*}

In fact, even if inconclusives and eliminations are combined, the error rate of making a false elimination is 0.0069, only a third of the error of a false identification:

\begin{eqnarray*}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{rcccr}
P (\text{same source} \mid \text{inconclusive or elimination}) &=& \frac{11+4}{748+1425} &=& 0.0069 \\
P (\text{different source} \mid \text{inconclusive or elimination}) &=& \frac{737+1421}{748+1425} &=& 0.9931 
\end{array}
\end{eqnarray*}

\hh{useful information, but not sure where to put it - probably for evaluating the error in the process}
Only cartridge cases caught with the cartridge catching device (XXX name?) were used to assemble kits. No other pre-screening of usability for comparisons was done. 

For 3234 comparisons, FTEs evaluated how many of the known cartridge cases were usable for an evaluation: all three specimens were used in 3018 cases, two were used in 207 cases, and only one was used in nine cases.




\subsection{Keisler Study \cite{keisler}}

Each test kit consisted of sets of 20 pairs of cartridge cases,  12 of the pairs were from the same source and 8 pairs were from different sources.

126 participants reported results as shown in \autoref{tab:keisler-totals}.


\begin{table}[hbt]
\centering
\caption{Evaluation results from Keisler Study.}\label{tab:keisler-totals}
{\tabulinesep=1.2mm
\begin{tabu}{|c|rrr|r|}\hline
Evaluations & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination & Totals \\\hline
Same source & 1508 & 4 & 0 & 1512 \\
Different source & 0 & 203 & 805 & 1008 \\\hline
Percent & 59.8\% & 8.2\% & 31.9\% & 100.0 \\\hline
\end{tabu}
}
\end{table}

\begin{table}[hbt]
\centering
\caption{Error rate assessment for Keisler Study.}\label{tab:keisler-errors}
{\tabulinesep=1.2mm
\begin{tabu}{|l|rr|}\hline
 & Missed Identification & Missed Elimination  \\ \hline
Option 2 (FTE error) & 0.0000  & 0.0000\\
Option 3 (Process error) & 0.0026 & 0.2014\\ \hline
\end{tabu}
}
\end{table}


The resulting set of conditional probabilities is given in \autoref{tab:keisler}.

\begin{table}[hbt]
\centering
\caption{Conditional probabilities from Keisler Study.}\label{tab:keisler}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & 1.0000 & 0.0193 & 0.0000 \\
Different source & 0.0000 & 0.9807 & 1.0000 \\\hline
\end{tabu}
}
\end{table}

Again, inconclusive results are much more indicative of different source results than they should be  based on the setup of the study; we would expect same source results with a probability of 0.60 given an inconclusive result. 

\subsection{Duez Study \cite{Duez:2017kha}}

using virtual microscopy, each participant is asked to make eight evaluations of breech face impressions. 

CCTS1: set of three knowns, four questioned breech face impressions. All questioned breech face impressions are from the same source as the knowns. 

CCTS2: set of three knowns, four questioned breech face impressions. Two questioned breech face impressions are from the same source, two are from different sources.

Both sets were evaluated 46 times each (10 trainees, 36 fully certified examiners). CCTS1 resulted in 46 x 4 correct identifications.

The design of the experiment does not allow us to quantify all of the quantities to evaluate examiner performance unless we aggregate performance over both sets.

Combined, the probability for a  same source pair is 6/8 or 75\% and the probability for  a different source  pair is 2/8 or 25\%.

\autoref{tab:duez-totals} shows results reported by 56 participants (46 examiners, 10 trainees).

\begin{table}[hbt]
\centering
\caption{Evaluation results from Duez Study.}\label{tab:duez-totals}
{\tabulinesep=1.2mm
\begin{tabu}{|c|rrr|r|}\hline
Evaluations (Examiner+Trainee) & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination & Totals \\\hline
Same source & 276+59 & 0+1 & 0+0 & 276+60 \\
Different source & 0+2 & 12+13 & 80+5 & 92+20 \\\hline
Percent & 75.2\% & 5.8\% & 19.0\% & 100.0\\\hline
\end{tabu}
}
\end{table}

\begin{table}[hbt]
\centering
\caption{Error rate assessment for Duez Study for certified examiners/trainees.}\label{tab:duez-errors}
{\tabulinesep=1.2mm
\begin{tabu}{|l|rr|}\hline
 & Missed Identification & Missed Elimination  \\ \hline
Option 2 (FTE error) & 0.0000/0.0000  & 0.0000/0.1000\\
Option 3 (Process error) & 0.0000/0.0167 & 0.1304/0.7500\\ \hline
\end{tabu}
}
\end{table}

\autoref{tab:duez-errors} shows the two sets of error rates for the Duez study. The error rate of certified examiners in making eliminations is much lower than that of the ten trainees. This is reflected in the error rates of the overall process. The process errors are much smaller for certified firearms examiners than for trainees. However, the probability to miss eliminations is again high, lab rules contribute to this error (see discussion below).

The set of conditional probabilities are shown in \autoref{tab:duez}. 
The differences in certified examiners and trainees are not large, so we are only showing the aggregates. Certified examiners have a perfect identification rate, only evaluations of different source pairs lead to inconclusives. 

\begin{table}[hbt]
\centering
\caption{Conditional probabilities from Duez Study.}\label{tab:duez}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & 0.9941 & 0.0385 & 0.0000 \\
Different source & 0.0059 & 0.9615 & 1.0000 \\\hline
\end{tabu}
}
\end{table}
Again, the probability of same source pairs given examiners reported an inconclusive result is much lower than the expect 75\% based on the setup of the study.

The authors report that
``13\% of examiners are not permitted to eliminate on individual characteristics
(therefore, their conclusions of inconclusive are perfectly acceptable)."

\hh{It is interesting to note that 7 of the examiners did not report eliminations due to  lab rules. These rules seem to be in contradiction to community accepted AFTE rules and need to be considered during reporting and testimony evaluations. XXX Bias against defense. XXX Needless to say that we do not agree with the authors' assessment that conclusions of inconclusives are perfectly acceptable.}






\subsection{Brundage-Hamby Study \cite{Hamby:2018hu}}

The Brundage-Hamby study consists of sets of 20 known samples and 15 questioned samples. The 20 known samples are 2 bullets from each of ten consecutively manufactured barrels. The Brundage-Hamby study is a closed set study, i.e. the 15 questioned bullets are known to be fired from one of these ten barrels, and examiners are asked to identify which of the knowns a questioned bullet matches. 
The study was originally reported on by Brundage in 1998 \cite{brundage} and updates on the study with increasing number of responses have been published several times since  \cite{hamby, Hamby:2018hu}. 
The design has also been copied for a study of cartridge cases of the same firearms in \citet{fadul}.

Results reported in the 2018 paper are shown in \autoref{tab:hamby-totals}.

\begin{table}
% \begin{tabular}{lrrrr}\hline
% Test Series	& \#Examiners &	\#Examiners 	& \#Inconclusively 	& \#Incorrectly  \\
% & & Reporting Inconclusives & Identified Bullets & Identified Bullets \\ \hline
% Brundage &	67 &	1 &	1 &	0\\
% Hamby &	630 &	4 &	7 &	0\\
% Totals: &	697 &	5 &	8 &	0\\ \hline
% \end{tabular}
\centering
\caption{\label{tab:hamby-totals} Evaluations reported from the combined Brundage-Hamby studies. Note the focus on identifications. }
{\tabulinesep=1.2mm
\begin{tabu}{|c|rrr|r|}\hline
Evaluations & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination & Totals \\\hline
Same source & 10447 & 8 & 0 & 10455 \\
Different source & 0 & ? & ? & 47047.5 \\\hline
\end{tabu}
}
\end{table}

\begin{table}[hbt]
\centering
\caption{Error rate assessment for Brundage-Hamby Study.}\label{tab:hamby-errors}
{\tabulinesep=1.2mm
\begin{tabu}{|l|rr|}\hline
 & Missed Identification & Missed Elimination  \\ \hline
Option 2 (FTE error) & 0.0000  & ? \\
Option 3 (Process error) & 0.0008 & ?\\ \hline
\end{tabu}
}
\end{table}

\begin{table}[hbt]
\centering
\caption{Conditional probabilities from Brundage-Hamby Study.}\label{tab:hamby}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & 1.000 & ? & ? \\
Different source & 0.000 & ? & ? \\\hline
\end{tabu}
}
\end{table}
The answer sheets for Brundage-Hamby sets only asks for reports of identifications for each of the questioned bullets. While the number of same source comparison is obviously one for each questioned bullets, i.e.\ each participant made exactly fifteen same source comparisons. However, the number of different source comparisons that were made  is an open question. Technically, there are nine different source comparisons for each questioned bullet for a total of 135 different source comparisons.  
However, these evaluations are not independent of each other: once a bullet is successfully identified to a set of knowns, none of the other sets of knowns can possibly be a match. Rather than having to make a call on a match, the evaluation shifts to a confirmation of an elimination. We will assume that an examiner will need to make an average of 4.5 different source comparisons before making an identification, resulting in an average of 67.5 different source comparisons for the whole set, and an approximate 47047.5 different source comparisons, in which no identification was reported (see \autoref{app:brundage} for mathematical details).
What is still missing is any information on eliminations versus inconclusive results for different source comparisons.



\subsection{Lyons Study}
This study involved the use of 10 consecutively manufactured extractors. First, the researchers established that there were sufficient individualizing characteristics to identify cartridges ejected by the same tool, and that there were sufficient individualizing characteristics to exclude cartridges ejected by sequentially manufactured tools. Then, they assembled 20 kits with pairs of 2 cartridges from each of the 10 extractors (the knowns) and 10 or 12 unknown cartridges, such that each known corresponded to an unknown, with some replication in most of the kits. Thus, the setup of this study is similar to the Hamby studies, which had 35 bullets each; 20 knowns from 10 barrels and 15 unknowns. As with the Hamby studies, as a closed set study with multiple knowns, it is not possible to determine the total number of different source comparisons. 

It should be noted that there are some methodological problems with the Lyons study - examiners who did not initially include all 12 unknown evaluations were given the chance to correct results; in addition, not every examiner evaluated the same number of unknowns. Excluding these results might provide different estimates, but we have chosen to use the numbers as reported in the paper for this calculation.

\begin{table}[hbt]
\centering
\caption{Evaluation results from Lyons Study. Note that because all unknowns match to a known, we can infer that there are at least the same number of different-source identifications as there are same-source eliminations. }\label{tab:lyons-totals}
{\tabulinesep=1.2mm
\begin{tabu}{|c|rrr|r|}\hline
Evaluations & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination & Totals \\\hline
Same source & 174\footnotemark &  1 & 3 & 178 \\
Different source & 3 & ? & ? & ? \\\hline
\end{tabu}
}
\end{table}
\footnotetext{\citet{lyons} reports 175 correct identifications, but it is clear from the discussion that one of those same source identifications was actually an inconclusive. 12 answer sheets with 12 correct identifications, one with 10 (out of 10) correct identifications, one with 9 correct identifications and 3 errors, and one with 11 correct identifications and one inconclusive. So 12*12 + 10 + 9 + 11 = 174. }


\begin{table}[hbt]
\centering
\caption{Conditional probabilities from Lyons Study. Note that because we cannot calculate the total number of different-source comparisons, we do not have the ability to estimate any probabilities which condition on examiner conclusions of inconclusive or elimination.}\label{tab:lyons}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & 0.9831 & ? & ? \\
Different source & 0.0169 & ? & ? \\\hline
\end{tabu}
}
\end{table}

\subsection{Bunch \& Murphy Study \cite{bunch2003comprehensive}}
\section{Discussion and Conclusions}

number of firearms very small 

positive predictive value (control group) vs. post-test rate (for an individual case)

Implications for court process - examiner states error rate, defense asks for inconclusive rate (established during controlled testing?) and elimination rate

\section{Other Thoughts/Interesting finds}
\begin{itemize}
\item Full pipeline errors
  \begin{itemize}
    \item \svp{\citet{edmondGuideInterpretingForensic2014} is a fingerprint study that discusses the potential for errors at any stage in the process and the limitations of error rate studies as they are currently performed.}
    \item \svp{\citet{hozoDecisionMakingWhenData2008} calls for what seems to be full pipeline analysis of errors, with included components tailored to the goals of the research. Focused primarily on clinical trials, so a good portion of the paper doesn't apply...}
  \end{itemize}
  
\item \svp{It may be worth comparing/contrasting AFTE guidelines to SWGTREAD and other disciplines (SWGTREAD has a few extra categories that make it interesting \citep{swgtread-conclusions-2013}) and Uniform Language Testimony and Reporting guidelines - e.g. \citet{ULTR-latent} and \citet{ULTR-firearms-pattern} (all of the ULTR guidelines are at \url{https://www.justice.gov/olp/uniform-language-testimony-and-reports}).}
\item \svp{Inconclusives also show up in diagnostic medicine (x-rays, mammograms, scans). Is the statistical treatment there any different given that typically there are better defined populations and some available statistical information on those populations? It's also worth noting that you can recall the patient and do additional (informative) testing with people; with forensic evidence, you can't get better data than what you had initially. }

\item \svp{From my read of \citet{tukeyConclusionsVsDecisions}, it seems to be an argument for inconclusives (e.g. doing nothing). It may be worth differentiating the error rate calculation focused on the examiner's decision from action taken by the legal system, which is a full-pipeline thing that includes systematic errors during evidence recording, collection, etc. (things the examiner doesn't control). What defendents care about is the full-scope conclusion, which includes consideration of all of the errors earlier in the pipeline. }
\end{itemize}
<<output-all-figs, eval = F, include = F>>=
thisdoc <- readLines("index.Rnw")

# First, get figures that aren't in code chunks
# Haven't really tested this part - regex may need work
raw_figures <- tibble(
  line_start = which(str_detect(thisdoc, "\\begin\\{figure\\}")), 
  line_end = which(str_detect(thisdoc, "\\end\\{figure\\}")),
  subfloats = purrr::map2(line_start, line_end, 
                          ~str_extract(thisdoc[.x:.y], "\\includegraphics(?:\\[.*\\])?\\{(.*)\\}") %>%
                            str_replace("\\includegraphics(?:\\[.*\\])?\\{(.*)\\}", "\\1"))
)

# Code chunk figures
chunk_figures <- tibble(
  line_start = which(str_detect(thisdoc, "^<<.*>>=$")),
  line_end = which(str_detect(thisdoc, "^@$")),
  chunk_name = str_replace(thisdoc[line_start], "<<([0-9A-z-]*?),?((?:.*=.*){1,})>>=", "\\1")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, chunk_name, ~thisdoc[.x] %>%
                              str_extract("fig\\.subcap ?= ?.*?\\),") %>%
                              str_replace("fig\\.subcap ?= ?", "") %>%
                              str_replace(",$", "") %>%
                              parse(text=.) %>% eval %>% length() %>% 
                              seq(1, ., by = 1) %>%
                              sprintf("figures/%s-%d.pdf", .y, .))
  )

# Knitr included graphics figures
chunk_incl_figures <- tibble(
  line_start = which(str_detect(thisdoc, "^<<.*>>=$")),
  line_end = which(str_detect(thisdoc, "^@$")),
  chunk_name = str_replace(thisdoc[line_start], "<<([0-9A-z-]*?),?((?:.*=.*){1,})>>=", "\\1")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, line_end, function(.x, .y) {
      thisdoc[.x:.y] %>%
        paste(collapse = " ") %>%
        str_extract(., "include_graphics\\(c?\\(?.*?\\)?\\)") %>%
        str_replace("include_graphics\\(", "") %>%
        str_extract_all('\\"[\\.A-z0-9 /-]*?\\"') %>%
        parse(text = .) %>% eval() %>%
        str_remove_all("\\\\|\"")
    })
  )

# All figures
figs <- bind_rows(raw_figures, chunk_figures, chunk_incl_figures) %>%
  arrange(line_start) %>%
  mutate(subfloats = purrr::map(subfloats, function(x) data_frame(file = x, exists = file.exists(x)))) %>%
  tidyr::unnest() %>%
  filter(exists) %>%
  mutate(fig_num = group_indices(., line_start, line_end)) %>%
  group_by(fig_num) %>%
  mutate(rn = row_number(),
         maxrn = n(),
         fig_label = letters[rn],
         fig_label = ifelse(maxrn > 1, fig_label, ""),
         fileext = tools::file_ext(file),
         fig_label = sprintf("Figure_%d%s.%s", fig_num, fig_label, fileext))

if (!dir.exists("separate_fig_files")) dir.create("separate_fig_files")
file.copy(figs$file, file.path("separate_fig_files", figs$fig_label), overwrite = T)

@
\section{References}

\bibliography{bibfile}

\section{Supplemental Material}
\subsection{All possible Brundage-Hamby Sets}\label{app:brundage}
Brundage-Hamby sets consist of a set of 20 known bullets (two each from the ten consecutively manufactured barrels) and 15 questioned bullets.
\citet{brundage} outlines the construction of sets of questioned bullets in detail as follows: ten bullets are chosen, one from each of the ten barrels. The remaining five bullets are picked at random from the barrels, such that at most three questioned bullets are from the same barrel. 

Using this strategy, a total of 1452 different sets can be constructed. These sets have questioned bullets of three main forms, listed in \autoref{tab:sets} from the perspective of the number of barrels with one, two, or three matching questioned bullets. 

\begin{table}
\caption{\label{tab:sets}Number of all possible sets of questioned bullets for Brundage-Hamby sets.}
\centering
\begin{tabular}{|r|ccc|l|} \hline
 &  one questioned & two questioned & three questioned  & number of \\
\# barrels & bullet matches & bullets match & bullets match & possibilities\\ \hline
I & 5 & 5 & 0 & 252 = ${10 \choose 5}{5 \choose 5}$\\
II & 6 & 3 & 1 & 840\\
III & 7 & 1 & 2 & 360\\ \hline
\end{tabular}
\end{table}

\autoref{fig:histogram} shows the histogram of the total number of different source comparisons before and identification is made.

<<histogram, fig.width=8, fig.height = 4, outwidth=".5\\textwidth", warning=FALSE, message=FALSE, fig.caption="Histogram of the total number of different source comparisons before an identification is made.", fig.label="fig:histogram">>=
library(tidyverse)
sets <- read.csv("data/allpossible-hambysets.csv")
sets$ds <- rowSums(sets-1)

sets %>% 
  ggplot(aes(x = ds)) + geom_histogram(binwidth=1, colour="grey50") +
  theme_bw() +
  xlab("Number of different source comparisons in a set before an identification is made") +
  ylab("Number of sets") +
  scale_x_continuous(breaks=c(49, 54.5, 61,  67.5, 74, 80.5, 86), labels=c("49", "54.5", "61", "67.5", "74", "80.5",  "86"))
@

\end{document}

% <!--include marginal distributions of training data and compare to test data-->

<< echo=FALSE, eval=FALSE>>=
training <- read.csv("data/features-hamby.csv")
gtrain <- training %>% 
  select(profile1_id, profile2_id, study1, study2, match, ccf:sum_peaks) %>%
  gather(feature, value, ccf:sum_peaks)
# now compare to test data
@
