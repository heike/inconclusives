\documentclass[doubleblind]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\usepackage{amsmath,amsthm}
\usepackage{amstext}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{float}
% \usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{csquotes}% display/block quotes
\usepackage{fullpage}

\usepackage[numbers]{natbib}

\newenvironment{definition}[1]% environment name 
{% begin code 
  \par\vspace{.75\baselineskip}\noindent 
  \textbf{Definition (#1)}\begin{itshape}% 
  \par\vspace{.5\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\journal{Forensic Science International}
%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
% \bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
% \bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num-names}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\noindent

\begin{frontmatter}

\title{Treatment of Inconclusives in the AFTE Range of Identifications\tnoteref{t1}}

\tnotetext[t1]{This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement \#70NANB15H176 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, University of California Irvine, and University of Virginia.}


%% Group authors per affiliation:
% \author{Elsevier\fnref{myfootnote}}
% \address{Radarweg 29, Amsterdam}
% \fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
% \author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
% \ead[url]{www.elsevier.com}
% 
% \author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
% \cortext[mycorrespondingauthor]{Corresponding author}
% \ead{support@elsevier.com}
% 
% \address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
% \address[mysecondaryaddress]{360 Park Avenue South, New York}

\author[isu,csafe]{Heike Hofmann}
\author[isu,csafe]{Alicia Carriquiry Hofmann}
\cortext[corauthor]{Corresponding author}\ead{srvander@iastate.edu}
\author[unl]{Susan Vanderplas\corref{corauthor}}
\address[isu]{Statistics Department, Iowa State University\\2438 Osborne Dr, Ames, IA 50011}
\address[csafe]{Center for Statistical Applications in Forensic Evidence, Iowa State University\\613 Morrill Rd, Ames, IA 50011}
\address[unl]{University of Nebraska-Lincoln}   

\begin{abstract}
The treatment of inconclusives in the assessment of errors has been a long-standing issue with far-reaching implications in the legal system. 
\end{abstract}

\begin{keyword}
forensic science, black box studies, proficeincy testing
\end{keyword}

\end{frontmatter}



\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\svp}[1]{{\textcolor{teal}{#1}}}
\noindent


<<echo = FALSE, message = FALSE, warning = FALSE>>=
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  dpi = 600,
  fig.align = "center",
  out.width = "\\textwidth",
  cache = TRUE,
  fig.path = "figures/",
  echo = FALSE
)
options(knitr.table.format = "latex")

library(stringr)

library(tidyverse)
library(scales)
library(multidplyr) # install_github("hadley/multidplyr")
library(bulletxtrctr) # install_github("csafe-isu/bulletxtrctr")
library(gridExtra)
library(kableExtra)
@


Examiners visually classify similarity of toolmark and firearm evidence according to the AFTE theory of identification \citep{identification} as one of identification, inconclusive or exclusion. Exact guidelines for this classification vary from lab to lab; some labs will exclude only on the basis of non-matching class characteristics, such as direction of the twist in rifling, land length or number of lands, or type of rifling. In other labs, CMS (consecutively matching striae) as defined by \citeauthor{biasotti} \citep{biasotti} is used as a measure to quantify the similarity of two lands. In virtually all labs, individual characteristics used to identify matching bullets are derived from visual assessment; some class characteristics may be directly measured, but these are not sufficient for individualization. 

% Identification using 3D Scanning Technology



\noindent\fbox{%
    \parbox{\textwidth}{%
\begin{enumerate}
\item Identification\hfill\newline
Agreement of a combination of individual characteristics and all discernible class characteristics where the extent of agreement exceeds that which can occur in the comparison of toolmarks made by different tools and is consistent with the agreement demonstrated by toolmarks known to have been produced by the same tool.


\item Inconclusive 
\begin{enumerate}
\item Some agreement of individual characteristics and all discernible class characteristics, but insufficient for an identification.
\item Agreement of all discernible class characteristics without agreement or disagreement of individual characteristics due to an absence, insufficiency, or lack of reproducibility.
\item Agreement of all discernible class characteristics and disagreement of individual characteristics, but insufficient for an elimination.
\end{enumerate}

\item Elimination \hfill\newline
Significant disagreement of discernible class characteristics and/or individual characteristics.

\item Unsuitable \hfill\newline
Unsuitable for examination.
\end{enumerate}
    }%
}

\citet{Dror:2018fp} have started the discussion to treat inconclusive results as an examiner's conscious decision to not decide on an identification or elimination. 
This view has drawn some criticism \citep{Biedermann:2018hr}.

In this paper we want to highlight the treatment of inconclusives in the assessment of error rates from a statistical point of view. We believe that this -- rather than further dividing the different opinions -- provides a unifying framework.



\section{Background}

The identification process -- i.e. the assessment of whether two samples come from the same source (were made by the same tool, the same shoe, the same finger, shot through the same barrel) or from different sources -- is quite complex. 
In order to assess how well this process is functioning, studies with known ground truth have to be employed because casework does not allow for an assessment of correctness.

There are different kinds of studies: closed set studies, open set studies, blind testing. 

Ground truth and error rates: 

Ground truth is the knowledge of whether two samples come from the same source or come in fact from two different sources. This knowledge is only available to the body setting up the test.   We distinguish between known matches (same source samples) and known non-matches (samples from different sources).
In particular, ground truth in case work is not known.

Once a study is run and test samples have been assessed by forensic examiners, error rates can be calculated. Differences between reported results and ground truth are considered to be errors in the identification process. A detailed discussion on the the exact procedure on how to deal with inconclusive results in calculating error rates is provided in section XXX.

The {\bf true positive rate}, also known as {\bf sensitivity}, is the probability that two samples from the same source are identified in the examination process as a match, $P(\text{identification mde} \mid \text{same source samples})$. 

The {\bf true negative rate}, also known as {\bf specificity}, is the probability that two samples from different sources result in an elimination in the examination process, $P(\text{elimination mde} \mid \text{different sources samples})$. 





\section{Error Rate Calculation}

National Research Council 2009:

\begin{displayquote} much forensic evidence—including, for example, bitemarks and firearm and toolmark identifications—is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline.\end{displayquote}

PCAST (2015) identified two important gaps:

\begin{displayquote} (1) the need for clarity on the scientific meaning of "reliable principles and methods" and "scientific validity" in the context of certain forensic disciplines, and (2) the need to evaluate specific forensic methods to determine whether they have been scientifically established to be valid and reliable.\end{displayquote}

PCAST Report (2016):

\begin{displayquote} In an online experiment, researchers asked mock jurors to estimate the frequency that a qualified, experienced forensic scientist would mistakenly conclude that two samples of specified types came from the same person when they actually came from two different people. The mock jurors believed such errors are likely to occur about 1 in 5.5 million for fingerprint analysis comparison; 1 in 1 million for bitemark comparison; 1 in 1 million for hair comparison; and 1 in 100 thousand for handwriting comparison.\end{displayquote}




\hh{The fundamental quantity in question is the probability that two pieces of evidence come from the same source or from different sources. In a legal setting the opinion provided by the expert is the major provider of information. So given expert testimony, how does or should the evaluation of the above probability change?  }

\begin{eqnarray*}
P(\text{same source} \mid \text{identification made}) = \\ 
P (\text{identification made} \mid \text{same source}) \times 
\frac{P(\text{same source})}{P(\text{identification made})}
\end{eqnarray*}

The probability to make an identification  could be either reported directly or calculated as a composite of true positive and false positive rates:

\[
P(\text{identification made}) = TPR \times P(\text{same source}) + FPR \times P(\text{different source}) 
\]

In the presence of studies, we can use existing data to evaluate the conditional probability of same source versus different sources given an expert's testimony directly.



\section{Studies}

Baldwin \cite{Baldwin:2014bb}, Keisler \cite{keisler}, Lyons \cite{lyons}, Hare, Fadul \cite{fadul}, Brundage-Hamby \citet{Hamby:2018hu}, Duez \cite{Duez:2017kha}, Chumbley \cite{Macziewski:2016jw}

confidence interval used in Baldwin: Pearson-Clopper, applied to Keisler sensitivity and specificity:

<<r functions, echo=FALSE>>=
clopper <- function(alpha, success, trials) {
  lower <- qbeta(alpha/2, success, trials-success+1)
  upper <- qbeta(1-alpha/2, success+1, trials-success)
  c(lower, upper)
}

# Keisler sensitivity
clopper(0.05, 1508, 1512)
# Keisler specificity
clopper(0.05, 805, 1008)
@

Baldwin Study:

setup of the study: 25 firearms,   each kit consists of 15 comparisons of 3 knowns to 1 questioned cartridge case. For all participants 5 of the sets were from Known same-source firearms and 10 of the sets were from Known different source firearms. {\bf ``We instructed all participants to refrain from sharing or discussing the contents or results of their sample sets and answers to minimize the risk of revealing the design".} \hh{this is a weakness of the design,  but it also fixes the frequencies of same source and different source occurrences and with it the probabilities for the events:}

\[
P(\text{same source}) = \frac{1}{3},  \ \ \ P(\text{different source}) = \frac{2}{3}
\]

218 participants sent evaluations:

\begin{tabular}{lrrrr}
& \# Pairs & Elimination & Inconclusives or no response & Identification\\
Known Matches & 1090 & 4 & 11 & 1075\\
Known Non-Matches & 2180 & 1421 & 735+2 & 22\\
\end{tabular}

Each of the above evaluations is actually based on a comparison on three known same source  cartridge cases to one questioned.

For 3234 comparisons, FTEs evaluated how many of the known cartridge cases were usable for an evaluation: all three specimens were used in 3018 cases, two were used in 207 cases, and only one was used in nine cases.


The probability of same source given an identification is made is 

\begin{eqnarray*}
P (\text{same source} \mid \text{identification made}) &=&  \frac{1075}{1075+ 22} = 0.9799 \\
P (\text{different source} \mid \text{identification made}) &=&  \frac{22}{1075+ 22} = 0.0201 \\
\end{eqnarray*}


\begin{eqnarray*}
P (\text{same source} \mid \text{inconclusive}) &=& \frac{11}{737+11} = 0.0147 \\
P (\text{different source} \mid \text{inconclusive}) &=& \frac{737}{737+11} = 0.9853 \\
\end{eqnarray*}


\begin{eqnarray*}
P (\text{same source} \mid \text{elimination made}) &=& \frac{4}{1421+4} = 0.0028 \\
P (\text{different source} \mid \text{elimination made}) &=&  \frac{1421}{1421+4} = 0.9972 \\
\end{eqnarray*}


Brundage-Hamby:

\begin{tabular}{lrrrr}
Test Series	& \# Examiners Participating in Test &	\# Examiners Reporting Inconclusives	& \#Inconclusively Identified Bullets	& \#Incorrectly Identified Bullets \\
Brundage &	67 &	1 &	1 &	0\\
Hamby &	630 &	4 &	7 &	0\\
Totals: &	697 &	5 &	8 &	0\\
\end{tabular}

\section{Discussion and Conclusions}

number of firearms very small 

\section{Other Thoughts/Interesting finds}
\begin{itemize}
\item Full pipeline errors
  \begin{itemize}
    \item \svp{\citet{edmondGuideInterpretingForensic2014} is a fingerprint study that discusses the potential for errors at any stage in the process and the limitations of error rate studies as they are currently performed.}
    \item \svp{\citet{hozoDecisionMakingWhenData2008} calls for what seems to be full pipeline analysis of errors, with included components tailored to the goals of the research. Focused primarily on clinical trials, so a good portion of the paper doesn't apply...}
  \end{itemize}
  
\item \svp{It may be worth comparing/contrasting AFTE guidelines to SWGTREAD and other disciplines (SWGTREAD has a few extra categories that make it interesting \citep{swgtread-conclusions-2013}) and Uniform Language Testimony and Reporting guidelines - e.g. \citet{ULTR-latent} and \citet{ULTR-firearms-pattern} (all of the ULTR guidelines are at \url{https://www.justice.gov/olp/uniform-language-testimony-and-reports}).}
\item \svp{Inconclusives also show up in diagnostic medicine (x-rays, mammograms, scans). Is the statistical treatment there any different given that typically there are better defined populations and some available statistical information on those populations? It's also worth noting that you can recall the patient and do additional (informative) testing with people; with forensic evidence, you can't get better data than what you had initially. }

\item \svp{From my read of \citet{tukeyConclusionsVsDecisions}, it seems to be an argument for inconclusives (e.g. doing nothing). It may be worth differentiating the error rate calculation focused on the examiner's decision from action taken by the legal system, which is a full-pipeline thing that includes systematic errors during evidence recording, collection, etc. (things the examiner doesn't control). What defendents care about is the full-scope conclusion, which includes consideration of all of the errors earlier in the pipeline. }
\end{itemize}
<<output-all-figs, eval = F, include = F>>=
thisdoc <- readLines("index.Rnw")

# First, get figures that aren't in code chunks
# Haven't really tested this part - regex may need work
raw_figures <- tibble(
  line_start = which(str_detect(thisdoc, "\\begin\\{figure\\}")), 
  line_end = which(str_detect(thisdoc, "\\end\\{figure\\}")),
  subfloats = purrr::map2(line_start, line_end, 
                          ~str_extract(thisdoc[.x:.y], "\\includegraphics(?:\\[.*\\])?\\{(.*)\\}") %>%
                            str_replace("\\includegraphics(?:\\[.*\\])?\\{(.*)\\}", "\\1"))
)

# Code chunk figures
chunk_figures <- tibble(
  line_start = which(str_detect(thisdoc, "^<<.*>>=$")),
  line_end = which(str_detect(thisdoc, "^@$")),
  chunk_name = str_replace(thisdoc[line_start], "<<([0-9A-z-]*?),?((?:.*=.*){1,})>>=", "\\1")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, chunk_name, ~thisdoc[.x] %>%
                              str_extract("fig\\.subcap ?= ?.*?\\),") %>%
                              str_replace("fig\\.subcap ?= ?", "") %>%
                              str_replace(",$", "") %>%
                              parse(text=.) %>% eval %>% length() %>% 
                              seq(1, ., by = 1) %>%
                              sprintf("figures/%s-%d.pdf", .y, .))
  )

# Knitr included graphics figures
chunk_incl_figures <- tibble(
  line_start = which(str_detect(thisdoc, "^<<.*>>=$")),
  line_end = which(str_detect(thisdoc, "^@$")),
  chunk_name = str_replace(thisdoc[line_start], "<<([0-9A-z-]*?),?((?:.*=.*){1,})>>=", "\\1")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, line_end, function(.x, .y) {
      thisdoc[.x:.y] %>%
        paste(collapse = " ") %>%
        str_extract(., "include_graphics\\(c?\\(?.*?\\)?\\)") %>%
        str_replace("include_graphics\\(", "") %>%
        str_extract_all('\\"[\\.A-z0-9 /-]*?\\"') %>%
        parse(text = .) %>% eval() %>%
        str_remove_all("\\\\|\"")
    })
  )

# All figures
figs <- bind_rows(raw_figures, chunk_figures, chunk_incl_figures) %>%
  arrange(line_start) %>%
  mutate(subfloats = purrr::map(subfloats, function(x) data_frame(file = x, exists = file.exists(x)))) %>%
  tidyr::unnest() %>%
  filter(exists) %>%
  mutate(fig_num = group_indices(., line_start, line_end)) %>%
  group_by(fig_num) %>%
  mutate(rn = row_number(),
         maxrn = n(),
         fig_label = letters[rn],
         fig_label = ifelse(maxrn > 1, fig_label, ""),
         fileext = tools::file_ext(file),
         fig_label = sprintf("Figure_%d%s.%s", fig_num, fig_label, fileext))

if (!dir.exists("separate_fig_files")) dir.create("separate_fig_files")
file.copy(figs$file, file.path("separate_fig_files", figs$fig_label), overwrite = T)

@
\section{References}

\bibliography{bibfile}

\end{document}

% <!--include marginal distributions of training data and compare to test data-->

<< echo=FALSE, eval=FALSE>>=
training <- read.csv("data/features-hamby.csv")
gtrain <- training %>% 
  select(profile1_id, profile2_id, study1, study2, match, ccf:sum_peaks) %>%
  gather(feature, value, ccf:sum_peaks)
# now compare to test data
@
