\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{lineno,hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage{authblk}
\usepackage{amsmath,amsthm,amstext}
\usepackage{bm}
\usepackage{graphicx,psfrag,epsf,wrapfig,subfig,float,capt-of}
\usepackage[dvipsnames]{xcolor}
\usepackage{booktabs,longtable,multirow,multicol,tabu,tabularx,threeparttable,colortbl,array}
\usepackage{adjustbox}
\usepackage{pdflscape,fullpage}
\usepackage{pbox}
\usepackage{natbib}

\usepackage{cleveref}

\bibliographystyle{plainnat}
\usepackage{enumerate}

\usepackage{csquotes}% display/block quotes

\setlength{\parindent}{0cm} % remove paragraph indent
\usepackage{parskip}

\newenvironment{definition}[1]% environment name 
{% begin code 
  \par\vspace{.75\baselineskip}\noindent 
  \textbf{Definition (#1)}\begin{itshape}% 
  \par\vspace{.5\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}

\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Necessary commands for study-wise summary
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\renewcommand\tabularxcolumn[1]{m{#1}}% for vertical centering text in X column

\newsavebox{\fmbox}
\newenvironment{fmpage}[1]
{\begin{lrbox}{\fmbox}\begin{minipage}[t]{#1}}
{\end{minipage}\end{lrbox}\fbox{\usebox{\fmbox}}}


\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\svp}[1]{{\textcolor{Fuchsia}{#1}}}
\newcommand{\ac}[1]{{\textcolor{WildStrawberry}{#1}}}
\newcommand{\new}[1]{{\textcolor{JungleGreen}{#1}}}

\begin{document}
\noindent

\title{Treatment of Inconclusives in the AFTE Range of Conclusions
\thanks{This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.}
}


\author[1, 2]{Heike Hofmann}
\author[3]{Susan Vanderplas}
\author[1, 2]{Alicia Carriquiry}
\affil[1]{Statistics Department, Iowa State University\\2438 Osborne Dr, Ames, IA 50011}
\affil[2]{Center for Statistics and Applications in Forensic Evidence (CSAFE), Iowa State University\\613 Morrill Rd, Ames, IA 50011}
\affil[3]{Statistics Department, University of Nebraska Lincoln\\340 Hardin Hall North Wing, Lincoln, NE 68583-0963}   
\maketitle

\begin{abstract}
In the past decade, and in response to the recommendations set forth by the \citet{nrc:2009}, scientists have conducted several black-box studies that attempt to estimate the error rates of firearm examiners. Most of these studies have resulted in vanishingly small error rates, and at least one of them \citep{Baldwin:2014bb} was cited by the President's Council of Advisors in Science and Technology (PCAST) during the Obama administration, as an example of a well-designed experiment. What has received little attention, however, is the actual calculation of error rates and in particular, the effect of inconclusive findings on those error estimates. The treatment of inconclusives in the assessment of errors has far-reaching implications in the legal system. Here, we revisit several black-box studies in the area of firearms examination and propose a unifying and justifiable approach to the calculation of error rates that  is directly applicable in forensic laboratories and the courtroom.
\end{abstract}


\renewcommand{\thefootnote}{\roman{footnote}}
<<echo = FALSE, message = FALSE, warning = FALSE>>=
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  dpi = 600,
  fig.align = "center",
  out.width = "\\textwidth",
  cache = TRUE,
  fig.path = "figures/",
  echo = FALSE
)
knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
options(knitr.table.format = "latex")

library(stringr)

library(tidyverse)
library(scales)
# library(multidplyr) # install_github("hadley/multidplyr")
library(bulletxtrctr) # install_github("csafe-isu/bulletxtrctr")
library(gridExtra)
library(kableExtra)
@
% 
% \hh{Big picture of the paper:
% \begin{enumerate}
% \item Error rates in firearms and toolmark examination are needed
% \item Theoretical discussion of error rates: error rates and predictive values
% \item Discussion of studies: naming conventions and testing procedures
% \item Evaluation of error rates: inconclusive results and a uniform evaluation sheet
% \item Summary of studies using the uniform evaluation sheet
% \item Discussion of strengths and weaknesses of studies, recommendations
% \end{enumerate}}
% 
% \tableofcontents



\section{Introduction and Background}
It has now been more than a decade since the publication of the \citet{nrc:2009} report, which discussed the need for better-designed studies to estimate error rates in forensic disciplines. 
In this paper, we examine several new studies that have been published since the 2009 report and assess the state of error rate studies in firearms and toolmark analysis. 
For each study, we calculate the error rates from the published study results using standardized methods. 
We also assess the impact of study design and treatment of inconclusives on the calculated error rates.
To lay a foundation for the common calculation methods used in this paper, we introduce some theory and vocabulary before comparing the studies and assessing the state of error rates in firearms and toolmark analysis.

Examiners visually classify the similarity of toolmark and firearm evidence according to the AFTE theory of identification using a three-point scale: identification, inconclusive, or elimination
\citep[][see also \Cref{tab:afte}]{identification}. 
Exact guidelines for this classification vary from lab to lab; some labs will exclude only based on non-matching class characteristics, such as the direction of rifling, the number of lands and their width, or the type of rifling \citep{bunch2003comprehensive}. 
In other labs, CMS (consecutively matching striae) as defined by  \citeauthor{biasotti}  \citep{biasotti} is used as a measure to quantify the similarity of two lands. 
In virtually all labs, the assessment of individual characteristics of bullet markings is done by visual inspection. 


\begin{table}[hbt]
\noindent\fbox{%
\parbox{.98\textwidth}{%
\begin{enumerate}
\item Identification\hfill\newline
Agreement of a combination of individual characteristics and all discernible class characteristics where the extent of agreement exceeds that which can occur in the comparison of toolmarks made by different tools and is consistent with the agreement demonstrated by toolmarks known to have been produced by the same tool.

\item Inconclusive 
\begin{enumerate}
\item Some agreement of individual characteristics and all discernible class characteristics, but insufficient for an identification.
\item Agreement of all discernible class characteristics without agreement or disagreement of individual characteristics due to an absence, insufficiency, or lack of reproducibility.
\item Agreement of all discernible class characteristics and disagreement of individual characteristics, but insufficient for an elimination.
\end{enumerate}

\item Elimination \hfill\newline
Significant disagreement of discernible class characteristics and/or individual characteristics.

\item Unsuitable \hfill\newline
Unsuitable for examination.
\end{enumerate}
}}
\caption{\label{tab:afte} AFTE Rules of Toolmark Identifications  \citep{identification}.}
\end{table}

The identification process -- i.e. the assessment of whether two samples come from the same source (were made by the same tool, the same shoe, the same finger, shot through the same barrel) or from different sources -- is quite complex. 
For firearms and toolmark evidence to be admissible in court, it must be possible to explicitly characterize the accuracy of the examination process \citep{giannelliDaubertInterpretingFederal1993}.

The need for scientific validation and experimentally-determined error rates has been identified in several reports evaluating the discipline of forensic science in the United States as critical for addressing problems within the field and improving the overall performance of the justice system. 
According to the  \citet{nrc:2009}:

\begin{displayquote} much forensic evidence - including, for example, bitemarks and firearm and toolmark identifications - is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline.\end{displayquote}

The  \citet{pcast2016} identified two important gaps:

\begin{displayquote} (1) the need for clarity on the scientific meaning of ``reliable principles and methods" and ``scientific validity" in the context of certain forensic disciplines, and (2) the need to evaluate specific forensic methods to determine whether they have been scientifically established to be valid and reliable.\end{displayquote}

When jurors are left to form their own conclusions about the error rates of forensic pattern disciplines, they often come up with estimates which are far lower than empirical studies, suggesting that jurors consider forensic evidence (in the absence of error rates determined from scientific studies) as of more determinative value than is warranted by the evidence. The PCAST Report (2016) summarizes the effect:

\begin{displayquote} In an online experiment, researchers asked mock jurors to estimate the frequency that a qualified, experienced forensic scientist would mistakenly conclude that two samples of specified types came from the same person when they came from two different people. The mock jurors believed such errors are likely to occur about 1 in 5.5 million for fingerprint analysis comparison; 1 in 1 million for bitemark comparison; 1 in 1 million for hair comparison; and 1 in 100 thousand for handwriting comparison.\end{displayquote}

Studies in the form of designed experiments serve as the basis for assessing the accuracy of the identification process.
Knowing ground truth means that the experimenter knows whether two samples come from the same source or from different sources.
Casework can not be used to assess the accuracy of the identification process, because ground truth is not knowable.
However, in scientific studies of forensic examination, ground truth is available to the experimenter because the test and the ``evidence" were explicitly designed.

In designed experiments, any physical factors that might affect the outcome of a subsequent assessment of pattern similarity can be controlled or systematically varied \citep{spiegelmanAnalysisExperimentsForensic2013}.
In the specific situation of firearms evidence, the type of firearm and the ammunition used are of particular interest.

In addition to the physical evidence-generation process, experimenters must carefully control the information participants have about the study design and structure. 
We provide an example of the effect of extraneous information on estimated error rates in \Cref{sec:studies}.
Studies that attempt to gain insight into the identification process must navigate these complexities, ideally without becoming too complicated themselves.

In the remainder of this section, we introduce terms commonly used in experiments with a specific focus on the forensic identification process:
\begin{description}
\item [Known and Questioned Samples] Most studies are set up to mirror casework, i.e.\ there is a set of samples of known origin and a set of questioned samples of unknown origin. The main objective of a study is to determine for a pair of known and questioned samples if they share the same source or come from different sources.
\item [Closed vs. Open Set] In a closed set study all questioned samples originate from the same source as one  (set of) known sample(s). 
Conversely, an open set study is one in which questioned samples may originate from sources outside of the known samples provided. 
Similarly, not all known samples might have a match among the questioned samples.
\item [White Box vs. Black Box] In a white-box study the experimenter attempts to understand \emph{why} an examiner came to a specific decision, in contrast to a black box study, which only evaluates the correctness of the decision without assessing the reasoning behind it. 
\item [Blind testing] A blind (or blinded) study is one in which the participant (in this case the examiner) does not know that they are being tested%
\footnote{
There is some variability in what the term ``double-blind" refers to in the context of forensic studies. 
The use in \citep{bunch2003comprehensive, stromanEmpiricallyDeterminedFrequency2014, Duez:2017kha} does not match the use in \citep{pcast2016,spiegelmanAnalysisExperimentsForensic2013,koehlerProficiencyTestsEstimate2013}.
At least some of the confusion stems from the use of the term in medical contexts, where ``double-blind" means that neither the patient nor the doctor knows whether the treatment received is the control or the treatment under investigation. 
In both uses of ``double-blind", the underlying goal is to remove any biases which may cause the evaluation of the available evidence differently.
In drug trials, the doctor judges whether the patient has improved or not, and thus, the doctor cannot know which treatment the patient has received, because the knowledge may affect his decision subconsciously. 
In forensic studies, the examiner assesses the provided evidence and comes to a decision; thus, in a double-blind forensic study, the examiner cannot know that the evidence is part of a designed study rather than casework, because knowing that extra information may subconsciously affect the examiner's assessment. 
\citet{kerkhoffPartdeclaredBlindTesting2018} provide a more thorough discussion of the different ways the term ``blind testing" has been used in forensic error rate studies.
}; that is, a study which appears to be part of a case, rather than research. 
Blind testing is often recommended  \citep{pcast2016,spiegelmanAnalysisExperimentsForensic2013,koehlerProficiencyTestsEstimate2013}, because the error rates from blind testing better generalize to casework: many studies in a variety of disciplines have shown that people behave differently when they know they are being tested. 

\item [Number of knowns from different sources] Some studies  \citep{keisler} provide only one known exemplar (or a set of multiple exemplars from the same source). Other studies, such as  \citep{brundage, hamby,Hamby:2018hu} and the Houston FSC and Phoenix studies from  \citet{case-validation}, include multiple different sources as knowns. 
\item [Study length] Most crime labs are understaffed and maintain a fairly large backlog of cases; as a consequence, examiner time is limited. While examiners should participate in these studies, they must balance the competing demands of a large and consequential workload and the benefit to FTE community. Studies that require examiners to make a large number of comparisons may be less likely to find a sufficient number of participants to generate an acceptable sample size.
\end{description}

Many of these considerations can be boiled down to limiting the examiner's knowledge about the study as much as possible, so that (ideally) the only information provided is the information in each pairwise comparison. 
Blind studies, for instance, remove the knowledge that evidence is from a designed test and not casework. 
Open set studies, which are preferable to closed set studies, remove the knowledge an examiner might have about whether a comparison is guaranteed to match one of the provided knowns. 
Studies with designs that limit the number of comparisons by allowing only a single known source and a single unknown source prevent the examiner from using any sort of deductive reasoning. 
However, open set studies provide only limited additional information even in the case where  multiple unknown sources are compared to a single known source. 

%% Transition to error rates
Once a study is designed and test samples have been assessed by forensic examiners, error rates can be calculated. 
In the remainder of this paper, we discuss the experimentally determined error rates reported in  studies in firearms and toolmark examination. 
We examine the variability in the methods for calculating and reporting error rates, and the different meanings and utility of different types of error rates. 
Using a set of different accuracy and error rates, we assess the state of firearms and toolmark analysis, and the implications of currently available data on the legal assessment of the reliability of testimony about firearms and toolmark related evidence.


\section{Calculating Error Rates}\label{sec:rates}
Before approaching the actual studies, we define a framework and some basic notation to facilitate a comparison of studies with different experimental designs and  explore the logic behind the numerical calculations. %order

\subsection{Classification Framework}

When firearms and toolmark examiners assess a set of evidence (generally, a pair of items, one of known provenance and one of unknown provenance) examiners decide whether the evidence is an identification, inconclusive, or an elimination.  
\Cref{fig:shape-overview} is a sketch showing the relationship between evidence (dots) and examiners' decisions (areas). The color for both evidence and decisions corresponds to the source of evidence (different/same) and the type of decision made, respectively. The different combinations of colored dots on top of colored regions correspond to the six potential outcomes of evidence assessment and examiner decision. Obviously, not all of these outcomes are correct.  

<<shape-overview, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas). In a perfect scenario dots only appear on the shaded area of the same color. Any dots on differently colored backgrounds indicate an error in the examination process.", out.width = "60%", fig.width = 6, fig.height = 4, dpi = 500, cache = F>>=
source("code/make_inconcl_plot.R") 
target_plot(100, 50, 10, 10, 50, 100) 
@

Deciding whether evidence is an identification, exclusion, or inconclusive provides a \emph{classification} of the evidence, allowing us to make use of elements of the larger \emph{classification framework}.
We will introduce the classification framework and the respective error rates using as an example a generic experiment:
let us assume that a representative sample of independent forensic toolmark examiners were asked to complete a total of $N$ comparisons, consisting of $S$ same-source comparisons and $D$ different source comparisons.
An aggregation of the examiners' evaluations can then be reported as shown in \Cref{tab:generic-results}. Note that the layout of the table  is nothing but a summary of all the possible combinations of examiners' decisions and the actual state of the evidence as sketched out in \Cref{fig:shape-overview}. The pieces of \Cref{fig:shape-overview} are shown along with letters $a$ to $f$ in \Cref{tab:generic-results}. These letters will be used throughout this section to calculate various accuracy rates and probabilities. 

\begin{table}
\centering
\caption{
An example results table for a generic experiment where comparisons are pairs known to be either from the same source  or  from different sources, and examiners classify each comparison as an identification, an inconclusive, or an elimination, as  specified in the AFTE rules of identification. 
Let $S$ be the total number of same source comparisons, then $S$ is the sum of $a, b,$ and $c$. Similarly, $D$, the total number of different source comparisons, can be written as $D = d + e + f$. The sum of $S$ and $D$ is the total number  of comparisons, $N$. 
}\label{tab:generic-results}
\begin{tabularx}{.985\textwidth}{Y|YYY|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\\hline
\multicolumn{1}{c}{} & Identification & Inconclusive & \multicolumn{1}{c}{Elimination} & Source Total\\\hline
Same Source & \begin{minipage}[c]{1em}$\bm{a}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.25cm]{figures/a.pdf} \end{minipage} & 
              \begin{minipage}[c]{1em}$\bm{b}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.3cm]{figures/b.pdf} \end{minipage} & 
              \begin{minipage}[c]{1em}$\bm{c}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.35cm]{figures/c.pdf} \end{minipage}& $S = a + b + c$\\
Different source & \begin{minipage}[c]{1em}$\bm{d}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.25cm]{figures/d.pdf} \end{minipage}& 
              \begin{minipage}[c]{1em}$\bm{e}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.3cm]{figures/e.pdf} \end{minipage}& 
              \begin{minipage}[c]{1em}$\bm{f}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.35cm]{figures/f.pdf} \end{minipage}&  $D = e + f + g$\\\hline
\multicolumn{1}{Y}{Conclusion Total} & $a + d$ & $b + e$ & \multicolumn{1}{c}{$c + f$} & \multicolumn{1}{Y}{$N = S + D$}\\\hline\hline
\end{tabularx}
\end{table}

The \emph{joint probability} of any source condition and any examiner conclusion can be found by taking the corresponding cell in \Cref{tab:generic-results} and dividing it by $N$, the total number of comparisons  in the study.\footnote{The joint probability of events A and B is the probability that both A and B occur. Here, the joint probability of an examiner identification and a same-source comparison would be equivalent to $a/N$.} 
Thus, the joint probability of a same-source comparison and examiner identification is $a/N$, while the joint probability of a different-source comparison and examiner identification would be $d/N$. 
The accuracy (or error rate) of a classification method can then be determined by assessing how often the process produces a correct (or incorrect) result.

\subsection{What makes an error?}\label{sec:error-rates}
In a classification problem, we would normally take the results in \Cref{tab:generic-results} and calculate various measures of accuracy and classification error.
In firearms and toolmark examination, however, this is complicated by a mismatch between the physical source of the evidence and the set of examiners' decisions.
There are two possibilities to describe the physical state of the evidence: two pieces of evidence are either from the same source or from different sources, but there are three primary outcomes from an examiner's decision.
The examiner can make an identification, an elimination, or an inconclusive determination.\footnote{The fourth category, unsuitable for examination, should be used when evaluating single pieces of recovered evidence; it does not result from the comparison of an unknown source to a known source.}
The difference in the number of possible source categories and resulting decisions raises the question about how to deal with inconclusive results when calculating error rates.
Under AFTE guidelines, an inconclusive result is an acceptable outcome of a comparison and  therefore can not be considered as an error made by the examiner.
It has been argued, however, that inconclusive decisions are systematic errors that occur during the evaluation process, because the final decision does not match the known information \citep{koehlerFingerprintErrorRates2007}.
While these two statements seem to be contradictory, we provide a foundation under which the two approaches can be partially reconciled in a way that provides additional insight into the examination process.

\citet{Dror:2018fp} suggest treating inconclusive results as equivalent to a ``decision with certainty that the quantity and quality of information are not sufficient to draw any conclusion regarding the source".
Under this framework, an examiner's assessment starts with inconclusive and is refined to identification or elimination given sufficient evidence in either direction.
Statistically, this would suggest that the decision about the amount of evidence available (inconclusive or not) would be independent of the source of the evidence (same or different source). That is, inconclusive decisions should be equally likely in same-source and different-source assessments.

There are three main ways that inconclusive decisions can be treated in calculating error rates: inconclusive decisions can be
(1)  excluded from error calculations, (2) included as correct results  \citep{Duez:2017kha}, or (3) included as incorrect results  \citep{chumbleyValidationToolMark2010}; each of these decisions has an impact on the actual value of the error rate as well as the interpretation of the resulting error rates. 
\Cref{tab:overall-err-rate} shows an overview of these three approaches to calculating the error rate based on the general structure of \Cref{tab:generic-results}, with a fourth option which will be addressed in more detail in \Cref{sec:inconclusives}. 

Each of the approaches to calculating the error rate has a  different meaning and interpretation; as a result, it is important to consider which error rate best suits the purpose of a study and its interpretation in the larger framework of pattern evidence. 
Error calculations in (1) arise in two scenarios:
in \citet{chumbleyValidationToolMark2010} examiners were asked to make either an identification or an elimination, and were not given the option of making an inconclusive decision.  
This results in cell values $b$ and $e$ of zero, i.e.\ the calculation of the overall error rate simplifies to a binary, symmetric decision process consisting of two actual states and two possible decisions. 
In this case, the overall error rate is the sum of the false positives and the false negatives, divided by the number of overall comparisons. 

\newcommand{\redsqd}[2][.8em]{\protect\fcolorbox{black}{red!50}{\makebox[#1][c]{#2\vphantom{a}}}}
\newcommand{\graysqd}[2][.8em]{\protect\fcolorbox{black}{black!30}{\makebox[#1][c]{#2\vphantom{a}}}}
\newcommand{\whitesqd}[2][.8em]{\protect\fcolorbox{black}{white!100}{\makebox[#1][c]{#2\vphantom{a}}}}

\newcommand{\redsq}[2][1.15em]{\protect\fcolorbox{black}{red!50}{\makebox[#1][c]{#2\vphantom{dp}}}}
\newcommand{\graysq}[2][1.15em]{\protect\fcolorbox{black}{black!30}{\makebox[#1][c]{#2\vphantom{dp}}}}
\newcommand{\whitesq}[2][1.15em]{\protect\fcolorbox{black}{white!100}{\makebox[#1][c]{#2\vphantom{dp}}}}
% \newcommand{\blacksq}{\fcolorbox{black}{black!100}{\phantom{x}}}
\newcommand{\xsq}{\protect\fcolorbox{black}{white!100}{\makebox[1.25em][c]{x\vphantom{dp}}}}

\begin{fmpage}{.985\textwidth}\captionsetup{type=table}\protect\captionof{table}{Different ways to calculate the overall error rate. 
An arrangement of two rows of three boxes resembles the interior cells of \Cref{tab:generic-results}.
We use the following convention:\\
\hfill\pbox{1.65em}{\small\whitesqd{}} correct decisions, cell values are included in the denominator\\
\hfill\pbox{1.65em}{\graysqd{}} cell value is zero or excluded from the calculation\\
\hfill\pbox{1.65em}{\redsqd{}} incorrect decision, cell values are included in the numerator and denominator of the error ratio.
}

\begin{tabularx}{.99\textwidth}{llcl}
\# & Type & Illustration & $P(Error) = $\\
1 & \pbox[l]{9em}{No Inconclusives\\or Incl. Ignored} & 
\pbox{6em}{\whitesq{a}\graysq{b}\redsq{c}\\\redsq{d}\graysq{e}\whitesq{f}} & 
$\displaystyle\frac{P(DS\text{ \& }Ident.) + P(SS \text{ \& } Elim.)}{P(Ident.) + P(Elim.)} = \displaystyle\frac{c + d}{a + c + d + f}$\\\\
%
2 & \pbox[l]{9em}{Inconclusives\\as Correct} & 
\pbox{6em}{\whitesq{a}\whitesq{b}\redsq{c}\\\redsq{d}\whitesq{e}\whitesq{f}} & 
$P(DS\text{ \& }Ident.) + P(SS\text{ \& }Elim.) = \displaystyle\frac{c+d}{N}$\\\\
%
3 & \pbox[l]{9em}{Inconclusives\\as Incorrect} & 
\pbox{6em}{\whitesq{a}\redsq{b}\redsq{c}\\\redsq{d}\redsq{e}\whitesq{f}} & 
$\begin{array}{r}
P(DS\text{ \& }Ident.) + P(SS\text{ \& }Elim.) + P(Inconclusives) =  \\
= \displaystyle\frac{b + c + d + e}{N} 
\end{array}$\\\\
4 & \pbox[l]{9em}{Inconclusives\\as Eliminations} & 
\pbox{6em}{\whitesq{a}\redsq{b}\redsq{c}\\\redsq{d}\whitesq{e}\whitesq{f}} & 
$\begin{array}{r}
P(DS\text{ \& }Ident.) + P(SS\text{ \& }Elim.) + P(SS\text{ \& }Inconcl.) =  \\
= \displaystyle\frac{b + c + d}{N} 
\end{array}$ \\
\end{tabularx}\label{tab:overall-err-rate}
\end{fmpage}

Under Scenario 1, inconclusive decisions are treated as a valid decision category but restricted from the calculation of the error rate. 
This solution  entirely ignores inconclusive decisions. However, in practice, inconclusive results are a substantial part of the identification process and are included in examiner testimony (often phrased as ``could not be excluded"). 
Thus, this elimination from consideration is not appropriate if the goal of a study is to assess the error rate of the entire evaluation process. 
\citet{koehlerProficiencyTestsEstimate2013} recommends using  the approach of option 1 for calculating error rates while  also tracking rates of inconclusive identifications ``for other purposes".


Under AFTE's Theory of Identification, inconclusive results are acceptable outcomes and not considered errors. This approach is captured in Option 2:  inconclusive decisions are treated as correct regardless of the actual state.
What this means in real terms, is that inconclusive decisions are counted as identifications if they are actually from the same source, but counted as eliminations if they are actually from a different source. 
Note that this approach is exploitable in a strange way: to give a very extreme example, an examiner could report inconclusive decisions for every evaluation over the rest of their career and never make an error.
However, when the goal is to assess the error rate of the examiner under the prevailing guidelines of the AFTE Theory of Identification, option 2 is a reasonable option for assessing error rates of firearm and toolmark examiners.

Option 3 provides an approach to view inconclusive results and the corresponding error rates within the wider framework of the legal system:
if a firearm does not mark well, a firearms examiner might not be able to make an identification or an elimination for reasons having nothing to do with the examiner's skill.  
From the  perspective of the \emph{identification process}, though, an inconclusive result does not result in a decision of identification or elimination and can therefore not be considered a successful assessment:
two pieces of evidence are  either from the same source or originate from different sources and an inability to distinguish between those options is an error.

Practically, Option 3 reflects the error of the \emph{examination process} rather than the examiner as an individual. 
In Option 4, which will become more relevant in \Cref{sec:inconclusives}, inconclusive results are treated the same as eliminations. 
This option places the primary focus of the examiner's assessment on identification, and treats any inability to make an identification as equivalent to an elimination.

The errors introduced in this section summarize the \emph{overall} error rate of a study. While these errors are informative, most studies provide a deeper insight into different aspects of the examination process by taking additional information into account.
%
In the next two sections we introduce two additional types of error rates: {\it source-specific} and {\it decision-specific} error rates.
Both of these approaches are additional assessments of the reliability of a classification method that leverage \emph{conditional probabilities}, i.e.\ probabilities which are updated to account for known (or assumed) information.
Rather than summarizing an experiment in a single number reflecting error or accuracy like before, these conditional probabilities allow us to provide a more specific assessment of the error accounting for known information, such as the examiner's decision or the source of the evidence. This allows us in particular to calculate separate accuracy rates for comparisons of same-source or different-source evidence. 
There is an additional advantage: some of these conditional probabilities do not require inconclusive results to be explicitly handled as errors or correct decisions. 


\subsection{Source-specific assessment}
The most common way to assess the success (or accuracy)  of a classification method is to calculate its \emph{sensitivity} and \emph{specificity}. 
Sensitivity, also called the true positive rate (TPR), is an examiner's ability to make an identification when examining same-source evidence (SS). 
Sensitivity can be expressed as the conditional probability $P (\text{Identification} \mid SS)$. 
Conversely, specificity, or the true negative rate (TNR), is an examiner's ability to make an elimination given evidence from different sources (DS), $P (\text{Elimination} \mid DS)$. 
We calculate the true negative rate as the conditional probability of the examiner making an elimination \emph{given} the pair under examination is from different sources. 
A conditional probability of an event \emph{given} another event is the probability that the two events both occur, divided by the probability of the second event, specifically for the true negative rate:
$$TNR = P(\text{Elimination} | DS) = \frac{P(\text{Elimination and } DS)}{P(DS)}.$$


Complementary to accuracy rates, we define two error rates of a classification method:
The \emph{false positive rate} (FPR) -- or failed elimination rate -- is the probability that an examination of different source evidence \hh{results in an error}. 
Similarly, the \emph{false negative rate} (FNR) -- or missed identification rate -- is the probability that an examination of same-source evidence \hh{results in an error}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/Source-specific-tree-diagram.pdf}
\caption{Tree Diagram illustrating the logic of source-specific error rates. Information flow happens from left to right: we are assessing an examiner's decision given that someone (not the examiner) knows the source of the evidence. Source-specific rates give the probability of an examiner's decision (Identification, Inconclusive, or Elimination) given same-source and different-source evidence.}\label{fig:FPR-specific-tree}
\end{figure}


%Note that we are keeping the definition of error rates a bit vague on purpose. 
As discussed in \Cref{sec:error-rates}, there are multiple ways to define an error, but the concepts of FPR and FNR hold for all of the  options of treating  inconclusive decisions outlined in \Cref{tab:overall-err-rate}.


In all  of these assessments, the
rates are calculated by conditioning on the \emph{origin of the evidence}. This process is shown graphically in \Cref{fig:FPR-specific-tree} using a decision tree to describe the comparisons which can be made conditioned on ground truth. Calculations for each value are shown in \Cref{tab:fpr-prob}.

\begin{table}\centering
\caption{
Source-specific probabilities, calculated using the quantities introduced in \Cref{tab:generic-results}. 
Cells in the main body of the table show conditional probabilities  of conclusion $Y$ (one of Identification, Inconclusive, or Elimination) given known source  $X$ (same, different). 
The last row in the table contains the marginal probabilities $P(Y)$, that is, the unconditional probability that the examiner's decision is $Y$. 
%In the last column of the table, the marginal probabilities $P(X)$ are shown; 
These marginal probabilities are determined by the experimental design.
}\label{tab:fpr-prob}
\begin{tabularx}{\textwidth}{Y|ccc|Y}
\multicolumn{5}{c}{\bf Source-specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & $a/S$ & $b/S$ & $c/S$ & $S/N$\\
Different source & $d/D$ & $e/D$ & $f/D$ & $D/N$\\\hline
\multicolumn{1}{Y}{Conclusion probability} & $(a + d)/N$ & $(b + e)/N$ & \multicolumn{1}{r}{$(c + f)/N$} & \multicolumn{1}{Y}{1.00}\\\hline\hline
\end{tabularx}
\end{table}
A google sheets workbook which  performs all of these calculations given counts $a, b, c, d, e, \text{ and } f$ is available at \url{http://bit.ly/FTE-error-rate-worksheet}.

In the context of \Cref{tab:fpr-prob}, conditional probabilities can be interpreted as follows:
given a  same-source pair of evidence, the examiner  makes an identification with probability $a/S$, the true positive rate (TPR). 
Conversely, for a  different-source evidence pair, the examiner  makes an elimination with probability $f/D$, the true negative rate (TNR).

\subsection{Decision-specific assessment}

An alternate way to evaluate a classification method is to assess the probability of a same-source or different-source comparison, given that the examiner has  made a  decision (or, more generically, a specific general conclusion).
The \emph{positive predictive value} (PPV) is the probability that a pair of evidence items have the same source, give that the examiner has made an identification, i.e.\ $P(SS | \text{Identification})$. 
Its complement, the \emph{false discovery rate} (FDR), is the probability that the evidence is from different sources, given that the examiner has made an identification, .

Similarly, we can define  predictive values when an examiner has made exclusions:
the \emph{negative predictive value} (NPV) is the probability that evidence is actually from different sources given that the examiner has made an exclusion:  $NPV = P(DS | \text{ Elimination})$. 
Its complement, the \emph{false omission rate} (FOR) is the probability that evidence is actually from the same source\footnote{In \citet{songEstimatingErrorRates2018}, the FDR is referred to as the false identification error rate, and the FOR is called the false exclusion error rate.} given an examiner has made an elimination.
As before, we can map out the conditional hierarchy using a decision tree, shown in \Cref{fig:PPV-specific-tree}. 
To reduce confusion with the naming or rates and predictive values we have labeled nodes in the trees of \Cref{fig:FPR-specific-tree} and \Cref{fig:PPV-specific-tree} corresponding to the rates defined in the text.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/Conclusion-specific-tree-diagram.pdf}
\caption{
Tree Diagram illustrating the logic of positive and negative predictive value calculations. Decision-specific values give the probability of evidence coming from the same source or different sources given an examiner's testimony.
}\label{fig:PPV-specific-tree}
\end{figure}


Using the quantities in \Cref{tab:generic-results}, we can calculate the decision-specific or conclusion-specific probabilities from \Cref{fig:PPV-specific-tree} as shown in \Cref{tab:ppv-prob}.

\begin{table}\centering
\caption{
Decision-specific probabilities, calculated using the quantities introduced in \Cref{tab:generic-results}. 
In each cell in the main body of the table, probabilities shown are the probability of source $X$, if the examiner's conclusion (one of Identification, Inconclusive, or Elimination) is $Y$. 
In the last row in the table, the marginal probabilities $P(Y)$ are shown, that is, the unconditional probability that the examiner's decision is $Y$. 
}\label{tab:ppv-prob}
\begin{tabularx}{\textwidth}{Y|ccc|Y}
\multicolumn{5}{c}{\bf Decision-Specific Probabilities}\\\hline
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & $a/(a+d)$ & $b/(b+e)$ & $c/(c+f)$ & $S/N$\\
Different source & $d/(a+d)$ & $e/(b+e)$ & $f/(c+f)$ & $D/N$\\\hline
\multicolumn{1}{Y}{Decision probability} & $(a + d)/N$ & $(b + e)/N$ & \multicolumn{1}{r}{$(c + f)/N$} & \multicolumn{1}{Y}{1.00}\\\hline\hline
\end{tabularx}
\end{table}


Predictive value assessments are used in assessing the implications of a particular classification label; they inform us about the \emph{likely state of the physical evidence} given the expert's decision or testimony. This makes the PPV, NPV, FDR, and FOR particularly useful when evaluating casework (or in a courtroom setting), because the true state of the evidence is unknown.

Note that we still cannot directly calculate the probabilities for any individual case or evidence: ground truth is still not knowable; however, the results of designed studies are used to give information  tailored to the situation in a particular case. 
While it is reasonable to condition on unknown quantities in a mathematical sense, in practice it is much more informative to condition on quantities with known values: in this case, the examiner's decision is the known information and the state of the physical evidence is unknown.

Conversely, source-specific assessments are useful when evaluating the performance of individuals on a designed test, because ground truth is known (and thus, can be conditioned upon) in designed studies.


\subsection{The practical interpretation of error rates}

The primary difference between the decision tree diagrams in Figures \ref{fig:FPR-specific-tree} and \ref{fig:PPV-specific-tree} is the variable on which we condition. 
The source-specific error rates, FPR and FNR, are calculated by conditioning on the source of the evidence (same-source/different source) and give the probability of an examiner's decision.
PPV and NPV are calculated by conditioning on an examiner's conclusion and give the probability of evidence being same-source or different-source.

From a practical point of view, whether source-specific or decision-specific rates are of interest depends upon where we are in the overall (legal) decision process: 
\begin{itemize}
\item As a firearms and toolmark examiner or as a lab director, we are interested in assessing accuracy based on ground truth, i.e. we are interested in source-specific rates to answer questions such as: given this evidence, are  trained firearms examiner more likely than trainees to arrive at the correct conclusion? (Case studies later show that yes, that is the case.) Alternatively, lab directors might want to know if all of the examiners employed by the lab are performing above a certain threshold.
\item As part of courtroom proceedings, we -- the jury, lawyers, and judges -- are interested in assessing the accuracy of the testimony offered by an expert, i.e. we are interested in conclusion-specific rates to answer questions of the form: given that an expert testified to the evidence as being an identification, what is the probability that the evidence comes from the same source? In this situation, we can update the overall error rate by conditioning on the examiner's conclusion, providing more specific evidence about the scenario under scrutiny in court. As we only know the examiner's testimony (and not ground truth), we must work with the decision-specific rates. 
\end{itemize}
These two forms of questions look deceptively similar but lead to different mathematical formulations. 
This difficulty is not exclusive to evaluations of firearms evidence -- it is a known and pervasive pitfall in other application areas, such as medical diagnostic testing \citep{Casscells:1978ba, Manrai:2014fp}.
The distinction between these two sources of error rates is perhaps a bit more approachable in the setting of a medical context: 
\begin{itemize}
\item A given diagnostic test has an estimated sensitivity (true positive rate, TPR) and a specificity (true negative rate, TNR) that combined determine the overall accuracy rate. The accuracy of the test is established via extensive experimentation. These quantities are useful for an accrediting agency such as the FDA, or a company; both are interested in whether the test works as claimed. 
\item A patient who has received a negative test result is more interested in a different value: the probability that the patient has the disease even though the test was negative. 
\end{itemize}
At the individual level, we want to use tailored information to guide our specific actions; at the system level, we are more interested in gross measures of performance. 

The two conditional probabilities that are referred to in the diagnostic test example are $P(\mbox{Test result} | \mbox{Disease status})$ and $P(\mbox{Disease status} | \mbox{Test result})$. 
Using Bayes' rule, and assuming we also know the prevalence of the disease in the population, we can convert between the two probabilities.
% tells us how to move from the former to the latter, assuming that we have additional information. In the case of the patient taking the test, the patient would also need to know about the baseline prevalence of the disease in her particular environment.
The parallel we draw here is the following:  the examiner's decision is equivalent to a diagnostic test, and the origin of the evidence is equivalent to the disease status. 
In contrast to the medical example, however, we do not have information about the sensitivity and specificity of the field of firearms examination as a whole, let alone about a specific examiner.
For that reason, we need designed studies to estimate the error rates of examinations in situations where ground truth is known.
% Furthermore, to estimate the probability of same or different source given a specific firearm examiner's conclusion, we also need an estimate of the individual examiner's probability of correct conclusion, which can perhaps be obtained via proficiency tests.\svp{XXX I don't think we want to go here... XXX}
As we rely on these studies to generalize to casework, it is of great importance that these studies are well designed so that it is possible to estimate any of the success and error rates discussed above.

\section{Studies}\label{sec:studies}

In this section, we  examine the results from several studies designed to assess the error rates of firearms examination. 
For each study, we provide the reported results and present error rates and conditional probabilities discussed in the previous section along with a visual comparison for each study. 
Using these comparisons we assess the state of these error rate studies and provide an overview of the current status quo of firearms examination.

\subsection{Error Rate Studies}
The studies included in this section are not an exhaustive list of every error rate study ever performed; rather, we have included the most central and most cited studies, as well as several of the studies mentioned in the PCAST and NAS reports concerning firearm and toolmark analysis. A brief description of each study is provided here for reference purposes, with summary tables of results and conditional probabilities provided in \Cref{sec:supplement}.

\begin{description}
\item[Baldwin] \citep{Baldwin:2014bb} is a study of Ruger SR9 cartridge cases. It is an open-set study with 15 comparisons of 3 knowns to 1 questioned cartridge case. Participants received sets with 10 different-source and 5 same-source comparisons.
\item[Keisler] \citep{keisler} is a study of cartridge case comparisons. It is an open set study with 20 comparisons of 1 known to 1 questioned cartridge cases. 126 participants received sets with 12 same-source and 8 different-source comparisons.
\item[Brundage-Hamby]  \citep{Hamby:2018hu} is a closed-set study of 10 consecutively manufactured Ruger P-95 barrels. 507 participants received sets consisting of 10 standards (2 bullets each) and 15 questioned bullets. 
\item[Lyons] \citep{lyons} is a closed-set study of 10 consecutively manufactured Colt 1911A1 extractors. Participants received a set of 10 standards and 12 questioned cases. The design of this study is similar to  \citep{Hamby:2018hu}.
\item[Duez] \citep{Duez:2017kha} is an open-set study of breech face comparisons using virtual microscopy. Each of 56 participants (46 trained, 10 trainees) received two test sets consisting of one standard (3 scans) and 4 questioned scans. In one set, all questioned scans were from the same source as the standard; in the second set, there were two different-source questioned scans and two same-source questioned scans.

\item[\svp{VCMER}] \svp{\citep{vcmer}} is an open-set study of breech face comparisons using virtual microscopy. Each of 76 participants received 16 test sets consisting of one standard (2 scans) and 1 questioned scans. In 17 sets, the questioned bullet matched the standards, and in 23 sets, the questioned bullet was fired from a different weapon.
\item[Bunch \& Murphy] \citep{bunch2003comprehensive} is an open-set study of consecutively manufactured Glock breech faces. In this study, 8 participants received 10 cartridge cases each  and were required to evaluate all possible pairwise comparisons. The number of same-source and different-source cartridge cases varied by test kit, ranging from 10 same-source cartridges to 10 different-source cartridges.
\item[Fadul] \citep{fadul} is a closed-set study of the breech face striations/impressions produced by 10 consecutively manufactured slides. Participants received a set of 10 standards and 15 questioned cases. The design of this study is similar to  \citep{Hamby:2018hu}.
\end{description}

Of the studies listed above, we have examined all but \citet{fadul}, which contained insufficient detail about examiner results and conclusions to calculate the quantities discussed in the previous section. 
Note that this paper is limited in scope to \emph{error rate studies} conducted using firearms and toolmark examiners. 
There are several promising automatic analysis tools \citep[][see]{aoas2,case-validation,chumbleyValidationToolMark2010,chuAutomaticIdentificationBullet2013,songEstimatingErrorRates2018,tai2018fully} which calculate error rates based on 2d images and 3d scans of bullet and cartridge cases.
These types of studies are not easily comparable to the black-box studies under consideration here: none of the automatic matching algorithms employs a category of `inconclusive' as a result, so the discussions on how to deal with inconclusive results are not applicable. That also implies, that for algorithms we can not distinguish between errors stemming from insufficient markings on evidence and errors inherent to the algorithm. %We could, however, estimate the algorithm error conditional on mark quality; this is left to a future study.

In the next sections, we discuss the design and results of each study, including calculation and comparison of source- and decision-specific error rates. 

\subsection{Consequences of Study Designs for Error Rate Estimation}\label{sec:design-consequences}
Our survey of the most commonly cited studies reveals a list of experimental design concerns similar to those identified in the 2017 addendum to the PCAST report \citep{pcast-addendum2017}. 

\begin{displayquote}
As described in the PCAST report, ``set-based" approaches can inflate examiners' performance by allowing them to take advantage of internal dependencies in the data. The most extreme example is the ``closed-set design", in which the correct source of each questioned sample is always present; studies using the closed-set design have underestimated the false-positive and inconclusive rates by more than 100-fold. This striking discrepancy seriously undermines the validity of the results and underscores the need to test methods under appropriate conditions. Other set-based designs also involve internal dependencies that provide hints to examiners, although not to the same extent as closed-set designs.
\end{displayquote}

The PCAST response, issued in  \citeyear{pcast-addendum2017}, identifies only one study \citep{Baldwin:2014bb} as appropriately designed to evaluate the validity and reliability of firearms analysis methods. 
Shortly after the report was issued, two more studies were published which also meet the criteria for reliable studies in the report:  \citet{keisler} and  \citet{Duez:2017kha}. 
All three ``good" studies used slightly different designs but share one essential element: each kit contained multiple comparisons composed of one or more samples from exactly one known source and one unknown source.
These designs substantially reduce or eliminate the internal dependencies which provide ``hints" to examiners by ensuring that each comparison of samples is considered independently.
Also, the design of these studies ensures that it is possible to exactly enumerate the number of same-source and different-source comparisons.\footnote{At least, it is possible with the additional assumption that a comparison between a multiple items from the same source and an unknown should count as a single comparison.}
All three studies are also limited to cartridge case comparisons. We could not identify any studies that assess the error rates of bullet or toolmark examination in a manner that would produce reliable and generalizable measurements.

As a result, studies that use less reliable designs, such as \citet{lyons},  \citet{Hamby:2018hu} (and the similarly designed  \citet{fadul}), and  \citet{bunch2003comprehensive} have historically been frequently referenced in admissibility hearings.
Of these, the study by  \citeauthor{bunch2003comprehensive} is perhaps statistically the most interesting design, but it has been superseded by studies with cleaner experimental designs, such as \citet{Baldwin:2014bb} and \citet{keisler}.
While \citet{bunch2003comprehensive} has internal dependencies due to the inclusion of multiple knowns, and as a result, the number of same-source and different-source comparisons cannot be fully determined, the study does have one desirable feature not found in even the well-designed studies which makes it worth discussing here.
Specifically,  \citeauthor{bunch2003comprehensive} varies the composition of the test kits: no kit had the same composition in terms of same and different-source comparisons. 
This ensures that even if examiners discuss the tests, they cannot gain any additional information from such discussions. 

The primary problem with  \citep{bunch2003comprehensive} is that it includes multiple known exemplars. 
Consequently, it is possible to use logical reasoning to reduce the set of comparisons.
For example, if an examiner is comparing unknown source A to known exemplars from sources 1 to 10, and A matches exemplars from source 2, it is not necessary to make comparisons to sources 3 - 10. 
This design ensures that it is not possible to count up the total number of different-source comparisons performed; as a result, we cannot compute the decision-specific error rates or the source-specific error rates for different-source comparisons. 
Any study which includes multiple known sources in a single comparison set will have this limitation. 
In \ref{sec:bunchmurphysim}, we use statistical simulation to explore the likely number of different-source comparisons performed in  \citep{bunch2003comprehensive}; this assessment includes both the effect of deductive reasoning and the sampling procedure used to create the test kits. 
We also perform a similar assessment for  \citep{Hamby:2018hu} in \ref{sec:brundage}; the design of this study does not include the variability introduced by the test kit assembly procedure used in  \citet{bunch2003comprehensive}. 

While studies that use multiple known sources have been conducted for more than 25 years, such studies are inherently flawed. 
These studies have an inherent bias, because they provide evidence that examiners can make identifications accurately (and do not make many false eliminations), but do not allow a quantification of the probability that examiners make eliminations correctly. 
As a result, when error rates derived from these studies are cited, they do not include errors that result from different source comparisons, which prevents evaluation of examiners on their ability to distinguish between different sources.
This fundamentally biases these studies so that in court they provide useful (but misleading) information to the prosecution while offering nothing useful to the defense.
That is, the underlying structure of these studies only provides information about how accurate examiners are when providing evidence against the defendant, producing a systematic bias when these rates are presented in court.

The design used in  \citet{Hamby:2018hu} was originally found in \citet{brundagethesis}, and dates back to  \citeyear{brundagethesis}. Its longevity makes it the most extensive study in forensic error rates in firearms (and possibly across pattern disciplines). 
Unfortunately, the study is flawed in two major respects: it uses multiple known sources, and it is a closed set study. 
The multiple known source problem was described above, but this problem is magnified when combined with the closed-set study design. 

In a closed-set design, the structure of the study helps examiners make the correct conclusion: if unknown source A is most similar to exemplars from source 8, then after the 10 comparisons are done, the examiner would rationally make an identification for source 8. 
In an open-set study, the examiner might rate the same pairwise comparison as inconclusive, because there is no additional information suggesting that the unknown must match one of the provided knowns.
In both the multiple known and closed set study designs, the use of deductive reasoning artificially reduces the error rates calculated using the study results, as discussed in the PCAST report and addendum \citep{pcast2016,pcast-addendum2017}.
    
It might be possible to modify the answer sheet so that examiners record each pairwise comparison, which would allow for assessment of the number of different-source comparisons performed. 
However, revising the answer sheet would not prevent the use of deductive reasoning, which gives examiners an advantage in error rate studies that does not exist in casework.
Currently, studies patterned after  \citet{brundagethesis} and \citet{Hamby:2018hu} use an answer sheet where examiners are only asked to report which of the 10 knowns the questioned bullet matches. 
Because the study does not ask examiners to report eliminations, there is no way to assess elimination error rates or the overall error rate consisting of both missed identifications and false eliminations.

In addition to the design issues which plague the closed-set studies, the execution of some of these studies makes the data even less reliable. 
Methodological issues, such as those described in  \citet{lyons}, where test sheets were returned to participants who misunderstood instructions, cast even more doubt on the utility of error rates derived from such studies and their ability to generalize to casework. 

Finally, some design constraints are common to most studies. 
Of the studies assessed in this paper, only  \citet{bunch2003comprehensive} uses a variable proportion of same and different source comparisons; without varying this percentage, examiners could compare notes and gain additional information about the study results. 
Many studies \citep[but not all, ][]{Baldwin:2014bb} pre-screened kit components to ensure that sufficient markings were present to allow for identification - this differs from casework and artificially inflates the rate of successful identifications and eliminations
\footnote{

In Baldwin  \citep{Baldwin:2014bb}, the only pre-screening of usability was to ensure cartridge cases were caught with the cartridge catching device. 
Additionally, for 3234 comparisons, FTEs evaluated how many of the known cartridge cases were usable for an evaluation: all three specimens were used in 3018 cases, two were used in 207 cases, and only one was used in nine cases.
The Baldwin study is fairly unique in this regard: most studies make some effort to control the quality of the evidence sent out for assessment.
}. 
Studies that tightly control the evidence quality and presence of individualizing marks would under-estimate the process error (though the estimates of examiner error are not necessarily affected).
In almost all studies, participants use lab rules for determining whether eliminations could be made on individual characteristics; while  \citet{Baldwin:2014bb} instructed participants to use a uniform set of rules, many participants reported that they did not or could not adhere to these guidelines.


<<data-setup, include = F>>=
clopper <- function(alpha, success, trials) {
  lower <- qbeta(alpha/2, success, trials-success+1)
  upper <- qbeta(1-alpha/2, success+1, trials-success)
  c(lower, upper)
}

library(tidyverse) 
studies <- read.csv("data/studies.csv")
studies <- studies %>% mutate(
    Study = factor(Study, 
                    levels=c("Baldwin (2014)", "Keisler (2018)", "Duez (2018)", "VCMER (2020)", "Mattijssen (2020)", "Bunch & Murphy (2003)", "Hamby (2019)", "Lyons (2009)"))
  )

cis <- studies %>% group_by(Study, Decision) %>% 
  filter(Type == "Observed") %>%
  summarize(
    pred_ss = sum(Number[Ground.truth=="Same Source"])/sum(Number),
    pred_ss_lower = clopper(0.05, sum(Number[Ground.truth=="Same Source"]), sum(Number))[1],
    pred_ss_upper = clopper(0.05, sum(Number[Ground.truth=="Same Source"]), sum(Number))[2]
  )

source_spec_cis <- studies %>% group_by(Study, Ground.truth) %>% 
  filter(Type == "Observed") %>%
  mutate(
    pred_ss = Number/sum(Number),
    pred_ss_lower = map2_dbl(Number, sum(Number), ~clopper(0.05, .x, .y)[1]),
    pred_ss_upper = map2_dbl(Number, sum(Number), ~clopper(0.05, .x, .y)[2])
  )

exp <- studies %>% 
#  filter(Decision == "Inconclusive") %>%
  group_by(Study, Decision) %>% 
  summarize(
    exp_ss = Number[Type=="Expected"][1] * sum(Number[Type == "Observed"]),
    exp_ss_lower = clopper(0.05, exp_ss, sum(Number[Type == "Observed"]))[1],
    exp_ss_upper = clopper(0.05, exp_ss, sum(Number[Type == "Observed"]))[2],
    exp_prob = exp_ss/sum(Number[Type == "Observed"])
  ) 
  
DontKnow_source_spec <- tibble(
  pred_ss = 0.5, 
  xmin=c(0,0,0,0,0,0), 
  xmax=c(1,1,1,1,1,1), 
  Study = c("Lyons (2009)", "Hamby (2019)", "Lyons (2009)", "Hamby (2019)", "Lyons (2009)", "Hamby (2019)"), 
  Decision=rep(c("Identification", "Inconclusive", "Elimination"), each=2),
  Ground.truth = "Different Source") %>%
  dplyr::mutate(Decision = factor(Decision, levels=c("Identification", "Inconclusive", "Elimination")))

DontKnow <- data.frame(
  pred_ss = 0.5, 
  xmin=c(0,0,0,0), 
  xmax=c(1,1,1,1), 
  Study = c("Lyons (2009)", "Hamby (2019)", "Lyons (2009)", "Hamby (2019)"), 
  Decision=rep(c("Inconclusive", "Elimination"), each=2))

errors <- studies %>% 
  mutate( # note, this is defining CORRECT decisions
    AFTE = (Ground.truth=="Same Source" & Decision != "Elimination") | 
      (Ground.truth=="Different Source" & Decision != "Identification"),
    Process = ((Ground.truth=="Same Source" & Decision == "Identification") | 
      (Ground.truth=="Different Source" & Decision == "Elimination")),
    Elimination = ((Ground.truth=="Same Source" & Decision == "Identification") | 
      (Ground.truth=="Different Source" & Decision %in% c("Inconclusive", "Elimination")))
         ) %>%
  group_by(Study, Ground.truth) %>% 
  filter(Type == "Observed") %>%
  summarize(
    Comparisons = sum(Number),
    AFTE = sum(Number[!AFTE]),
    Process = sum(Number[!Process]),
    Elimination = sum(Number[!Elimination]),
    .groups = "drop_last"
  ) %>% 
  mutate(
    Error = c("Missed Elimination", "Missed Identification")[as.numeric(Ground.truth=="Same Source")+1]
  )
@

\subsection{Assessment of Classification Error}

\Cref{fig:errors} shows the false positive and false negative rates for the six studies  investigated in this paper. 
Both AFTE errors (which do not consider inconclusive decisions as errors and represent examiner errors) and process errors (which include inconclusive decisions and represent errors attributable to evidence quality and lab policy) are shown; examiner error rates are low for both missed elimination and missed identifications in most studies (though intervals reveal the effect of sample size, which was also noted in e.g.  \citet{pcast-addendum2017}. 
Process errors, however, occur at higher rates and are particularly concerning for missed eliminations. 
There is consistently a much larger discrepancy between the two error rates for eliminations than for identifications; this phenomenon is examined in more detail in \Cref{sec:inconclusives}.
Only a portion of the discrepancy between the examiner (AFTE) and process error rates for missed eliminations can be explained by lab policies that only allow eliminations based on mismatches in class characteristics. 


The Bunch \& Murphy study, for instance, was conducted at the FBI, where all examiners followed the same lab policy precluding eliminations based on class characteristic mismatches; as a result, the study has the largest difference between the two missed elimination rates.

Even though  \citet{Baldwin:2014bb} made efforts to control for the effects of lab policy in determination of inconclusive findings, the efforts were not completely successful.
This is despite instructions given to participants: ``A very important aspect of this work that needs to be clearly understood is that the study specifically asked participants not to use their laboratory or agency peer review process... Some [participants] indicated that the design of our study with all cartridges fired from the same model of firearm using the same type of ammunition would prohibit the use of a finding of elimination, while others used a mixture of inconclusive and elimination or did not use inconclusive at all to indicate a finding other than identification."

In  \citet{Duez:2017kha}, the authors report that ``13\% of examiners are not permitted to eliminate on individual characteristics (therefore, their conclusions of inconclusive are perfectly acceptable)." 
Interestingly, it seems that some examiners reported eliminations in contravention of lab rules.
Policies that forbid elimination based on a mismatch between individual characteristics alone seem to directly contradict the AFTE Theory of Identification, which allows for eliminations based on ``significant disagreement of discernible class characteristics and/or individual characteristics".


The rates reported in this section are calculated directly from information reported in the papers without adjustments accounting for the use of deductive reasoning; as such, they represent the best possible case for error rates in studies that include multiple knowns, such as  \citet{Hamby:2018hu} and  \citet{bunch2003comprehensive}. 
More realistic estimates of error rates that account for the use of deductive reasoning can be found in \ref{sec:sim}.


<<errors, echo=FALSE, fig.width=8, fig.height=5,fig.cap="Percentages of missed eliminations and missed identifications by study. 95\\% Pearson-Clopper confidence intervals are drawn around the error estimates. Missed eliminations cannot be calculated for closed-set studies.", out.width=".8\\textwidth">>=

gg <- errors %>% gather(Type, Number, AFTE, Process, Elimination) %>% 
  mutate(
    Type = factor(Type, levels=c("AFTE", "Process", "Elimination"))
    ) 
levels(gg$Type) <- c(
  "AFTE error\n(Option 2)",
  "Process error\n(Option 3)",
  "Inconclusives are\nEliminations (Option 4)")

gg <- gg %>% group_by(Type, Study, Error) %>%
  mutate(
    err_lower = clopper(0.05, Number, Comparisons)[1]*100,
    err_upper = clopper(0.05, Number, Comparisons)[2]*100,
    pos = c("white", "grey")[1 + as.numeric(Study) %% 2]
  ) 

gg %>%
  ggplot(aes(x = Number/Comparisons*100, y = Study)) +
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin =1 - 0.3, ymax = 1 + 0.6), fill = "grey90", alpha = .25, inherit.aes = F) + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin =3 - 0.3, ymax = 3 + 0.6), fill = "grey90", alpha = .25, inherit.aes = F) + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin =5 - 0.3, ymax = 5 + 0.6), fill = "grey90", alpha = .25, inherit.aes = F) + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin =7 - 0.3, ymax = 7 + 0.6), fill = "grey90", alpha = .25, inherit.aes = F) + 
  geom_point(aes(shape=Type, colour = Type, y = as.numeric(Study)+0.3*(as.numeric(Type)-1.5)) ) +
  geom_point(aes(x=35, y = as.numeric(Study)),
                 data = filter(gg, is.na(Comparisons)), shape='?', size = 3.5,
                 colour = "grey20") +
  facet_grid(Error~.) + #, space="free") +
  theme_bw() +
  xlab("(Source-specific) Error Percentages") +
  xlim(c(0, NA)) +
  geom_errorbarh(aes(xmin=err_lower, xmax=err_upper, colour = Type, y=as.numeric(Study)+0.3*(as.numeric(Type)-1.5)),
                 size=0.5, height = 0.3,  alpha =0.9) +
  scale_y_continuous(breaks=1:length(levels(errors$Study)), labels=levels(errors$Study), minor_breaks = NULL) +
  scale_colour_brewer(palette="Dark2") +
  theme(legend.position = "bottom", legend.direction = "horizontal")

  #+
  # guides(colour=guide_legend(ncol=1))
@



% \svp{Technically these aren't really overall error rates, but they are the most commonly reported.}



\subsection{Source-specific error rates}

The false-positive rate and false-negative rate reported in \Cref{fig:errors} are the error rates typically used to characterize a classification method. 
These rates, however, are functions of a larger set of probabilities which are calculated conditional on the known source of the evidence. 
In \Cref{fig:source-spec-cis}, we see the full set of source-specific conditional probabilities: the probability that an examiner will make an identification, elimination, or inconclusive determination given that the source of the evidence is different (top panel) or the same (bottom panel). 
Probabilities are shown with 95\% Pearson-Clopper confidence intervals, which provide a visual indication of the estimate's variability.
It is apparent that examiners are extremely good at working with same-source evidence: there are relatively few inconclusive determinations, almost no missed identifications, and very few false eliminations. 
It is also apparent that examiners have much more difficulty when examining evidence that arises from different sources.  As an example, examiners who participated in the Bunch and Murphy study \citep{bunch2003comprehensive} had a significantly higher probability of reaching an inconclusive conclusion than an elimination when evidence items had a different source.
Closed-set designs (used in  \citet{Hamby:2018hu} and  \citet{lyons}) do not even allow for the estimation of different-source probabilities because it is not possible to estimate the number of different source comparisons which are made; this inherent bias has the unintentional effect of masking the fact that examiners are not as accurate when evaluating different-source comparisons. 
While in most studies (except Bunch \& Murphy), the probability of making an inconclusive determination for different source evidence is below the probability of making an elimination, the probability that examiners will correctly make an elimination is still extremely low relative to the probability of a correct decision in the evaluation of same source evidence.
It may be that examiners are trained to look for similarities, instead of differences, or that it is simply more difficult to classify a difference as opposed to a similarity; further, some labs inexplicably forbid eliminations  unless there is a class characteristic mismatch.
Regardless of the explanation, it is apparent that when presented with a different source comparison, examiners make an elimination much less frequently than an identification when presented with a same-source comparison. 

<<source-spec-cis, echo=FALSE, fig.width=8, fig.height=5, fig.cap="Pearson-Clopper 95\\% confidence intervals for the probability of an examiner's conclusion given source of the evidence. No assessments can be made about different-source specific probabilities when the total number of comparisons cannot be determined. The probability of inconclusive determinations on different source evidence is quite high, much higher than for same source evidence, where the probability for an inconclusive result is close to zero.", out.width=".8\\textwidth">>=

figdata <- source_spec_cis %>% 
  ungroup() %>%
  mutate(
    Decision = factor(Decision, levels=c("Identification", "Inconclusive", "Elimination"))
  ) 

figdata %>%
  ggplot(aes(y = pred_ss, x = Study, colour = Decision, shape = Decision)) +
  facet_grid(Ground.truth~.) +
  ylim(c(0,1)) +
  ylab("Probability of examiner decision given source") +
  theme_bw() +
  geom_hline(yintercept=c(0,1), colour = "grey20", size=0.25) +
  geom_errorbar(position = position_dodge(width = 0.6),
    aes(ymin = pred_ss_lower, ymax=pred_ss_upper, x=Study, group = Decision), 
    size=0.5, width = 0.5, alpha =0.9) +
  geom_point(position = position_dodge(width = 0.6), ) +
  scale_shape(solid = F) + 
  geom_errorbar(aes(ymin=xmin, ymax=xmax, x=Study), 
                 data = DontKnow_source_spec, linetype=2, width = 0.5, colour = "grey20") +
  geom_point(data = DontKnow_source_spec, shape='?', size = 3.5, colour = "grey20") + 
  coord_flip() + 
  theme(legend.position = "bottom") +
  scale_colour_brewer(palette="Dark2") +
   geom_col(aes(x = Study, y = I(1)), width = 1, colour = NA,
          alpha = 0.1, fill = "black", linetype = "solid",
          data = figdata %>%
 filter(Study %in% c("Lyons (2009)", "Bunch & Murphy (2003)", "Keisler (2018)", "VCMER (2020)"))) 
@

\subsection{Decision-specific error rates}\label{sec:dec-spec-res}


In courtroom testimony, we do not know ground truth - we do not know whether the comparison that is presented is from the same source or from different sources. 
As a result, we cannot use the source-specific error rates or probabilities in \Cref{fig:source-spec-cis} to justify the testimony and say something like ``the probability of an identification given that the evidence is from the same source is nearly 1".
In these situations, the ``data'' we have is the examiner's decision, so we wish to compute the reverse conditional probability: the probability of same-source evidence given that the examiner made an identification.
\Cref{fig:cis} shows the probability of same source evidence conditional on the examiner's assessment for five published studies. 
Note that because there are two possible sources for the evidence (same or different), the equivalent probabilities for different-source evidence can be obtained by subtracting the same-source probability from 1.


As before, the probabilities computed from the study results are shown with their respective 95\% (Pearson-Clopper) confidence intervals. 
The circles shown in light grey in the middle panel show the expected proportion of inconclusive results which are from the same source. 
The calculation of these expected values will be discussed in more detail in the next section.

<<cis, echo=FALSE, fig.width=8, fig.height=6, fig.cap="Pearson-Clopper 95\\% confidence intervals for the probability of same source evidence given an examiner's conclusion. Expected values for each of the probabilities as discussed in \\Cref{sec:inconclusives} are shown as grey targets. Some of the studies do not allow an assessment of all of the predictive values.", out.width=".8\\textwidth">>=

cis %>% 
  ungroup() %>%
  mutate(
    Decision = factor(Decision, levels=c("Identification", "Inconclusive", "Elimination"))
  ) %>%
  ggplot(aes(x = pred_ss, y = Study)) +
  facet_grid(Decision~.) +
  xlim(c(0,1)) +
  xlab("Probability for same source given examiner's decision") +
  theme_bw() +
#  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="red", size=4.5) +
#  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="white", size=3.5) +
  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="grey50", size=2.5) +
  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="white", size = 1.5) +
  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="grey50", size=.5) +
    geom_vline(xintercept=c(0,1), colour = "grey20", size=0.25) +
  geom_errorbarh(
    aes(xmin = pred_ss_lower, xmax=pred_ss_upper, y=Study), 
    size=0.5, height = 0.5, colour = "grey20", alpha =0.9) +
  geom_point() +
  geom_errorbarh(aes(xmin=xmin, xmax=xmax, y=Study), 
                 data = DontKnow, linetype=2) +
  geom_point(data = DontKnow, shape='?', size = 3.5)


@


In \Cref{fig:cis}, it is  clear that the probability of same-source evidence given an inconclusive evaluation is much lower than the calculated target probability.
The decision-specific probabilities provide indications that the distribution of inconclusive determinations is very different from our expectations. 

\section{Inconclusive Evaluations, Errors, and the Legal System}\label{sec:inconclusives}
Before we discuss the results of inconclusive findings in the selected studies, we must explicitly characterize our expectations for decision-specific probabilities of inconclusive findings. 
That is, given that an examiner states that a finding is inconclusive, what is our expectation for the probability that the evidence is from the same source or from different sources?
In the absence of any additional information, we have no reason to believe that a finding of inconclusive should be related to either same source or different source origin. 
An inconclusive finding, according to the AFTE rules, indicates that there is insufficient agreement or disagreement between discernible individual features to make an identification or elimination. 
This suggests that there is an implicit threshold of the amount of evidence necessary to arrive at a definitive conclusion \citep{Dror:2018fp}.
We have no reason to believe that these thresholds are asymmetric; that is, the threshold to go from inconclusive to identification should be the same as the threshold to go from inconclusive to elimination. 
Statistically, we would express this symmetry as the independence between the source and an inconclusive finding: that is, $P(\text{same source} | \text{inconclusive}) = P(\text{same source})$ and $P(\text{different source} | \text{inconclusive}) = P(\text{different source})$.


This implies that we can calculate the ``target" probability for each decision: the target for identification is 100\%: all same source pairs should result in an identification. 
Similarly, the target for elimination is 0\%: not even a single same-source pair should result in an elimination. 
Same-source pairs should be assessed as inconclusive with frequency proportional to the number of same-source comparisons in the study. 
Conceptually, \Cref{fig:shape-overview} demonstrates this characteristic: the rate of inconclusives is the same for both same-source and different-source evidence.
That is, under relatively ideal conditions (where there are inconclusive evaluations, but no errors according to the AFTE process), the expected conditional probabilities in an experiment would match those shown in \Cref{tab:ideal-probabilities}. 

\begin{table}
\caption{Expected conditional probabilities in a world where examiners do not make mistakes but do make inconclusive decisions. }\label{tab:ideal-probabilities}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & $P(SS|\text{Identification}) = 1 $& $P(SS|\text{Inconclusive}) = P(SS)$ & $P(SS | \text{Elimination}) = 0$ \\
Different source & $P(DS| \text{Identification}) = 0$ & $P(DS| \text{Inconclusive}) = P(DS)$ & $P(DS | \text{Elimination})= 1$ \\\hline
\end{tabu}
}
\end{table}

In case work we are not able to assess the probability of same source or different source comparisons.  
However, those probabilities are easily accessible in formal studies or blinded proficiency testing, as $P(SS)$ and $P(DS)$ in these studies are determined by the experimental design.

As an example, in the Baldwin study  \citep{Baldwin:2014bb}, we would expect the probabilities for same and different source given an inconclusive result to be: 
\begin{eqnarray*}
P(\text{same source} \mid \text{Inconclusive}) &=& P(\text{same source}) = \frac{1}{3},  \\
P(\text{different source} \mid \text{Inconclusive}) &=& P(\text{different source}) = \frac{2}{3}
\end{eqnarray*}
This expectation is based on the study's design: 5 of the 15 comparisons were among same-source pairs and 10 were among different-source pairs. 
However, in Baldwin, as in the other studies, the probability of same-source evidence given an inconclusive is much lower than the expectation.


<<inconclusives, echo=FALSE, fig.width=8, fig.height=3.75,fig.cap="Probability of inconlusive decisions by same-source/different source comparisons. The probability of inconclusive determinations for same source comparisons is generally low (with a 95\\% confidence interval well below 5\\%). Different source comparisons end up in inconclusive results significantly more often. The Bunch \\& Murphy and Baldwin studies have a particularly high percentage of inconclusive results.", out.width=".8\\textwidth", eval = F, include = F>>=

inc <- studies %>% group_by(Study, Ground.truth) %>% 
  filter(Type == "Observed") %>%
  summarize(
    Comparisons = sum(Number),
    Inconclusive = sum(Number[Decision=="Inconclusive"]),
    inc_lower = clopper(0.05, Inconclusive, Comparisons)[1],
    inc_upper = clopper(0.05, Inconclusive, Comparisons)[2]
  )

max_prob <- inc %>% ungroup() %>% filter(Ground.truth == "Different Source") %>% summarize(prop = max(inc_upper, na.rm = T)) %>% as.numeric()

DontKnow <- data.frame(
  x = max_prob/2, 
  xmin=c(0,0,0,0), 
  xmax=max_prob*c(1,1,1,1), 
  Study = c("Lyons (2009)", "Hamby (2019)", "Lyons (2009)", "Hamby (2019)"), 
  Ground.truth="Different Source")

inc %>% 
  ggplot(aes(x = Inconclusive/Comparisons, y = Study)) +
  geom_errorbarh(aes(xmin = inc_lower, xmax=inc_upper, y=Study),
                 size=0.5, height = 0.5, colour = "grey20", alpha =0.9) +
  geom_point() +
  theme_bw() +
  facet_grid(Ground.truth~.) +
  xlab("Conditional probability of inconclusive decisions") +
  geom_errorbarh(aes(x = 0, xmin=xmin, xmax=xmax, y=Study),
                 data = DontKnow, linetype=2,
                 size=0.5, height = 0.5, colour = "grey20", alpha =0.9) +
  geom_point(x = max_prob/2,  data = DontKnow, shape='?', size = 3.5)
@


This finding is similar to the implications of \citet{biedermannAreInconclusiveDecisions2019}, though the authors do not follow their reported results to the logical conclusion. 
Specifically, they show that there is information in the finding of an inconclusive that is useful: inconclusive determinations are  more likely to occur when the evidence originates from different sources. 
In the studies we examined, the vast majority of paired comparisons for which source could not be conclusively established by the participant should have been eliminations. Therefore, we propose the following: 
%We take this conclusion one step further: 
if inconclusive determinations are essentially another way to say ``different source", then it makes sense to 
% get rid of the category and declare 
 collapse inconclusive evaluations and eliminations, at least when calculating error rates. 

Calculations are shown below for \citet{Baldwin:2014bb}:
%
\begin{align}\label{eqn:baldwin-calcs}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{rcccr}
P (\text{same source} \mid \text{elimination}) &=& 4 / 1425 &=& 0.0028 \\
P (\text{same source} \mid \text{inconclusive or elimination}) &=& (11+4) / (748+1425) &=& 0.0069 \\
%P (\text{different source} \mid \text{inconclusive or elimination}) &=& \displaystyle\frac{737+1421}{748+1425} &=& 0.9931 \\ % This isn't mentioned in the text description - I suspect the calc shown below was possibly meant instead?
P (\text{different source} \mid \text{identification}) &=& 22 / 1097 &=& 0.0201
\end{array}
\end{align}

and also for \citet{mattijssenValidityReliabilityForensic2020}:
%
\begin{align}\label{eqn:mattijssen-calcs}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{rcccr}
P (\text{same source} \mid \text{elimination}) &=& 95 / 879 &=& 0.1081 \\
P (\text{same source} \mid \text{inconclusive or elimination}) &=& (487+95) / (1302+879) &=& 0.2669 \\
%P (\text{different source} \mid \text{inconclusive or elimination}) &=& \displaystyle\frac{737+1421}{748+1425} &=& 0.9931 \\ % This isn't mentioned in the text description - I suspect the calc shown below was possibly meant instead?
P (\text{different source} \mid \text{identification}) &=& 74 / 2439 &=& 0.0303
\end{array}
\end{align}


Examining the experimentally determined error rates for firearms and toolmark identification leads to a disturbing realization. 
There is an order of magnitude difference between the probability of same-source evidence given an elimination and the probability of different-source evidence given an identification. 
In fact, even if inconclusives and eliminations are combined as under Option 4 the probability of a false elimination is 0.0069, only a third of the probability of a false identification.
These observations are shown visually in \Cref{fig:asymmetric-errors} and explicitly calculated in \Cref{eqn:baldwin-calcs,eqn:mattijssen-calcs} for studies with sufficient error counts \citep{Baldwin:2014bb,mattijssenValidityReliabilityForensic2020}. 
It is necessary to have at least one observation in each cell in order to bound the probabilities as well as sufficient comparisons to be able to differentiate between e.g. 0.02 and 0.2; this is particularly important when we deal with very low and very high probabilities, such as success and error rates. 

<<asymmetric-errors, echo = F, fig.width = 8, fig.height = 3, fig.cap = "Decision-specific error rates. For all AFTE-decision criteria studies (excluding \\citet{mattijssenValidityReliabilityForensic2020}, which used likelihood ratios as a primary criteria), errors in identification are more likely than errors in eliminations. In \\citet{Baldwin:2014bb} and \\citet{mattijssenValidityReliabilityForensic2020}, which are the only studies with a large number of participants and evaluations, confidence intervals for wrong identifications do not overlap, indicating that  these errors are significantly higher than wrongful eliminations.", cache = F>>=
# Even if inconclusives are treated as eliminations, error rates are still not higher for eliminations than identifications (removed from caption)

# This was to create a similar chart as the calculations for Baldwin, but Baldwin is the only study with enough data for the calculations to be meaningful...
elimplus <- studies %>%
  dplyr::filter(Decision != "Identification", 
                Type == "Observed") %>%
  group_by(Study, Ground.truth) %>%
  summarize(
    Decision = "Elimination + Inconclusive", 
    Number = sum(Number)
    )


errs <- data.frame(
  Ground.truth = c("Same Source", "Same Source", "Different Source"),
  Decision = c("Elimination", "Elimination + Inconclusive", "Identification"),
  stringsAsFactors = FALSE
)

# Decision-specific
errors2 <- studies %>% 
  filter(Type == "Observed") %>%
  select(-Type) %>%
  bind_rows(elimplus) %>%
  group_by(Study, Decision) %>%
  mutate(Total = sum(Number)) %>%
  right_join(errs) %>%
  mutate(
    rate = Number/Total,
    lower = clopper(0.05, Number, Total)[1],
    upper = clopper(0.05, Number, Total)[2]
    ) %>%
  ungroup() 

errors2 %>%
  arrange(Study) %>%
  mutate(
    Decision = factor(Decision, levels = c("Identification", "Elimination", "Elimination + Inconclusive")),
    ErrorType = factor(c("Identification of DS", "Elimination of SS", "Elimination or Inconclusive of SS")[as.numeric(Decision)]),
    ErrorType = factor(ErrorType, levels = c("Identification of DS", "Elimination of SS", "Elimination or Inconclusive of SS"))
  ) %>%
  filter(!(Study %in% c("Hamby (2019)", "Lyons (2009)", 
                        "Bunch & Murphy (2003)"))) %>%
  ggplot(aes(x = rate*100, y = Study)) +
  geom_hline(yintercept = "Mattijssen (2020)", colour = "black",
             size=16, alpha = 0.1) +
  geom_hline(yintercept = "Duez (2018)", colour = "black",
             size=16, alpha = 0.1) +
  geom_hline(yintercept = "Baldwin (2014)", colour = "black",
             size=16, alpha = 0.1) +
  geom_point(aes(colour = ErrorType, shape = ErrorType),
             position = position_dodge(width = 0.7),
             size = 2.5) + 
  geom_errorbarh(
    aes(xmin = lower*100, xmax = upper*100, 
        colour = ErrorType), 
    position = position_dodge(width = 0.7)) +
  scale_shape_discrete("Type of Error") +
  scale_colour_manual("Type of Error", values = c("steelblue", "darkorange", "brown")) +
  theme_bw() +
  xlab("Error Percentages (decision-specific conditional probabilities) ") 

@



<<baldwin-shape-overview, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas) as reported in \\citet{Baldwin:2014bb}. Each dot represents approximately 2 examiner evaluations. Unlike in \\Cref{fig:shape-overview}, there is a significant difference in the probability of an inconclusive evaluation when a pair is different source or same source. ", fig.width = 6, fig.height = 4, dpi = 1000, dev="pdf", out.width = "70%">>=
baldwin <- c(a = 1075, b = 11, c = 4, d = 22, e = 737, f = 1421)

do.call(target_plot, as.list(c(round(baldwin/2), pointsize = .5, bordersize = 0.001)))
# target_plot(1075, 11, 4, 22, 737, 1421, pointsize = .5, bordersize = .001) # too dense
@
<<keisler-shape-overview, eval = T, include = F, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas) as reported in \\citet{Baldwin:2014bb}. Each dot represents approximately 2 examiner evaluations. Unlike in \\Cref{fig:shape-overview}, there is a significant difference in the probability of an inconclusive evaluation when a pair is different source or same source. ", fig.width = 6, fig.height = 4, dpi = 1000, dev="pdf", out.width = "70%">>=
keisler <- c(a = 1508, b = 4, c = 0, d = 0, e = 203, f = 805)

do.call(target_plot, as.list(c(keisler, pointsize = .5, bordersize = 0.001)))
# target_plot(1075, 11, 4, 22, 737, 1421, pointsize = .5, bordersize = .001) # too dense
@
<<duez-shape-overview, eval = T, include = F, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas) as reported in \\citet{Baldwin:2014bb}. Each dot represents approximately 2 examiner evaluations. Unlike in \\Cref{fig:shape-overview}, there is a significant difference in the probability of an inconclusive evaluation when a pair is different source or same source. ", fig.width = 6, fig.height = 4, dpi = 1000, dev="pdf", out.width = "70%">>=
duez <- c(a = 276+59, b = 1, c = 0, d = 2, e = 25, f = 85)

do.call(target_plot, as.list(c(duez, pointsize = .5, bordersize = 0.001)))
# target_plot(1075, 11, 4, 22, 737, 1421, pointsize = .5, bordersize = .001) # too dense
@
<<bunch-shape-overview, eval = T, include = F, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas) as reported in \\citet{Baldwin:2014bb}. Each dot represents approximately 2 examiner evaluations. Unlike in \\Cref{fig:shape-overview}, there is a significant difference in the probability of an inconclusive evaluation when a pair is different source or same source. ", fig.width = 6, fig.height = 4, dpi = 1000, dev="pdf", out.width = "70%">>=
bunch <- c(a = 70, b = 0, c = 0, d = 0, e = 172, f = 118)

do.call(target_plot, as.list(c(bunch, pointsize = .5, bordersize = 0.001)))
# target_plot(1075, 11, 4, 22, 737, 1421, pointsize = .5, bordersize = .001) # too dense
@


\Cref{fig:baldwin-shape-overview} provides a visual demonstration of this discrepancy: unlike in \Cref{fig:shape-overview}, there is a difference in the density of points in the inconclusive sections of the chart. Note that while the probability of same source given an elimination is affected by lab policies towards the treatment of inconclusives, these policies do not explain the continued discrepancy in error probabilities when inconclusives and eliminations are considered together.



% \subsection{What will examiners bet on?}
% \subsection{Equitable Errors?} % Something that isn't quite as cavalier as "betting" and gets at the heart of the question better - which error is more likely?

% Consider taking out this entire paragraph.  Not that I do not agree, I do.  But it is more preachy than scientific, and really does not add much.
%Under a system with equal consequences for each type of error (false elimination or missed identification), we would ideally want the probability of each type of error to be the same - that is, it should be equally likely to make an error in favor of the prosecution or in favor of the defense. 
%In the legal system, however, there are not equal consequences for each type of error. 
%If a person is guilty, there are usually multiple pieces of evidence which contribute information used by the judge or jury to make a decision; thus, the consequences of an elimination of same-source evidence are generally relatively low. 
%However, there are many examples of convictions which were made based on faulty forensic evidence and led to the unjust imprisonment or execution of individuals who were innocent. 
%As a society, we generally adhere to the principle that false convictions are more heinous than allowing a guilty person to go free.  
%This principle, known as Blackstone's ratio (``It is better that ten guilty persons escape than that one innocent suffer.") would suggest that equal probability of each type of error is insufficient. 
%In accordance with this principle, it would be preferable for $P(\text{SS} | \text{Elimination})$ to be higher than $P(\text{DS} | \text{Identification})$. 


%Unfortunately, this is not what we find.
\Cref{fig:asymmetric-errors} focuses on the larger studies \svp{with designs which allow for estimation of these error rates}, and shows the estimated probabilities (and the corresponding confidence intervals) of an incorrect identifications and eliminations.
We see that in the three studies with a large enough sample size, the point estimate of the probability of a false identification is higher than the probability of a false elimination. In the case of the \citet{Baldwin:2014bb}, the difference is statistically significant.
Even when inconclusives are treated as as eliminations, the same pattern persists.
%the only study with sufficient sample size to show significant differences between error rates still shows that the probability of different source evidence given an examiner identification is significantly higher than the probability of same-source evidence given an examiner elimination or inconclusive.
%Note that \citet{Duez:2017kha} and \citet{keisler} have insufficient sample size to distinguish between the probabilities with statistical significance.
%However, 
While the trend is the same in the three studies, \citet{Baldwin:2014bb} suggests that in the absence of definitive information, examiners tend to more often conclude identification than elimination.
%are willing to gamble on an identification much more than they are willing to gamble on an elimination. 
In the courtroom (or before, in a plea bargain situation), this results in a  bias in favor of the prosecution (or against the defense).

% Again, consider eliminating this paragraph?
There are many ways this bias could arise: there are, of course, well documented motivational and cognitive biases \citep{motivationalcognitivebiases, robertson2016blinding} which may affect examiners.
In addition, it may be easier to identify similarities than to explicitly identify dissimilarities \citep{bagnaraSymmetrySimilarityEffects1983, ashbyUnifiedTheorySimilarity1988}.%\svp{XXX find papers?}
Examiner training  primarily focus on making identifications, rather than spending an equal amount of time on making eliminations.
In the case of bullet examination, there is an objective criteria for sufficient similarity: 6 consecutively matching striae, according to \citet{biasotti}; similar criteria have been proposed for algorithmic assessment of cartridge cases\citep{song3DTopographyMeasurements2014}.
However, in both instances, there is not a corresponding threshold for dissimilarity.
Here, we do not attempt to identify the source of the bias; we only identify that there is observable bias in the distribution of inconclusive results and suggest that this bias must be addressed.
%Whatever the source of the bias, however, one thing is clear: the discrepancy between the decision-specific conditional probabilities corresponding to errors is fundamentally unjust, and the bias is in the opposite direction of the ideal expressed by Blackstone.
%This problem must be addressed for forensic firearms and toolmark examination to be used equitably. 

\section{Recommendations and Conclusions}
% Recap study weaknesses: 
% \begin{itemize}
% \item number of firearms and firearm types very small
% \item all good studies are of cartridge cases
% \item well-designed studies are essential for proper assessment of error rates
% \end{itemize}

It seems clear from our assessment of the currently available studies that there is significant work to be done before we can confidently state an error rate associated with different components of firearms and toolmark analysis.
In particular, there is a  need for studies that are both large (many examiners and many evaluations) and that meet the following design criteria:
\begin{itemize}
\item Single known source for all comparisons
\item Open set, so that some questioned items do not originate from any of the sources inclued in the study and so that some sources do not contribute any questioned samples.
\item Random allocation of different test kits to participants, to minimize the possibility of information exchange.
\item Clear instructions, so that every participant follows the same protocol.
\item Questioned samples that span the range of difficulty, including challenging comparisons.
\end{itemize}

%well designed (single known source for each comparison, open set, with limited information which can be shared between participants). 
All of the published studies that meet at least some of the design criteria focus only on cartridge cases; it is imperative that similarly well designed studies examining bullets and toolmarks be conducted and published to validate the entire discipline of firearms and toolmark analysis.

While published studies vary in their treatment of inconclusive decisions when calculating and reporting error rates or accuracies, most studies report 
% what is reported as error rates in most studies is 
what we have termed source-specific conditional probabilities. 
We propose that error rates be calculated for the examiner and the process separately. 
Examiner-specific error rates should not count inconclusives as errors, while  process-specific error rates should have to count inconclusives as errors.  Process-specific error rates would then reflect the inability of the examiner to make a correct determination due to insufficient identifying information on the evidence.
These two separate measures will be used differently:  examiner error rates can be used for evaluation within the forensics lab setting (for e.g. qualification purposes in proficiency tests), but process error rates are essential for the proper use of firearms evidence in court: process error rates are germane to the question of whether evidence can inform about the suspect's guilt.

We also argue that a simpler alternative to this treatment of error rates in legal settings would be to report the decision-specific conditional error rates: the probability that, given an examiner's conclusion, that conclusion is incorrect.
An error rate computed in this way provides the most relevant characterization of error in a court setting, where ground truth is not known, but the examiner's decision is given. 
Further, decision-specific error rates avoid the need to treat inconclusives differently than identification or elimination.

In practice, using decision-specific error rates depends on the availability of information that is not currently collected. Specifically, we would need to know examiner-specific probabilities of incorrect conclusions as well as examiner-specific historical decisions. We might be able to learn the  former from reliable proficiency testing, while the latter could be obtained from past lab reports and testimony.

Specifically, in court, we suggest that when the admissibility of an examiner's testimony is assessed, the examiner is asked to state the lab policy on making eliminations, the rate at which the examiner makes inconclusive decisions, along with any relevant error rates specific to the lab and to themselves. We also suggest that during cross-examination after the testimony was presented, the defense ask about the decision-specific probability of an error (relative to the examiner's decision), which will provide specific information which is relevant to the testimony at hand.

% Discuss the systematic biases which all work against the defendant: 
% \begin{itemize}
% \item error rate studies often have more same-source comparisons than what would be expected. 
% \item Some studies are designed such that it is not possible to determine the error rate for different-source comparisons.
% \item The discipline as a whole is focused on examiner error rates, rather than the treatment of the entire process. \svp{This is the only one which may not be completely against the defense}
% \item Examiners have a lower threshold for identification than for elimination - so when a comparison is different-source, examiners are more likely to say that it is inconclusive than they are when a comparison is same-source
% \item There is a systematically higher probability of different source evidence given an examiner's identification than the probability of same source evidence given an examiner's elimination. This runs against the principles which form the foundation of our legal system.
% \item This bias \svp{ against the defense in inconclusives mirrors biases in admissibility of forensic evidence  \citep{morenoWhatHappensWhen2004, epsteinPreferringWiseMan2014, roachFORENSICSCIENCEMISCARRIAGES2009}}
% \end{itemize}

In the process of examining these different error rate calculations, we discovered a series of systematic biases in error rate studies and the examination process which all work against the defendant. For example, 
in many studies, there are more same-source comparisons than what would be expected in case work. Consequently, false elimination rates can be estimated more precisely than false identification rates. 
In fact, in some common study designs, it is only possible to estimate the false elimination error rate; the rate of false identifications cannot be estimated at all. 
This, too, ensures that there is no way to invalidate the examiner's testimony on the basis that it might contribute to a false conviction of the defendant. 
In addition, examiners appear to have a lower threshold for identification than for elimination; when evidence originates from different sources, examiners are more likely to arrive at an inconclusive decision than they are when the evidence has the same source. 
As a result, there is a systematically higher probability of different-source evidence given an examiner's identification than the probability of same-source evidence given an examiner's elimination. 
This fundamentally contradicts the principles which form the foundation of our legal system. 
These biases mirror biases in the admissibility of forensic evidence which are also more likely to favor the prosecution \citep{morenoWhatHappensWhen2004, epsteinPreferringWiseMan2014, roachFORENSICSCIENCEMISCARRIAGES2009}.


% Future work: Examine the distribution of automated scores (RF score, CCF) and cutoff-based classifications compared to examiner decisions. Are algorithms less biased than examiners? (yes, duh). 

%The biases identified in this paper increase the importance of the development and implementation of automatic, unbiased assessment algorithms in forensic laboratories. 
An effective approach to eliminate the biases associated with the current approach to firearm and tool examination is to rely on automatic, objective algorithms to compute a \emph{degree of similarity} between two items.  Research and development of algorithms are underway \citep{chumbleyValidationToolMark2010,songEstimatingErrorRates2018,aoas2,tai2018fully}, but more work must be conducted before they can be implemented in real case work.
These algorithms generally include positive and negative criteria, which may contribute to their ability to make unbiased decisions about the similarity or dissimilarity of the evidence.
Algorithms generally are symmetric in the assessment of positive and negative criteria -- e.g.\ if a high number of consecutively matching striae is considered evidence in favor of an identification, a low number of consecutively matching striae is consequently evidence in favor of an elimination.
Through awareness of the biases that already exist in the evaluation of firearms and toolmark evidence, we can avoid the pitfalls of developing machine learning algorithms using biased training data \citep{howard2018ugly,machinelearninglaw}, ensuring that any automatic processes are as fair as possible.
It is important that before these algorithms are applied to casework, we examine the distribution of automated scores and cutoff-based classifications compared to examiner decisions to determine whether the algorithms are, in fact, less biased than examiners. 
Even though the use of algorithms is not far in the future,
we should also work to resolve the biases associated with the current practice of firearm and toolmark identification. At minimum, we should re-consider the definition and calculation of error rates and modify some of the factors which contribute to the uneven distribution of inconclusives. 



% \svp{Some mentions of quality considerations in  \cite{epsteinPreferringWiseMan2014} -  \cite{UNITEDSTATESAmerica2011} (fingerprints) - need for a precise standard of what makes something high-quality enough for analysis/comparison}
% 
% \svp{Design problems: PCAST Report addendum \cite{pcast-addendum2017}: }
% \begin{quote}\svp{The firearms discipline clearly recognizes the importance of empirical studies. However, most of these studies used flawed designs. As described in the PCAST report, ``set-based" approaches can inflate examiners' performance by allowing them to take advantage of internal dependencies in the data. The most extreme example is the ``closed-set design", in which the correct source of each questioned sample is always present; studies using the closed-set design have under estimated the false-positive and inconclusive rates by more than 100-fold. This striking discrepancy seriously undermines the validity of the results and underscores the need to test methodsunder appropriate conditions. Other set-based designs also involve internal dependencies that provide hints to examiners, although not to the same extent as closed-set designs.}
% 
% \svp{To date, there has been only one appropriately designed black-box study: a 2014 study commissioned by the Defense Forensic Science Center (DFSC) and conducted by the Ames Laboratory, which reported an upper 95\% confidence bound on the false-positive rate of 2.2\%.}
% \end{quote}

% \section{References}
\bibliography{bibfile}
\clearpage
\appendix
\section{Study Summaries and Results}\label{sec:supplement}
\paragraph{Baldwin} The Baldwin study  \citep{Baldwin:2014bb} was designed such that each test kit consists of 15 sets of 3 known cartridge cases and 1 questioned cartridge case. In 5 of the 15 sets the questioned cartridge was from the same source as the knowns, while the other 10 questioned cartridges were from different sources as their respective knowns.

25 firearms were used for the study, such that within each kit no firearm was re-used for either knowns or questioned cartridge cases, i.e. no additional information could be gained by comparing any cartridge cases across sets.

Results are summarized in \Cref{tab:baldwin-summary}. 

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked for accuracy on 2020-04-24
\captionsetup{type=table}\captionof{table}{Baldwin study results, conclusion-specific and source-specific probabilities, and reported overall error rates. }\label{tab:baldwin-summary}
{\large\bf A Study of False-Positive and False-Negative Error Rates in Cartridge Case Comparisons}
\emph{ \citeauthor{Baldwin:2014bb} ( \citeyear{Baldwin:2014bb})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# SS Comparisons & \# DS Comparisons & \\\hline
Open set & 5 & 10 & 218 examiners\\
\end{tabularx}

\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same Source & 1075  & 11 & 4 & 1090\\
Different source & 22 & 735+2\footnote{Two comparisons were not reported and are considered to be inconclusives.} & 1421 & 2180\\\hline
\multicolumn{1}{Y}{Conclusion Total} & 1097 & 748 & \multicolumn{1}{r}{1425} & \multicolumn{1}{Y}{3270}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9799 & 0.0147 & 0.0028 & 0.3333\\
Different source & 0.0201 & 0.9853 & 0.9972 & 0.6667\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.3355 & 0.2287 & \multicolumn{1}{r}{0.4358} & \multicolumn{1}{Y}{1.0000}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9862 & 0.0101 & 0.0037 & 0.3333\\
Different source & 0.0101 & 0.3381 & 0.6518 & 0.6667\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.3355 & 0.2287 & \multicolumn{1}{r}{0.4358} & \multicolumn{1}{Y}{1.0000}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0037  & 0.0101 & 0.0080\\
3 & Process error & 0.0138 & 0.3482 & 0.2367 \\
\end{tabularx}
\end{fmpage}

\clearpage
\paragraph{Keisler} The Keisler Study  \citep{keisler} was designed so that each test kit consisted of sets of 20 pairs of cartridge cases from Smith \& Wesson pistols, where 12 of the pairs were from the same source and 8 pairs were from different sources. 

Kits were assembled using only 9 Smith \& Wesson pistols (i.e. there is a potential to gain additional information by making comparisons across sets). However, participants were instructed to only compare single pairs.

Results from the study are shown in \Cref{tab:keisler-summary}.

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked for numerical accuracy 2020-04-24
\captionsetup{type=table}\captionof{table}{{\large\bf Isolated Pairs Research Study} \label{tab:keisler-summary}}
\emph{ \citeauthor{keisler} ( \citeyear{keisler})}\\
A study of Smith \& Wesson cartridge cases.
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# SS Comparisons & \# DS Comparisons & \\\hline
Open set & 12 & 8 & 126 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same source & 1508 & 4 & 0 & 1512 \\
Different source & 0 & 203 & 805 & 1008 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 1508 & 207 & \multicolumn{1}{r}{805} & \multicolumn{1}{Y}{2520}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.0000 & 0.0193 & 0.0000 & 0.6000\\
Different source & 0.0000 & 0.9807 & 1.0000 & 0.4000\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.5984 & 0.0821 & \multicolumn{1}{r}{0.3194} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9974 & 0.0026 & 0.0000 & 0.6000\\
Different source & 0.0000 & 0.2014 & 0.7986 & 0.4000 \\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.5984 & 0.0821 & \multicolumn{1}{r}{0.3194} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0000 & 0.0000  & 0.0000 \\
3 & Process error & 0.0026 & 0.2014 & 0.0821 \\
\end{tabularx}
\end{fmpage}

\clearpage
\paragraph{Duez} The Duez study  \citep{Duez:2017kha} used virtual microscopy to evaluate scans of cartridge cases. Each participant was asked to make eight evaluations of breech face impressions. These consisted of two sets: 
\begin{itemize}
\item[CCTS1] set of three knowns, four questioned breech face impressions. All questioned breech face impressions are from the same source as the knowns. 
\item[CCTS2] set of three knowns, four questioned breech face impressions. Two questioned breech face impressions are from the same source, two are from different sources.
\end{itemize}

Both sets were evaluated by 56 participants (46 fully certified examiners and 10 trainees). CCTS1 resulted in 56 x 4 correct identifications.
The design of the experiment does not allow us to quantify all of the quantities to evaluate examiner performance unless we aggregate performance over both sets. \Cref{tab:duez-summary} shows the results of this aggregation.

%Combined, the probability for a  same source pair is 6/8 or 75\% and the probability for  a different source  pair is 2/8 or 25\%.

%The top of  shows results reported by 56 participants (46 examiners, 10 trainees).

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{Duez study results, conclusion-specific and source-specific probabilities, and reported overall error rates. For the conditional probabilities, 
the differences in certified examiners and trainees are not large, so only the aggregate results are shown. Certified examiners have a perfect identification rate, only evaluations of different source pairs lead to inconclusives. }\label{tab:duez-summary}
{\large\bf Development and Validation of a Virtual Examination Tool for Firearm Forensics}
\emph{ \citeauthor{Duez:2017kha} ( \citeyear{Duez:2017kha})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# SS Comparisons & \# DS Comparisons & \\\hline
Open set & 6 & 2 & 46+10 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same source & 276+59 & 0+1 & 0+0 & 276+60 \\
Different source & 0+2 & 12+13 & 80+5 & 92+20 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 276+61 & 12+14 & \multicolumn{1}{r}{80+5} & \multicolumn{1}{Y}{449}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9941 & 0.0385 & 0.0000 & 0.7500 \\
Different source & 0.0059 & 0.9615 & 1.0000 & 0.2500 \\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.7522 & 0.0580 & \multicolumn{1}{r}{0.1897} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9970 & 0.0030 & 0.0000 & 0.7500\\
Different source & 0.0179 & 0.2232 & 0.7589 & 0.2500\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.7522 & 0.0809 & \multicolumn{1}{r}{0.1897} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0000/0.0000  & 0.0000/0.1000 & 0.0000/0.0303\\
3 & Process error & 0.0000/0.0167 & 0.1304/0.7500 & 0.0326/0.2000\\ \hline
\end{tabularx}
\end{fmpage}

\clearpage
\paragraph{VCMER} \svp{The VCMER study  \citep{vcmer}} used virtual microscopy to evaluate scans of cartridge cases. Each participant was asked to make sixteen evaluations of breech face impressions, selected from a total of forty sets in a balanced incomplete block design. Each set consisted of two exemplars and one questioned bullet. \svp{Data from this study was also reported in \citet{lilienFirearmForensicsBlackBox2019}.}

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{VCMER study results, conclusion-specific and source-specific probabilities, and reported overall error rates. }\label{tab:VCMER-summary}
{\large\bf Firearm Forensics Black-Box Studies for Examiners and Algorithms using Measured 3D Surface Topographies}
\emph{ \citeauthor{vcmer} ( \citeyear{vcmer})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# SS Comparisons & \# DS Comparisons & \\\hline
Open set & 17 & 23 & 76 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same source & 453 & 38 & 0 & 491 \\
Different source & 3 & 254 & 436 & 693 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 456 & 292 & \multicolumn{1}{r}{436} & \multicolumn{1}{Y}{1184}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9934 & 0.1301 & 0.0000 & 0.4147 \\
Different source & 0.0066 & 0.8699 & 1.0000 & 0.5853 \\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.3851 & 0.2466 & \multicolumn{1}{r}{0.3682} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9226 & 0.0774 & 0.0000 & 0.4147\\
Different source & 0.0043 & 0.3665 & 0.6291 & 0.5853\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.3851 & 0.2466 & \multicolumn{1}{r}{0.3682} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0000  & 0.0043 & 0.0034\\
3 & Process error & 0.0774 & 0.3709 & 0.2492\\ \hline
\end{tabularx}
\end{fmpage}



\clearpage
\paragraph{Brundage-Hamby} The Brundage-Hamby study consists of sets of 20 test fires from known barrels and 15 questioned bullets. The 20 known test fires are 2 bullets from each of ten consecutively manufactured barrels. The Brundage-Hamby study is a closed set study, i.e. the 15 questioned bullets are known to be fired from one of these ten barrels. Participants (firearm examiners) are asked to identify which of the knowns a questioned bullet matches. 

The study was originally reported on by Brundage in 1998  \citep{brundage}. Updates on the study with increasing number of responses have been published several times since   \citet{hamby, Hamby:2018hu}. 
The design of the Brundage-Hamby is well known in the forensics community and has been copied in  \citet{fadul} for a study of cartridge cases of the same firearms. Slight modifications of the study design are also common  \citep{lyons}. 

Results reported in the 2019 paper are shown in \Cref{tab:hamby-summary}.

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{Evaluations, conditional error rates, and overall error rates from the combined Brundage-Hamby studies. Note the focus on identifications.}\label{tab:hamby-summary}
{\large\bf A Worldwide Study of Bullets Fired From 10 Consecutively Rifled 9MM RUGER Pistol Barrels - Analysis of Examiner Error Rate}
\emph{ \citeauthor{Hamby:2018hu} ( \citeyear{Hamby:2018hu})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Closed set & 10 & 15 & 507 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same Source & 10447 & 8 & 0 & 10455 \\
Different source & 0 & ? & ? & 47047.5\footnote{This number is imputed based on the average number of pairwise comparisons an examiner would have to do to complete a Hamby study. Details can be found in the supplemental material.} \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 10447 & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{57502.5}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.0000 & ? & 0.0000 & 0.1818\\
Different source & 0.0000 & ? & 1\footnote{The elimination-specific conditional probabilities are only identifiable in this case because there were not any false eliminations. In any other situation, we would not be able to calculate these probabilities because we do not have the total number of eliminations.} & 0.8282\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.1817 & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9992 & 0.0008 & 0.0000 & 0.1818\\
Different source & 0.0000 & ? & ? & 0.8282\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.1817 & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0000 & ? & ?\\
3 & Process error & ? & ? & ? \\\hline
\end{tabularx}
\end{fmpage}

\clearpage
\paragraph{Bunch \& Murphy}  \citep{bunch2003comprehensive}\hfill\newline
Bunch \& Murphy used a study design that is not conducive to a quick summary. The study consisted of 8 test kits of varying composition; the test kits were evaluated by 8 examiners at the FBI laboratory. 

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{Results and summary tables for the Bunch and Murphy study. Values in this table are as reported in the study; however, due to the study's structure, it is possible to perform fewer comparisons, because not all comparisons are independent. Results for independent comparisons are shown in \Cref{tab:bunch-indep}}\label{tab:bunch-summary}
{\large\bf A Comprehensive Validity Study for the Forensic Examination of Cartridge Cases}\emph{ \citeauthor{bunch2003comprehensive} (\citeyear{bunch2003comprehensive})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Open set & variable & variable & 8 FBI examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Reported (Nominal) Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same source & 70 & 0 & 0 & 70 \\
Diff source & 0 & 172 & 118 & 290 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 70 & 172 & \multicolumn{1}{r}{ 118} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf (Nominal) Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1 & 0 & 0 & 0.2414\\
Diff source & 0 & 1 & 1 & 0.7586\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.1944 & 0.4778 & \multicolumn{1}{r}{0.3278} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf (Nominal) Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.0000 & 0.0000 & 0.0000 & 0.2414\\
Diff source & 0.0000 & 0.5931 & 0.4069 & 0.7586\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.1944 & 0.4778 & \multicolumn{1}{r}{0.3278} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf (Nominal) Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0000 & 0.0000 & 0.0000\\
3 & Process error & 0.0000 & 0.5931 & 0.4778 \\
\end{tabularx}
\end{fmpage}
\clearpage
\paragraph{Lyons} The Lyons study  \citep{lyons} examined marks made by 10 consecutively manufactured extractors (manufactured by Caspian Arms Ltd).

Kits were assembled from 32 cartridge cases: 20 known cartridge cases from pairs of 2 cartridges from each of the 10 extractors (the knowns) and 12 questioned cartridges, such that each known corresponded to at least one questioned, with some replication in most of the kits (one kit accidentally only had ten questioned cartridges). Thus, the setup of this study is similar to the Brundage-Hamby study. This study suffers from the same problems as the Brundage-Hamby study: it is a closed set study with multiple knowns and asks for identifications only. We can therefore only estimate a fraction of the relevant error rates. 
It is also not possible to determine the total number of independent different source comparisons. 
The study results are shown in \Cref{tab:lyons-summary}, along with computed conclusion-specific and source-specific probabilities and error rates.

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{}\label{tab:lyons-summary}
{\large\bf The Identification of Consecutively Manufactured Extractors} \emph{ \citeauthor{lyons} ( \citeyear{lyons})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Closed set & 10 & 10~or~12 & 15 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same Source & 174\footnote{ \citet{lyons} reports 175 correct identifications, but it is clear from the discussion that one of those same source identifications was in fact an inconclusive. 12 answer sheets with 12 correct identifications, one with 10 (out of 10) correct identifications, one with 9 correct identifications and 3 errors, and one with 11 correct identifications and one inconclusive. So 12$\cdot$12 + 10 + 9 + 11 = 174. } &  1 & 3 & 178 \\
Different source & 3 & ? & ? & ? \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 177 & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{?}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9831 & ? & ? & ?\\
Different source & 0.0169 & ? & ? & ?\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & ? & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9775 & 0.0056 & 0.0169 & ?\\
Different source & ? & ? & ? & ?\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & ? & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0169 & ? & ?\\
3 & Process error & 0.0225 & ? & ? \\\hline
\end{tabularx}
\end{fmpage}

\clearpage
\paragraph{Mattijssen} The Mattijssen study  \citep{mattijssenValidityReliabilityForensic2020} examined firing pin aperture shear marks made by 200 9mm Luger Glock pistols confiscated in the Netherlands. The study considered both 3D scans and 2D images; examinations were of the resulting digital representations rather than the actual physical objects. 77 examiners from 5 continents participated in the study; of these, 75 were fully qualified examiners. Of the participants, 58 indicated that they provided categorical conclusions (exclusion/inclusion/inconclusive), 13 provided probabilistic conclusions, and 6 used a 5-step reporting scale as in \citet{pauw2013faid}.

Participants were initially shown comparison images (aligned by computer algorithm) with varying degrees of similarity as a calibration step. Then, 60 comparison images were shown (consisting of 60 sets of either matching or non-matching shear marks, aligned by computer algorithm). Participants were first asked to determine the similarity on a 5-point scale from (almost) no similarity to (almost) total similarity. Once these evaluations were complete, participants were shown each comparison image again, and were asked to answer 3 additional questions assessing 1) conclusion consistent with same/different source, 2) degree of support for this conclusion, in 6 stages corresponding to approximate likelihood ratios, and 3) whether the examiner would have provided an inconclusive conclusion in casework. The addition of these 3 questions allowed the authors to frame this study in a way that is consistent with the AFTE theory of identification, and thus, in a way which is compatible with the way study results are analyzed in this analysis (though of course the methodology is different). 

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{Results for the study's examination of error rates in firearm examiners}\label{tab:mattijssen-summary}
{\large\bf Validity and Reliability of Forensic Firearms Examiners} \emph{ \citeauthor{mattijssenValidityReliabilityForensic2020} ( \citeyear{mattijssenValidityReliabilityForensic2020})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Open set & 38 & 22 & 77 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same Source & 2365 &  487 & 95 & 2947 \\
Different source & 74 & 815 & 784 & 1673 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 2439 & 1302 & \multicolumn{1}{r}{879} & \multicolumn{1}{Y}{4620}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9697 & 0.3740 & 0.1081 & 0.6379\\
Different source & 0.0303 & 0.6260 & 0.8919 & 0.3621\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.5279 & 0.2818 & \multicolumn{1}{r}{0.1903} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.8025 & 0.1653 & 0.0322 & 0.6379\\
Different source & 0.0442 & 0.4871 & 0.4686 & 0.3621\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.4279 & 0.2818 & \multicolumn{1}{r}{0.1903} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0312 & 0.0442 & 0.0509\\
3 & Process error & 0.1975 & 0.5314 & 0.3184 \\\hline
\end{tabularx}
\end{fmpage}

Interestingly, \citet{mattijssenValidityReliabilityForensic2020} also examined the conclusions drawn from a cross-correlation based similarity assessment in both 2D (images) and 3D (scans). We include the reference comparison tables for these as well, because this allows us to compare the results from an objective algorithm with the results from examiners. Note that the algorithms do not produce inconclusive results. 

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{Results from a set of comparisons of 2D images evaluated with an automatic algorithm.}\label{tab:mattijssen-summary2d}
{\large\bf Validity and Reliability of Forensic Firearms Examiners} \emph{ \citeauthor{mattijssenValidityReliabilityForensic2020} ( \citeyear{mattijssenValidityReliabilityForensic2020})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Open set & 200 & 79600 & Computer (2D algorithm)\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same Source & 198 &  0 & 2 & 200 \\
Different source & 1012 & 0 & 78588 & 79600 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 1210 & 0 & \multicolumn{1}{r}{78590} & \multicolumn{1}{Y}{79800}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.16364 & - & 0.00003 & 0.00251\\
Different source & 0.83636 & - & 0.99997 & 0.99749\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.01516 & - & \multicolumn{1}{r}{0.98484} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.99000 & - & 0.01000 & 0.00251\\
Different source & 0.01271 & - & 0.98729 & 0.99749\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.01516 & - & \multicolumn{1}{r}{0.98484} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.01000 & 0.01271 & 0.01271\\
3 & Process error & 0.01000 & 0.01271 & 0.01271 \\\hline
\end{tabularx}
\end{fmpage}

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{Results from a set of comparisons of 3D images evaluated with an automatic algorithm.}\label{tab:mattijssen-summary3d}
{\large\bf Validity and Reliability of Forensic Firearms Examiners} \emph{ \citeauthor{mattijssenValidityReliabilityForensic2020} ( \citeyear{mattijssenValidityReliabilityForensic2020})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Open set & 200 & 79600 & Computer (3D algorithm)\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same Source & 198 &  0 & 2 & 200 \\
Different source & 999 & 0 & 78601 & 79600 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 1197 & 0 & \multicolumn{1}{r}{78590} & \multicolumn{1}{Y}{79800}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.16364 & - & 0.00003 & 0.00251\\
Different source & 0.83459 & - & 0.99997 & 0.99749\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.01500 & - & \multicolumn{1}{r}{0.98500} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.99000 & - & 0.01000 & 0.00251\\
Different source & 0.01255 & - & 0.98745 & 0.99749\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.01500 & - & \multicolumn{1}{r}{0.98500} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.01000 & 0.01255 & 0.01254\\
3 & Process error & 0.01000 & 0.01255 & 0.01254 \\\hline
\end{tabularx}
\end{fmpage}
\clearpage
\section{Simulation of comparisons in studies with multiple knowns}\label{sec:sim}

\subsection{Bunch and Murphy simulation}\label{sec:bunchmurphysim}
In \citet{bunch2003comprehensive}, there are two sources of unknown information: the composition of the study test kits, and potential examiner use of deductive logic. 
While it is possible to assess the effect of deductive logic by eliminating redundant comparisons systematically, because the composition of the test kits was random, there is not a single value for the reduction in comparisons due to the use of deduction.
As a result, we must first simulate the composition of a test kit, and then evaluate the minimal number of comparisons which must be completed using deductive reasoning.

The process for assembling the test kits provides sufficient information to simulate the composition of the eight test kits used in the study.
Using the simulation method, we created 500,000 sets of 8 test kits; from these simulated sets, we identified any which match the reported values of 70 same-source and 290 possible different-source comparisons (287,536). We considered adding the restriction of 45 consecutively manufactured comparisons, but found that this reduced the number of simulations to 6,416; given that we do not assess the consecutively manufactured comparisons separately, adding this restriction was deemed unnecessary. Using these sets, we can then estimate the minimal number of comparisons which are necessary using deductive reasoning in addition to examination. As an independent comparison is one in which the examiner has no relevant prior information about either of the cartridges, the minimal set of comparisons necessary to evaluate all cartridges would also be the set of independent comparisons.

R code to reproduce this simulation (and the deductive reasoning algorithm) can be found at \url{https://gist.github.com/srvanderplas/9cb0268df99e97a9ce327bb1489f7046}.

\noindent\begin{fmpage}{.985\textwidth}\centering
\captionsetup{type=table}\captionof{table}{Results and summary tables for the Bunch and Murphy study, with only independent pairwise comparisons included. 95\% bootstrap intervals are provided for quantities  estimated via simulation. To allocate the inconclusives and eliminations, we used the rule stated in  \citet{bunch2003comprehensive}: FBI examiners can exclude only on class characteristic mismatches. It should be noted that in none of the 287,536 simulations which matched the other set criteria did we find a set which had more than 109 class characteristic mismatches, suggesting that there may have been some eliminations made on individual characteristics, but as this cannot be confirmed, estimates of eliminations only include class characteristic mismatches. }\label{tab:bunch-indep}
{\large\bf A Comprehensive Validity Study for the Forensic Examination of Cartridge Cases}\emph{ \citeauthor{bunch2003comprehensive} ( \citeyear{bunch2003comprehensive})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Open set & variable & variable & 8 FBI examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Independent Comparisons}\\
\multicolumn{5}{c}{Mean (95\% CI)}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same source & 28.7 [26, 31] & 0 & 0 & 28.7 [26, 31] \\
Diff source & 0 & 156.5 [134, 181] & 42.5 [22, 64] & 199.1 [184, 217] \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 28.7 [26, 31] & 156.5 [134, 181] & \multicolumn{1}{r}{42.5 [22, 64] } & \multicolumn{1}{Y}{}\\\hline\hline
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Independent Comparisons: Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.000 & 0.000 & 0.000 & 0.126 [0.110, 0.142]\\
Diff source & 0.000 & 1.000 & 1.000 & 0.874 [0.858, 0.890] \\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.126 [0.110, 0.142] & 0.687 [0.597, 0.779] & \multicolumn{1}{r}{0.187 [0.095, 0.278]} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Independent Comparisons: Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.000 & 0.000 & 0.000 & 0.126 [0.110, 0.142]\\
Diff source & 0.000 & 0.787 [0.682, 0.891] & 0.214 [0.109, 0.318] &  0.874 [0.858, 0.890] \\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.126 [0.110, 0.142] & 0.687 [0.597, 0.779] & \multicolumn{1}{r}{0.187 [0.095, 0.278]} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Independent Comparisons: Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.000 & 0.000 & 0.000\\
3 & Process error & 0.000 & 0.787 [0.682, 0.891] & 0.687 [0.587, 0.779] \\
\end{tabularx}
\end{fmpage}

\clearpage
\subsection{Enumeration of all possible Brundage-Hamby set comparisons}\label{sec:brundage}

As discussed in \Cref{sec:design-consequences}, closed-set studies like \citet{Hamby:2018hu} do not allow us to easily calculate the number of different-source comparisons performed. However, we can estimate the number of comparisons that might need to be performed using logic similar to that employed in \Cref{sec:bunchmurphysim} (but without the variability introduced by the unique experimental design in \citet{bunch2003comprehensive}).

Brundage-Hamby sets consist of a set of 20 known bullets (two each from the ten consecutively manufactured barrels) and 15 questioned bullets.
 \citet{brundage} outlines the construction of sets of questioned bullets in detail as follows: ten bullets are chosen, one from each of the ten barrels. The remaining five bullets are picked at random from the barrels, such that at most three questioned bullets are from the same barrel. 

Using this strategy, a total of 1452 different sets can be constructed. These sets have questioned bullets of three main forms, listed in \Cref{tab:sets} from the perspective of the number of barrels with one, two, or three matching questioned bullets. 

\begin{table}
\caption{\label{tab:sets}Number of all possible sets of questioned bullets for Brundage-Hamby sets.}
\centering
\begin{tabular}{|r|ccc|l|} \hline
 &  one questioned & two questioned & three questioned  & number of \\
\# barrels & bullet matches & bullets match & bullets match & possibilities\\ \hline
I & 5 & 5 & 0 & 252 = $\binom{10}{5}\binom{5}{5}$\\
II & 6 & 3 & 1 & 840\\
III & 7 & 1 & 2 & 360\\ \hline
\end{tabular}
\end{table}

\Cref{fig:histogram} shows the histogram of the total number of different source comparisons before and identification is made.

<<histogram, fig.width=8, fig.height = 3, outwidth="\\textwidth", warning=FALSE, message=FALSE, fig.cap="Histogram of the total number of different source comparisons before an identification is made.">>=
library(tidyverse)
sets <- read.csv("data/allpossible-hambysets.csv")
sets$ds <- rowSums(sets-1)

sets %>% 
  ggplot(aes(x = ds)) + geom_histogram(binwidth=1, colour="grey50") +
  theme_bw() +
  theme(axis.title.x = element_blank()) + 
  xlab("Number of different source comparisons in a set before an identification is made") +
  ylab("Number of sets") +
  scale_x_continuous(breaks=c(49, 54.5, 61,  67.5, 74, 80.5, 86), labels=c("49", "54.5", "61", "67.5", "74", "80.5",  "86"))
@


\end{document}