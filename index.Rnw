\documentclass{elsarticle}
\usepackage[utf8]{inputenc}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\usepackage{amsmath,amsthm,amstext}
\usepackage{bm}
\usepackage{graphicx,psfrag,epsf,wrapfig,subfig,float,capt-of}
\usepackage[dvipsnames]{xcolor}
\usepackage{booktabs,longtable,multirow,multicol,tabu,tabularx,threeparttable,colortbl,array}
\usepackage{adjustbox}
\usepackage{pdflscape,fullpage}
\usepackage{pbox}
\usepackage[numbers]{natbib}
\usepackage{enumerate}

\usepackage{csquotes}% display/block quotes

\setlength{\parindent}{0cm} % remove paragraph indent
\usepackage{parskip}

\newenvironment{definition}[1]% environment name 
{% begin code 
  \par\vspace{.75\baselineskip}\noindent 
  \textbf{Definition (#1)}\begin{itshape}% 
  \par\vspace{.5\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Necessary commands for study-wise summary
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\renewcommand\tabularxcolumn[1]{m{#1}}% for vertical centering text in X column

\newsavebox{\fmbox}
\newenvironment{fmpage}[1]
{\begin{lrbox}{\fmbox}\begin{minipage}[t]{#1}}
{\end{minipage}\end{lrbox}\fbox{\usebox{\fmbox}}}

\journal{Forensic Science International}
%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
% \bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
% \bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num-names}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\noindent

\begin{frontmatter}

\title{Treatment of Inconclusives in the AFTE Range of Identifications\tnoteref{t1}}

\tnotetext[t1]{This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement \#70NANB15H176 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, University of California Irvine, and University of Virginia.}


%% Group authors per affiliation:
% \author{Elsevier\fnref{myfootnote}}
% \address{Radarweg 29, Amsterdam}
% \fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
% \author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
% \ead[url]{www.elsevier.com}
% 
% \author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
% \cortext[mycorrespondingauthor]{Corresponding author}
% \ead{support@elsevier.com}
% 
% \address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
% \address[mysecondaryaddress]{360 Park Avenue South, New York}

\author[isu,csafe]{Heike Hofmann}
\author[unl]{Susan Vanderplas\corref{corauthor}}
\cortext[corauthor]{Corresponding author}\ead{susan.vanderplas@unl.edu}
\author[isu,csafe]{Alicia Carriquiry}
\address[isu]{Statistics Department, Iowa State University\\2438 Osborne Dr, Ames, IA 50011}
\address[csafe]{Center for Statistical Applications in Forensic Evidence, Iowa State University\\613 Morrill Rd, Ames, IA 50011}
\address[unl]{Statistics Department, University of Nebraska Lincoln\\340 Hardin Hall North Wing, Lincoln, NE 68583-0963}   

\begin{abstract}
The treatment of inconclusives in the assessment of errors has been a long-standing issue with far-reaching implications in the legal system. 
\end{abstract}

\begin{keyword}
forensic science, black box studies, proficiency testing
\end{keyword}

\end{frontmatter}



\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\svp}[1]{{\textcolor{teal}{#1}}}
% boxes for illustrations

\noindent


\renewcommand{\thefootnote}{\roman{footnote}}
<<echo = FALSE, message = FALSE, warning = FALSE>>=
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  dpi = 600,
  fig.align = "center",
  out.width = "\\textwidth",
  cache = TRUE,
  fig.path = "figures/",
  echo = FALSE
)
knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
options(knitr.table.format = "latex")

library(stringr)

library(tidyverse)
library(scales)
library(multidplyr) # install_github("hadley/multidplyr")
library(bulletxtrctr) # install_github("csafe-isu/bulletxtrctr")
library(gridExtra)
library(kableExtra)
@

\hh{Big picture of the paper:
\begin{enumerate}
\item Error rates in firearms and toolmark examination are needed
\item Theoretical discussion of error rates: error rates and predictive values
\item Discussion of studies: naming conventions and testing procedures
\item Evaluation of error rates: inconclusive results and a uniform evaluation sheet
\item Summary of studies using the uniform evaluation sheet
\item Discussion of strengths and weaknesses of studies, recommendations
\end{enumerate}}

\tableofcontents

\section{Introduction and Background}
Examiners visually classify similarity of toolmark and firearm evidence according to the AFTE theory of identification  as one of identification, inconclusive or elimination
\citep[][see also \autoref{tab:afte}]{identification}. Exact guidelines for this classification vary from lab to lab; some labs will exclude only on the basis of non-matching class characteristics, such as direction of the twist in rifling, land length or number of lands, or type of rifling \cite{bunch2003comprehensive}. In other labs, CMS (consecutively matching striae) as defined by  \citeauthor{biasotti}  \citep{biasotti} is used as a measure to quantify the similarity of two lands. In virtually all labs, the assessment of individual characteristics of bullet markings is done by visual inspection. 


\begin{table}[hbt]
\noindent\fbox{%
\parbox{.98\textwidth}{%
\begin{enumerate}
\item Identification\hfill\newline
Agreement of a combination of individual characteristics and all discernible class characteristics where the extent of agreement exceeds that which can occur in the comparison of toolmarks made by different tools and is consistent with the agreement demonstrated by toolmarks known to have been produced by the same tool.

\item Inconclusive 
\begin{enumerate}
\item Some agreement of individual characteristics and all discernible class characteristics, but insufficient for an identification.
\item Agreement of all discernible class characteristics without agreement or disagreement of individual characteristics due to an absence, insufficiency, or lack of reproducibility.
\item Agreement of all discernible class characteristics and disagreement of individual characteristics, but insufficient for an elimination.
\end{enumerate}

\item Elimination \hfill\newline
Significant disagreement of discernible class characteristics and/or individual characteristics.

\item Unsuitable \hfill\newline
Unsuitable for examination.
\end{enumerate}
}}
\caption{\label{tab:afte} AFTE Rules of Toolmark Identifications  \citep{identification}.}
\end{table}

The identification process -- i.e. the assessment of whether two samples come from the same source (were made by the same tool, the same shoe, the same finger, shot through the same barrel) or from different sources -- is quite complex. 
In order for firearms and toolmark evidence to be admissible in court, it must be possible to explicitly characterize the accuracy of the identification process \citep{giannelliDaubertInterpretingFederal1993}.

The need for scientific validation and experimentally-determined error rates has been identified in several reports evaluating the discipline of forensic science in the United States as critical for addressing problems within the field and improving the overall performance of the justice system. 
According to the  \citet{nrc:2009}:

\begin{displayquote} much forensic evidence - including, for example, bitemarks and firearm and toolmark identifications - is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline.\end{displayquote}

The  \citet{pcast2016} identified two important gaps:

\begin{displayquote} (1) the need for clarity on the scientific meaning of ``reliable principles and methods" and ``scientific validity" in the context of certain forensic disciplines, and (2) the need to evaluate specific forensic methods to determine whether they have been scientifically established to be valid and reliable.\end{displayquote}

When jurors are left to form their own conclusions about the error rates of forensic pattern disciplines, they often come up with estimates which are far lower than empirical studies, suggesting that jurors consider forensic evidence (in the absence of error rates determined from scientific studies) as of more determinative value than is warranted by the evidence. The PCAST Report (2016) summarizes the effect:

\begin{displayquote} In an online experiment, researchers asked mock jurors to estimate the frequency that a qualified, experienced forensic scientist would mistakenly conclude that two samples of specified types came from the same person when they actually came from two different people. The mock jurors believed such errors are likely to occur about 1 in 5.5 million for fingerprint analysis comparison; 1 in 1 million for bitemark comparison; 1 in 1 million for hair comparison; and 1 in 100 thousand for handwriting comparison.\end{displayquote}

% In addition to these reports which clearly identify the need for error rate studies, it is often essential to have clearly quantified known or potential error rates \citep{giannelliDaubertInterpretingFederal1993} in order for testimony about the results of forensic examinations to be admitted into evidence.

Studies in form of designed experiments serve as the basis for assessing the accuracy of the identfication process.
Knowing ground truth means that the experimenter has the knowledge of whether two samples come from the same source or from different sources.
Casework can not be used to assess the accuracy of the identification process, because ground truth is not knowable.
% \hh{XXX not sure whether we strictly need the next sentence, but I'll give it the benefit of the doubt. XXX}
However, in scientific studies of forensic examination, ground truth is available to the experimenter because the test and the ``evidence" were explicitly designed.

In designed experiments any physical factors that might have an effect on the outcome of a subsequent assessment of pattern similarity \svp{can} be controlled or systematically varied \citep{spiegelmanAnalysisExperimentsForensic2013}.
In the specific situation of firearms evidence, the type of firarm and the ammunition used are of particular interest.
%the type of implement making the pattern, manufacturing order, maintenance procedures, ammunition or marking surface, and many other factors (see  \citet{spiegelmanAnalysisExperimentsForensic2013} for a thorough review of all of the physical factors which might be controlled for or varied in a comprehensive study). 

In addition to the physical evidence-generation process, experimenters must carefully control the amount of information participants have about the study design and structure. 
When this information is not strictly controlled, examiners may use deductive reasoning and other strategies that are useful for completing the test but that do not correspond to casework. 
We provide an example of this phenomenon in \autoref{sec:studies}.

As a result, studies which attempt to gain insight into the identification process must also navigate these complexities, ideally without becoming too complicated to reasonably execute.

In the remainder of this section we will go into a discussion of terms commonly used in experiments with a specific focus on the forensic identification process:
%There are several considerations when designing a study of the identification process:
\begin{description}
\item [Known and Questioned Samples] Most studies are set up to mirror casework, i.e.\ there is a set of samples of known origin and a set of questioned samples of unknown origin. The main objective  in a study is then to determine for a pair of known and questioned samples if they share the same source or come from different sources.
\item [Closed vs. Open Set] In a closed set study all questioned samples originate from the same source as one  (set of) known sample(s). 
Conversely, an open set study is one in which questioned samples may originate from sources outside of the known samples provided. 
Similarly, not all known samples might have a match among the questioned samples.
\item [White box vs. Black Box] In a white-box study the experimenter makes an attempt to understand \emph{why} an examiner came to a specific decision, in contrast to a black box study, which only evaluates the correctness of the decision without assessing the reasoning behind it. 
\item [Blind testing] A blind (or blinded) study is one in which the participant (in this case the examiner) does not know that they are being tested
\footnote{
There is some variability in what the term ``double blind" refers to in the context of forensic studies. 
The use in \cite{bunch2003comprehensive, stromanEmpiricallyDeterminedFrequency2014, Duez:2017kha} does not match the use in \cite{pcast2016,spiegelmanAnalysisExperimentsForensic2013,koehlerProficiencyTestsEstimate2013}.
At least some of the confusion stems from the use of the term in medical contexts, where ``double blind" means that neither the patient nor the doctor knows whether the treatment received is the control or the treatment under investigation. 
In both uses of ``double blind", the underlying goal is to remove any biases which may cause the evaluation of the available evidence differently.
In drug trials, the doctor judges whether the patient has improved or not, and thus, the doctor cannot know which treatment the patient has received, because the knowledge may affect his decision subconsciously. 
In forensic studies, the examiner is assessing the provided evidence and making a determination; thus, in a double-blind forensic study, the examiner cannot know that the evidence is part of a designed study rather than casework, because knowing that extra information may subconsciously affect the examiner's assessment. 
\citet{kerkhoffPartdeclaredBlindTesting2018} provides a more thorough discussion of the different ways the term ``blind testing" has been used in forensic error rate studies.
}; that is, a study which appears to be part of a case, rather than research. 
Blind testing is often recommended  \cite{pcast2016,spiegelmanAnalysisExperimentsForensic2013,koehlerProficiencyTestsEstimate2013}, because the error rates from blind testing better generalize to casework: many studies in a variety of disciplines have shown that people behave differently in situations that are known to be tests. 
%Blind tests also better assess the process of comparison as a whole (rather than the performance of an individual examiner).

\item [Number of knowns from different sources] Some studies  \citep{keisler} provide only one known exemplar (or a set of multiple exemplars from the same source). Other studies, such as  \citep{brundage, hamby,Hamby:2018hu} and the Houston FSC and Phoenix studies from  \citet{case-validation}, include multiple different sources as knowns. 
\item [Study length] Most crime labs are understaffed and maintain a fairly large backlog of cases; as a consequence, examiner time is limited. While examiners should participate in these studies, they must balance the competing demands of a large and consequential workload and the benefit to FTE community. As such, studies which require examiners to make a large number of comparisons may be less likely to find sufficient participants to generate an acceptable sample size.
\end{description}

Many of these considerations can be boiled down to limiting the examiner's knowledge about the study as much as possible, so that (ideally) the only information provided is the information in each pairwise comparison. 
Blind studies, for instance, remove the knowledge that evidence is from a designed test and not casework. 
Open set studies, which are preferable, remove the knowledge an examiner might have about whether a comparison is guaranteed to match one of the provided knowns. 
Studies with designs that limit the number of comparisons by allowing only a single known source and a single unknown source prevent the examiner from using any sort of deductive reasoning. 
Even studies which have a single known source and multiple unknown sources provide only limited additional information, and only in the case where two or more of the unknown sources match each other.

%% Transition to error rates
Once a study is designed and test samples have been assessed by forensic examiners, error rates can be calculated. 
In the remainder of this paper, we discuss the experimentally determined error rates reported in  studies in firearms and toolmark examination. 
We examine the variability in the methods for calculating and reporting error rates, and the different meanings and utility of different types of error rates. 
Using a set of different accuracy and error rates, we assess the state of firearms and toolmark analysis, and the implications of currently available data on the legal assessment of the reliability of testimony about firearms and toolmark related evidence.


\section{Calculating Error Rates}\label{sec:rates}

% \svp{It is unfortunate that t}he methods used to establish error rates experimentally and the interpretations of those error rates in the literature are highly variable. }% Transition from study design section

% In this paper, we discuss the treatment of inconclusives in the assessment of error rates from a statistical point of view. % We believe that this -- rather than further dividing the different opinions -- provides a unifying framework.
% We examine technical accuracy and practical utility of  error rates published in existing error rate studies of firearms and toolmark examination. 
Before approaching the actual studies, we will define a framework and some basic notation to facilitate a comparison of studies with different experimental designs and  explore the logic behind the numerical calculations. %order

\subsection{Classification Framework}

When examiners assess a set of evidence (generally, a pair of items, one of known provenance and one of unknown provenance) and decide whether the evidence is an identification, inconclusive, or an elimination, they provide a \emph{classification} of the evidence in a set of given categories.  
\autoref{fig:shape-overview} is a sketch showing the relationship between evidence (dots) and examiners' decisions (areas). The color for both evidence and decisions corresponds to the source of evidence (different/same) and the type of decision made, respectively. The different combinations of colored dots on top of colored regions correspond to the six potential outcomes of evidence assessment and examiner decision. Obviously, not all of these outcomes are correct.  

<<shape-overview, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas). In a perfect scenario dots only appear on the shaded area of the same color. Any dots on differently colored backgrounds indicate an error in the examination process.", out.width = "60%", fig.width = 6, fig.height = 4, dpi = 500, cache = F>>=
source("code/make_inconcl_plot.R") 
target_plot(100, 50, 10, 10, 50, 100) 
@

% \begin{figure}
% \centering
% % \includegraphics[width=.5\textwidth]{figures/shape-overview.pdf}
% \caption{\label{fig:shape-overview} Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas). In a perfect scenario dots only appear on the shaded area of the same color. Any dots on differently colored backgrounds indicate an error in the examination process. }
% \end{figure}


%Evaluating Classification Decisions%\label{sec:evaluating-classification-decisions}

We will introduce the classification framework and the respective error rates at the example of a generic experiment:
%Starting with the results from a generic experiment, where
let us assume that a representative sample of independent forensic toolmark examiners were asked to complete a total of $N$ comparisons, consisting of $S$ same-source comparisons and $D$ different source comparisons.
An aggregation of the examiners' evaluations can then be reported as shown in \autoref{tab:generic-results}. Note that the layout of the table  is nothing but a summary of all the possible combinations of examiners' decisions and actual state of the evidence as sketched out in \autoref{fig:shape-overview}. The pieces of \autoref{fig:shape-overview} are shown along with letters $a$ to $f$ in \autoref{tab:generic-results}. These letters will be used throughout this section to calculate various accuracy rates and probabilities. 

\begin{table}
\centering
\caption{
An example results table for a generic experiment where comparisons are pairs known to be either of the same source  or  from different sources, and examiners classify each comparison as an identification, an inconclusive, or an elimination, as  specified in the AFTE rules of identification. 
Let $S$ be the total number of same source comparisons, then $S$ is the sum of $a, b,$ and $c$. Similarly, $D$, the total number of different source comparisons, can be written as $D = d + e + f$. The sum of $S$ and $D$ is the total number  of comparisons, $N$. 
%The probability of the occurrence of any cell in the interior of the table $(a, b, c, d, e, \text{ or } f)$ can be found by dividing the respective letter by $N$, the total number of comparisons.
}\label{tab:generic-results}
\begin{tabularx}{.985\textwidth}{Y|YYY|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\\hline
\multicolumn{1}{c}{} & Identification & Inconclusive & \multicolumn{1}{c}{Elimination} & Source Total\\\hline
Same Source & \begin{minipage}[c]{1em}$\bm{a}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.25cm]{figures/a.pdf} \end{minipage} & 
              \begin{minipage}[c]{1em}$\bm{b}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.3cm]{figures/b.pdf} \end{minipage} & 
              \begin{minipage}[c]{1em}$\bm{c}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.35cm]{figures/c.pdf} \end{minipage}& $S = a + b + c$\\
Different source & \begin{minipage}[c]{1em}$\bm{d}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.25cm]{figures/d.pdf} \end{minipage}& 
              \begin{minipage}[c]{1em}$\bm{e}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.3cm]{figures/e.pdf} \end{minipage}& 
              \begin{minipage}[c]{1em}$\bm{f}$ \end{minipage}\begin{minipage}[c]{1em} \includegraphics[width=.35cm]{figures/f.pdf} \end{minipage}&  $D = e + f + g$\\\hline
\multicolumn{1}{Y}{Conclusion Total} & $a + d$ & $b + e$ & \multicolumn{1}{c}{$c + f$} & \multicolumn{1}{Y}{$N = S + D$}\\\hline\hline
\end{tabularx}
\end{table}

The \emph{joint probability} of any source condition and any examiner conclusion can be found by taking the corresponding cell in \autoref{tab:generic-results} and dividing it by $N$, the total number of comparisons  in the study.\footnote{The joint probability of events A and B is the probability that both A and B occur. Here, the joint probability of an examiner identification and a same-source comparison would be equivalent to $a/N$.} 
Thus, the joint probability of a same-source comparison and examiner identification is $a/N$, while the joint probability of a different-source comparison and examiner identification would be $d/N$. 
The accuracy (or error rate) of a classification method can then be determined by assessing how often the process produces a correct (or incorrect) result.

\subsection{What makes an error?}\label{sec:error-rates}
In a classification problem, we would normally take the results in \autoref{tab:generic-results} and calculate various measures of accuracy and classification error.
In firearms and toolmark examination, however, this is complicated by a mismatch between the physical source of the evidence and the set of examiners' decisions.
There are two possibilities to describe the physical state of the evidence: two pieces of evidence are either from the same source or from different sources, but there are three primary outcomes from an examiner's decision.
The examiner can make an identification, an elimination, or an inconclusive determination.\footnote{The fourth category, unsuitable for examination, is one which should be used when evaluating single pieces of recovered evidence; it does not result from the comparison of an unknown source to a known source.}
The difference in number of possible source categories and resulting decisions raises some questions on how inconclusive results should be dealt with when calculating error rates.
Under AFTE guidelines, an inconclusive result is an acceptable outcome of a comparison and  therefore can not considered to be an error made by the examiner.
It has been argued, however, that inconclusives are systematic errors which occur during the identification process, because the final conclusion does not match the known information \citep{koehlerFingerprintErrorRates2007}.
While these two statements seem to be contradictory, in this paper, we provide a foundation under which the two approaches can be partially reconciled in a way that provides additional insight into the examination process. % my attempt % HH I like it :)
% \hh{While these two statements are seemingly directly contradictory, we argue that XXX urgh. phrasing. what I want to say is that we can bring these statements together, and our guidance should be to determine what is closest to the truth. }

%\hh{XXX I feel like that Dror gets us on a different path - more of a bayesian classification problem with inconclusives as a prior and an updated posterior based on additional information available. I really like the thought, though - it would also fit well with quality of scans; need to think about it. HH: new day, let's keep Dror for now. }
\citet{Dror:2018fp} suggest treating inconclusive results as equivalent to a ``decision with certainty that the quantity and quality of information are not sufficient to draw any conclusion regarding the source".
Under this framework, an examiner's assessment starts with inconclusive and is refined to identification or elimination given sufficient evidence in either direction.
Statistically, this would suggest that the decision about the amount of evidence available (inconclusive or not) would be independent of the source of the evidence (same or different source). That is, inconclusives should be equally likely in same-source and different-source assessments.

\svp{XXX Statistically, Mathematically - we sound like we're trying to emphasize different approaches here, and the two statements aren't synchronous XXX}
%Mathematically, t
There are three main ways that inconclusives can be treated in calculating error rates:
(1) inconclusives can be excluded from error calculations, (2) included as correct results  \cite{Duez:2017kha}, or (3) included as incorrect results  \cite{chumbleyValidationToolMark2010}; each of these decisions has an impact on the actual value of the error rate as well as the interpretation of the resulting error rates. 
\autoref{tab:overall-err-rate} shows an overview of these three approaches to calculating the error rate based on the general structure of \autoref{tab:generic-results}, with a fourth option which will be addressed in more detail in \autoref{sec:inconclusives}. 
% Note that there is a fourth option listed, which treats inconclusive results as eliminations. 
% While this at the moment seems quite arbitrary, we will come back to this option at a later point in \hh{XXX}. %\autoref{sec:}. 
Each of the approaches to calculating the error rate has a  different meaning and interpretation; as a result, it is important to consider which error rate best suits the purpose of a study and its interpretation in the larger framework of pattern evidence. 
There are two main scenarios resulting in error rate calculations of type 1:
in \citet{chumbleyValidationToolMark2010} examiners were asked to make either a positive identification or a positive elimination, and were not given the option of making an inconclusive decision.  
This results in cell values $b$ and $e$ of zero, i.e.\ the calculation of the overall error rate simplifies to a binary, symmetric decision process consisting of two actual states and two possible decisions. 
In this case, the overall error rate is the sum of the false positives and the false negatives, divided by the number of overall comparisons. 

\newcommand{\redsqd}[2][.8em]{\protect\fcolorbox{black}{red!50}{\makebox[#1][c]{#2\vphantom{a}}}}
\newcommand{\graysqd}[2][.8em]{\protect\fcolorbox{black}{black!30}{\makebox[#1][c]{#2\vphantom{a}}}}
\newcommand{\whitesqd}[2][.8em]{\protect\fcolorbox{black}{white!100}{\makebox[#1][c]{#2\vphantom{a}}}}

\newcommand{\redsq}[2][1.15em]{\protect\fcolorbox{black}{red!50}{\makebox[#1][c]{#2\vphantom{dp}}}}
\newcommand{\graysq}[2][1.15em]{\protect\fcolorbox{black}{black!30}{\makebox[#1][c]{#2\vphantom{dp}}}}
\newcommand{\whitesq}[2][1.15em]{\protect\fcolorbox{black}{white!100}{\makebox[#1][c]{#2\vphantom{dp}}}}
% \newcommand{\blacksq}{\fcolorbox{black}{black!100}{\phantom{x}}}
\newcommand{\xsq}{\protect\fcolorbox{black}{white!100}{\makebox[1.25em][c]{x\vphantom{dp}}}}

\begin{fmpage}{.985\textwidth}\captionsetup{type=table}\protect\captionof{table}{Different ways to calculate the overall error rate. 
An arrangement of two rows of three boxes resembles the interior cells of \autoref{tab:generic-results}.
We use the following convention:\\
\hfill\pbox{1.65em}{\small\whitesqd{}} correct decisions, cell values are included in the denominator\\
\hfill\pbox{1.65em}{\graysqd{}} cell value is zero or excluded from the calculation\\
\hfill\pbox{1.65em}{\redsqd{}} incorrect decision, cell values are included in the numerator and denominator of the error ratio.
}

\begin{tabularx}{.99\textwidth}{llcl}
\# & Type & Illustration & $P(Error) = $\\
%0 & \pbox[l]{9em}{Inconclusives\\Not Allowed} & 
%\pbox{6em}{\whitesq{a}\redsq{c}\\\redsq{d}\whitesq{f}} & 
%$P(DS\text{ \& }Ident.) + P(SS\text{ \& }Elim) = \displaystyle\frac{c + d}{a + c + d + f}$\\\\
%
1 & \pbox[l]{9em}{No Inconclusives\\or Incl. Ignored} & 
\pbox{6em}{\whitesq{a}\graysq{b}\redsq{c}\\\redsq{d}\graysq{e}\whitesq{f}} & 
$\displaystyle\frac{P(DS\text{ \& }Ident.) + P(SS \text{ \& } Elim.)}{P(Ident.) + P(Elim.)} = \displaystyle\frac{c + d}{a + c + d + f}$\\\\
%
2 & \pbox[l]{9em}{Inconclusives\\as Correct} & 
\pbox{6em}{\whitesq{a}\whitesq{b}\redsq{c}\\\redsq{d}\whitesq{e}\whitesq{f}} & 
$P(DS\text{ \& }Ident.) + P(SS\text{ \& }Elim.) = \displaystyle\frac{c+d}{N}$\\\\
%
3 & \pbox[l]{9em}{Inconclusives\\as Incorrect} & 
\pbox{6em}{\whitesq{a}\redsq{b}\redsq{c}\\\redsq{d}\redsq{e}\whitesq{f}} & 
$\begin{array}{r}
P(DS\text{ \& }Ident.) + P(SS\text{ \& }Elim.) + P(Inconclusives) =  \\
= \displaystyle\frac{b + c + d + e}{N} 
\end{array}$\\\\
4 & \pbox[l]{9em}{Inconclusives\\as Eliminations} & 
\pbox{6em}{\whitesq{a}\redsq{b}\redsq{c}\\\redsq{d}\whitesq{e}\whitesq{f}} & 
$\begin{array}{r}
P(DS\text{ \& }Ident.) + P(SS\text{ \& }Elim.) + P(SS\text{ \& }Inconcl.) =  \\
= \displaystyle\frac{b + c + d}{N} 
\end{array}$ \\
\end{tabularx}\label{tab:overall-err-rate}
\end{fmpage}

The second option 1 scenario is to allow for inconclusives as a decision category but restrict them from the calculation of the error rate entirely. 
This solution does entirely ignore inconclusives, though, as they are part of the process and can be included in examiner testimony (usually phrased as ``could not be excluded"). 
\svp{Thus,} this elimination from consideration is not appropriate if the goal of a study is to assess the error rate of the entire evaluation process. 
\citet{koehlerProficiencyTestsEstimate2013} recommends using  option 1 approach for calculating error rates while  also tracking rates of inconclusive identifications ``for other purposes".


Under AFTE's Theory of Identification, however, inconclusive results are acceptable outcomes and not considered errors. Option 2 shows this way of treating inconclusives as correct regardless of the actual state.
What this means in real terms, is that inconclusives are effectively counted as identifications if they are actually from the same source, but counted as eliminations if they are actually from a different source. Note that this approach is 
%\svp{XXX do we really want to use exploitable instead of making it a bit less personal?} 
exploitable in a strange way: to give a very extreme example, an examiner could report inconclusives for the rest of their career and never make an error.
However, when the goal is to assess the error rate of the examiner under the prevailing guidelines of the AFTE Theory of Identification, option 2 is a reasonable option for assessing error rates of firearm and toolmark examiners.
%, with the slightly odd side-effect that an examiner who marks every comparison as inconclusive does not provide any useful actionable information, but effectively has a zero error rate.

Option 3 provides an approach to view inconclusive results and the corresponding error rates within the wider framework of the legal system:
if a firearm does not mark well, a firearms examiner might not be able to make an identification or an elimination for reasons having nothing to do with the examiner's skill.  
From the  perspective of the \emph{identification process}, though, an inconclusive result does not result in a decision of identification or elimination and can therefore not be considered a successful assessment:
two pieces of evidence are  either from the same source or originate from different sources and an inability to distinguish between those options is an error.

Practically, option 3 reflects the error of the \emph{examination process} rather than the examiner as an individual. 
%\hh{XXX The next couple of sentence only repeat the above}
%To illustrate why this is the case, consider that an examiner may, upon inspection of a fired bullet, find that there are insufficient individualizing characteristics to make an identification.
%This does not reflective negatively on the examiner's skill; it is merely a consequence of the contact made between the ammunition and the gun during the firing process. 
%Nonetheless, even though a pair of objects originated from the same physical source, it is not possible to evaluate the evidence and make an identification. 
%The inconclusives are representative of the errors involved in the process of evidence recording, rather than the evaluation. 

%\hh{XXX do we need to say something about option 4 here? }

In option 4, which will become more relevant in \autoref{sec:inconclusives}, inconclusives are treated the same as eliminations. 
This option places the primary focus of the examiner's assessment on identification, and treats any failure to make an identification as equivalent to an elimination.

The errors introduced in this section summarize the overall error rate. While these errors are informative, most studies allow us a deeper insight into different aspects of the examination process by taking additional information into account.
%
In the next two sections we introduce two additional types of error rates: {\it source-specific} and {\it decision-specific} error rates.
Both of these approaches are additional assessments of the reliability of a classification method which leverage \emph{conditional probabilities}, i.e.\ probabilities which are updated to account for known (or assumed) information.
Rather than summarizing an experiment in a single number reflecting error or accuracy like before, 
these conditional probabilities allow us to provide a more specific assessment of the error accounting for known information, such as the examiner's decision or the source of the evidence. This allows us in particular to calculate separate accuracy rates for comparisons of same-source or different-source evidence. 
%about certain scenarios, but do not broadly summarize an experiment using a single number.
%The next two sections detail two different sets of conditional probabilities which both assess the accuracy and reliability of the classification process.
This has the additional advantage that some of these conditional probabilities do not require inconclusives to be explicitly handled as errors or correct decisions. 
%In addition, examining the conditional probabilities offers some additional insights into the process of firearms examination and the types of errors which are likely to be made.



\subsection{Source-specific assessment}

%\hh{When assessing the accuracy of a classification \emph{success rates} and \emph{error rates} are of interest, i.e.\ the interest shifts from the classification to the numerical quantity indicating how often the process produces correct and incorrect results.}

%\svp{Note that both the TPR and TNR are measures of the accuracy of a classification method.}

The most common way to assess the success (or accuracy)  of a classification method is to calculate its \emph{sensitivity} and \emph{specificity}. 
Sensitivity, also called the true positive rate (TPR), is an examiner's ability to make an identification when examining same source evidence (SS). 
Sensitivity can be expressed as the conditional probability $P (\text{Identification} \mid SS)$. 
Conversely, specificity, or the true negative rate (TNR), is an examiner's ability to make an elimination given evidence from different sources (DS), $P (\text{Elimination} \mid DS)$. 
%Mathematically, w
We calculate the true negative rate as the conditional probability of the examiner making an elimination \emph{given} the pair under examination is from different sources. 
A conditional probability of an event \emph{given} another event is the probability that the two events both occur, divided by the probability of the second event, specifically for the true negative rate:
$$TNR = P(\text{Elimination} | DS) = \frac{P(\text{Elimination and } DS)}{P(DS)}.$$


% \hh{XXX before we talk about inconclusives we really can't discuss what an error is.}

Complementary to accuracy rates, we define two error rates of a classification method:
%Another common method of assessing accuracy of a classification method is to calculate the false positive rate (FPR) and the false negative rate (FNR). 
The \emph{false positive rate} (FPR) -- or failed elimination rate -- is the probability that an examination of different source evidence does not result in an elimination. %: $FPR = 1 - TNR$. 
Similarly, the \emph{false negative rate} (FNR) -- or missed identification rate -- is the probability that an examination of same-source evidence does not result in an identification. %: $FNR = 1 - TPR$. %makes an elimination from evidence which originates from the same source (SS), $FNR = P(\text{Elimination} | SS)$. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/Source-specific-tree-diagram.pdf}
\caption{Tree Diagram illustrating the logic of source-specific error rates. \hh{Information flow happens from left to right: we are assessing an examiners decision given that someone (not the examiner) knows the source of the evidence.} Source-specific rates give the probability of an examiner's decision (Identification, Inconclusive, or Elimination) given same-source and different-source evidence.}\label{fig:FPR-specific-tree}
\end{figure}

Note that we are keeping the definition of error rates a bit vague on purpose. 
As discussed in \autoref{sec:error-rates}, there are multiple ways to define an error; but the concepts of FPR and FNR hold for all of the  options of treating  inconclusives outlined in \autoref{tab:overall-err-rate}.


In all  of these assessments, the
rates are calculated by conditioning on the \emph{origin of the evidence}. This process is shown graphically in \autoref{fig:FPR-specific-tree} using a decision tree to describe the comparisons which can be made conditioned on ground truth. Calculations for each value are shown in \autoref{tab:fpr-prob}.

\begin{table}\centering
\caption{
Source-specific probabilities, calculated using the quantities introduced in \autoref{tab:generic-results}. 
In each cell in the main body of the table, probabilities shown are the probability of conclusion $Y$ (one of Identification, Inconclusive, or Elimination) if the source is $X$ (same, different). 
In the last row in the table, the marginal probabilities $P(Y)$ are shown, that is, the unconditional probability that the examiner's decision is $Y$. 
In the last column of the table, the marginal probabilities $P(X)$ are shown; these are determined by the experimental design.
}\label{tab:fpr-prob}
\begin{tabularx}{\textwidth}{Y|ccc|Y}
\multicolumn{5}{c}{\bf Source-specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & $a/S$ & $b/S$ & $c/S$ & $S/N$\\
Different source & $d/D$ & $e/D$ & $f/D$ & $D/N$\\\hline
\multicolumn{1}{Y}{Conclusion probability} & $(a + d)/N$ & $(b + e)/N$ & \multicolumn{1}{r}{$(c + f)/N$} & \multicolumn{1}{Y}{1.00}\\\hline\hline
\end{tabularx}
\end{table}
A google sheets workbook which  performs all of these calculations given counts $a, b, c, d, e, \text{ and } f$ is available at \url{http://bit.ly/FTE-error-rate-worksheet}.

\svp{Thus, assuming a comparison is same-source, the examiner would make an identification with probability $a/S$. This is equivalent to the true positive rate, or TPR. 
Assuming a comparison is different source, the examiner would make an elimination with probability $f/D$. This is equivalent to the true negative rate, or TNR.}

% \svp{XXX need to provide real-world meaning (e.g. court testimony, if possible) for a couple of these values.}

% \hh{XXX should we pull up some of the discussion that only regards these error rates? source-specific probabilities can not be calculated from casework, they allow an assessment of the performance of an examiner, but not an assessment of the evidence. source-specific probabilities are the error rates typically reported. Our argument here is that they are largely irrelevant in legal proceedings}\svp{Let's leave that to the practical impact section - it's close enough to this section that it should still be connectable, and it will make more sense when contrasted against the decision specific assessment.}

\subsection{Decision-specific assessment}

%The use of false positive and false negative rates to characterize a classification task is extremely common, however, there are alternate ways to evaluate a classification method.
An alternate way to evaluate a classification method is to assess \svp{the probability of a same-source or diferent-source comparison, given an examiner's decision (or, more generically, a specific general conclusion).}
The \emph{positive predictive value} (PPV) is the probability that, if an examiner makes an identification, the evidence is from the same source, that is, $P(SS | \text{Identification})$. 
Its complement, the \emph{false discovery rate} (FDR), is the probability that if an examiner makes an identification, the evidence is from different sources.

The \emph{negative predictive value} (NPV) is the probability that, if an examiner makes an elimination, the evidence is from different sources, or $P(DS | \text{ Elimination})$. 
Its complement, the \emph{false omission rate} (FOR) is the probability that, if an examiner makes an elimination, the evidence is actually from the same source\footnote{In \citet{songEstimatingErrorRates2018}, the FDR is referred to as the false identification error rate, and the FOR is called the false exclusion error rate.}.
As before, we can map out the conditional hierarchy using a decision tree, shown in \autoref{fig:PPV-specific-tree}. 
To reduce confusion with the naming or rates and predictive values we have labeled nodes in the trees of \autoref{fig:FPR-specific-tree} and \autoref{fig:PPV-specific-tree} \svp{corresponding to the rates defined in the text.}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/Conclusion-specific-tree-diagram.pdf}
\caption{
Tree Diagram illustrating the logic of positive and negative predictive value calculations. Decision-specific values give the probability of evidence coming from the same source or different sources given an examiner's testimony.
}\label{fig:PPV-specific-tree}
\end{figure}


Using the quantities in \autoref{tab:generic-results}, we can calculate the decision-specific or conclusion-specific probabilities from \autoref{fig:PPV-specific-tree} as shown in \autoref{tab:ppv-prob}.

\begin{table}\centering
\caption{
Decision-specific probabilities, calculated using the quantities introduced in \autoref{tab:generic-results}. 
In each cell in the main body of the table, probabilities shown are the probability of source $X$, if the examiner's conclusion (one of Identification, Inconclusive, or Elimination) is $Y$. 
In the last row in the table, the marginal probabilities $P(Y)$ are shown, that is, the unconditional probability that the examiner's decision is $Y$. 
}\label{tab:ppv-prob}
\begin{tabularx}{\textwidth}{Y|ccc|Y}
\multicolumn{5}{c}{\bf Decision-Specific Probabilities}\\\hline
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & $a/(a+d)$ & $b/(b+e)$ & $c/(c+f)$ & $S/N$\\
Different source & $d/(a+d)$ & $e/(b+e)$ & $f/(c+f)$ & $D/N$\\\hline
\multicolumn{1}{Y}{Decision probability} & $(a + d)/N$ & $(b + e)/N$ & \multicolumn{1}{r}{$(c + f)/N$} & \multicolumn{1}{Y}{1.00}\\\hline\hline
\end{tabularx}
\end{table}


Predictive value assessments are used in assessing the implications of a particular classification label; they inform us about the \emph{likely state of the physical evidence} given the expert's decision or testimony. 

This makes the PPV, NPV, FDR, and FOR particularly useful when evaluating casework (or in a courtroom setting), because the true state of the evidence is unknown.
Mathematically it makes sense to condition on unknown quantities, however, in practice it is much more informative to condition on quantities with known values, because these probabilities are immediately applicable in the specified situation.

Conversely, source-specific assessments are useful when evaluating the performance of individuals on a designed test, because ground truth is known (and thus, can be conditioned upon) in designed studies.


\subsection{Practical interpretation of error rates}

%Mathematically, t
The primary difference between the decision tree diagrams in Figure \ref{fig:FPR-specific-tree} and \ref{fig:PPV-specific-tree} is the conditioned variable. 
The source-specific error rates, FPR and FNR, are calculated by conditioning on the source of of the evidence (same-source/different source) and give the probability of an examiner's decision.
PPV and NPV are calculated by conditioning on an examiner's conclusion and give the probability of evidence being same-source or different source.

From a practical point of view, whether source-specific or decision-specific rates are of interest depends upon where in the overall (legal) decision process we are: 
\begin{itemize}
\item as a firearms and toolmark examiner or a lab director, we are interested in assessing accuracy based on ground truth, i.e. we are interested in source-specific rates to answer questions of the form: given this evidence, are  trained firearms examiner more likely than trainees to come to the correct conclusion? (Case studies later show that yes, that is the case.) Alternately, lab directors might want to know if all of the examiners employed by the lab are performing above a certain threshold.
\item as part of courtroom proceedings, we -- the jury, lawyers, and judges -- are interested in assessing accuracy of testimonies, i.e. we are interested in conclusion-specific rates to answer questions of the form: given an expert testified to the evidence as being an identification, what is the probability that the evidence actually does come from the same source? In this situation, we can update the overall error rate by conditioning on the examiner's conclusion, providing more specific evidence about the scenario under scrutiny in court. As we only know the examiner's testimony (and not ground truth), we must work with the decision-specific rates. 
\end{itemize}
These two forms of questions look deceptively similar, but lead to different mathematical equations. 
This difficulty is not exclusive to evaluations of firearms evidence -- it is a known and pervasive pitfall in other application areas, such as medical diagnostic \svp{testing}  \cite{Casscells:1978ba, Manrai:2014fp}.
The distinction between these two sources of error rates is perhaps a bit more approachable in the setting of a medical context: a given test has a sensitivity (true positive rate, TPR) and a specificity (true negative rate, TNR) that combined determine the overall accuracy rate. 
However, if you as a patient receive a positive test result, what you really care about is the probability that, given your positive result, you actually have the disease. 
This probability is the positive predictive value, or PPV of the test, and is equivalent to the decision-specific probability $P(SS\mid \text{Identification})$. 

% \hh{XXX We haven't talked about inconclusives yet - we should put a teaser here that we are going to come back to that.}
% \svp{One additional complication to this evaluation is the treatment of inconclusives: under AFTE guidelines, inconclusives are not errors, but it has been argued that in at least some sense, inconclusives are systematic errors, because the conclusion does not match the known information \citep{koehlerFingerprintErrorRates2007}.
% We will address this complication in \autoref{sec:inconclusive}.
% }
  
% \hh{XXX A sentence on why we need case studies to evaluate and assess all of these error rates. }

In case work, because ground truth is not known, we cannot definitively know that an examiner's evaluation is correct. 
For that reason, we need designed studies to estimate the error rates of examinations in situations where ground truth is known. 
As we rely on these studies to generalize to casework, it is of great importance that these studies are well designed \svp{so that it is possible to estimate any of the success and error rates discussed above.}
%so as to allow for the estimation of any of the success or error rates discussed above.

\section{Studies}\label{sec:studies}

In this section, we  examine the results from a number of studies designed to assess the error rates of firearms examination. 
For each study we provide the reported results and present error rates and conditional probabilities discussed in the previous section along with a visual comparison for each study. 
Using these comparisons we assess the state of these error rate studies and provide an overview of the current status-quo of firearms examination.

%The structure of this section is supposed to mirror the theoretical introduction of different error rates and other aspects of faults in the identification process.

% \hh{
% \begin{enumerate}
% \item Description of studies
% \item How do we calculate error rates in practice and problems related to that that prevent us from having actual numbers in some studies and simulation approaches
% \item source-specific error rates
% \item decision-specific error rates
% \item inconclusives
% \end{enumerate}
% XXX should we move the individual discussion points for studies into the overall discussion or weave the weaknesses of each study into our assessment here?
%\svp{My vote is to weave things in. I've already started. Some won't fit with the flow, but that's ok, this thing is already pretty long.}


\subsection{Error Rate Studies}
The studies included in this section are not an exhaustive list of every study ever performed, rather we tried to include the most central and most cited studies, as well as several of the studies mentioned in the PCAST and NAS reports concerning firearm and toolmark analysis. A brief description of each study is provided here for reference purposes, with summary tables of results and conditional probabilities provided in \ref{sec:supplement}.

\begin{description}
\item[Baldwin] \citep{Baldwin:2014bb} is a study of Ruger SR9 cartridge cases. It is an open-set study with 15 comparisons of 3 knowns to 1 questioned cartridge case. Participants received sets with 10 different-source and 5 same-source comparisons.
\item[Keisler] \citep{keisler} is a study of cartridge case comparisons. It is an open set study with 20 comparisons of 1 known to 1 questioned cartridge cases. 126 participants received sets with 12 same-source and 8 different-source comparisons.
\item[Brundage-Hamby]  \citep{Hamby:2018hu} is a closed-set study of 10 consecutively manufactured Ruger P-95 barrels. 507 participants received sets consisting of 10 standards (2 bullets each) and 15 questioned bullets. 
\item[Lyons] \citep{lyons} is a closed-set study of 10 consecutively manufactured Colt 1911A1 extractors. Participants received a set of 10 standards and 12 questioned cases. The design of this study is similar to  \citep{Hamby:2018hu}.
\item[Duez] \citep{Duez:2017kha} is an open-set study of breech face comparisons using virtual microscopy. Each of 56 participants (46 trained, 10 trainees) received two test sets consisting of one standard (3 scans) and 4 questioned scans In one set, all questioned scans were from the same source as the standard; in the second set, there were two different-source questioned scans and two same-source questioned scans.
\item[Bunch \& Murphy] \citep{bunch2003comprehensive} is an open-set study of consecutively manufactured Glock breechfaces. In this study, each of 8 participants received 10 cartridge cases and was required to evaluate each possible pairwise comparison. In this study, the number of same-source and different-source cartridge cases varied by test kit from 10 same-source cartridges to 10 different-source cartridges.
\item[Fadul] \citep{fadul} is a closed-set study of the breech face striations/impressions produced by 10 consecutively manufactured slides. Participants received a set of 10 standards and 15 questioned cases. The design of this study is similar to  \citep{Hamby:2018hu}.
%\item [Chumbley] \citep{chumbleyValidationToolMark2010} \svp{compared the performance of an automated algorithm with examiners' performance for limited portions of marks made by consecutively manufactured chisles at different angles. The Chumbley study had 50 participants, but did not report the number of same source and different source comparisons in the examiner portion of the study, making it unsuitable for further analysis. }
%\item [Hare] \citep{aoas2} \svp{assessed the performance of a computational algorithm for bullet analysis, using binary classification (no inconclusives). Subsequent studies  \cite{case-validation} of the algorithm presented in  \citep{aoas2} suggest that on the bullet-to-bullet comparison level, the algorithm perfectly classifies bullets from 3 different external test sets.} \hh{XXX }
\end{description}

\hh{Besides the seven studies identified here, ... XXX?}
\svp{
Of the nine studies identified, one  \citep{aoas2} presents a method for quantitative comparison of bullets with no experimental lab-based examiner assessments. 
Two other studies  \citep{chumbleyValidationToolMark2010, fadul} did not provide sufficient detail about examiner results and conclusions to conduct a partial analysis based on the AFTE theory of identification.
}

In the next sections, we discuss the design and results of each study, including calculation and comparison of source- and decision-specific error rates. 

\subsection{Consequences of Study Designs for Error Rate Estimation}
Our survey of the most commonly cited studies reveals a list of experimental design concerns similar to those identified in the 2017 addendum to the PCAST report \cite{pcast-addendum2017}. 

\begin{displayquote}
As described in the PCAST report, ``set-based" approaches can inflate examiners performance by allowing them to take advantage of internal dependencies in the data. The most extreme example is the ``closed-set design", in which the correct source of each questioned sample is always present; studies using the closed-set design have underestimated the false-positive and inconclusive rates by more than 100-fold. This striking discrepancy seriously undermines the validity of the results and underscores the need to test methods under appropriate conditions. Other set-based designs also involve internal dependencies that provide hints to examiners, although not to the same extent as closed-set designs.
\end{displayquote}

The PCAST response, issued in  \citeyear{pcast-addendum2017}, identifies only one study \cite{Baldwin:2014bb} as appropriately designed to evaluate the validity and reliability of firearms analysis methods. 
Shortly after the report was issued, two more studies were published which also meet the criteria for reliable studies in the report:  \citet{keisler} and  \citet{Duez:2017kha}. 
All three ``good" studies used slightly different designs, but maintained certain essentials: each kit consisted of multiple comparisons composed of one or more samples from exactly one known source and one unknown source.
These designs substantially reduce or eliminate the internal dependencies which provide ``hints" to examiners by ensuring that each comparison of samples is considered independently.
In addition, studies with similar designs ensure that it is possible to exactly enumerate the number of same-source and different-source comparisons\footnote{At least, it is possible with the additional assumption that a comparison between a multiple items from the same source and an unknown should count as a single comparison.}.
In what is almost certainly a coincidence, all three studies are also limited to cartridge case comparisons. \svp{As a result, however, we could not identify any studies which assess the error rates of bullet or toolmark examination in a manner that would produce reliable and generalizable measurements.}


\svp{As a result, studies which use less reliable designs, such as} \citet{lyons},  \citet{Hamby:2018hu} (and the similarly designed  \citet{fadul}), and  \citet{bunch2003comprehensive} \svp{have historically been frequently referenced in admissibility hearings.}
Of these, the study by  \citeauthor{bunch2003comprehensive} is perhaps the least flawed.
\svp{It has also been superceded by studies which have better experimental design, such as \citet{Baldwin:2014bb} and \citet{keisler}.}
While it has internal dependencies due to the inclusion of multiple knowns, and as a result, the number of same-source and different-source comparisons cannot be fully determined, it does have one \svp{desireable} feature not found in even the well designed studies \svp{which makes it worth discussing here}.
Specifically,  \citeauthor{bunch2003comprehensive} varies the composition of the test kits, so that no kit had the same composition in terms of same and different-source comparisons. 
This ensures that even if examiners discuss the tests, they cannot gain any additional information from such discussions. 

The primary problem with  \cite{bunch2003comprehensive} is that it includes multiple known exemplars. 
As a result, it is possible to use logical reasoning to reduce the set of comparisons.
For example, if an examiner is comparing unknown source A to known exemplars from sources 1 to 10, and A matches exemplars from source 2, it is not necessary to make comparisons to sources 3 - 10. 
This design ensures that it is not possible to count up the total number of different-source comparisons performed; as a result, we cannot compute the decision-specific error rates or the source-specific error rates for different-source comparisons. 
Any study which includes multiple known sources in a single comparison set will have this limitation. 
In \ref{sec:bunchmurphysim}, we use statistical simulation to explore the likely number of different-source comparisons performed in  \cite{bunch2003comprehensive}; this assessment includes both the effect of deductive reasoning and the sampling procedure used to create the test kits. 
We also perform a similar assessment for  \cite{Hamby:2018hu} in \ref{sec:brundage}; the design of this study does not include the variability introduced by the test kit assembly procedure used in  \cite{bunch2003comprehensive}. 

While studies which use multiple known sources have been conducted for more than 25 years, such studies are inherently flawed. 
Multiple-known studies are inherently biased: they provide evidence that examiners can make identifications accurately (and do not make many false eliminations), but do not quantify the probability that examiners make eliminations correctly. 
As a result, when error rates derived from these studies are cited, they do not include errors which result from different source comparisons, which prevents evaluation of examiners on their ability to distinguish between different sources.
This fundamentally biases these studies so that in court they provide useful (but misleading) information to the prosecution while offering nothing useful to the defense.
That is, the fundamental structure of these studies only provides information about how accurate examiners are when providing evidence against the defendent.

The design used in  \cite{Hamby:2018hu} was originally found in  \cite{brundagethesis} and dates back to  \citeyear{brundagethesis}. 
As such, it was a groundbreaking study, and its longevity makes it the most extensive study in forensic error rates in firearms (and possibly across pattern disciplines). 
Unfortunately, the study is flawed in two major respects: it uses multiple known sources, and it is a closed set study. 
The multiple known source problem was described above, but this problem is magnified when combined with the closed-set study design. 

In a closed-set design, the structure of the study helps examiners make the correct conclusion: if unknown source A is most similar to exemplars from source 8, then after the 10 comparisons are done, the examiner would rationally make an identification for source 8. 
In an open-set study, the examiner might rate the same pairwise comparison as inconclusive, because there is no additional information suggesting that the unknown must match one of the provided knowns.
In both the multiple known and closed set study designs, the use of deductive reasoning artificially reduces the error rates calculated using the study results.
    
It might be possible to modify the answer sheet so that examiners record each pairwise comparison, \svp{which would allow for assessment of} the number of different-source comparisons performed. 
However, revising the answer sheet would not prevent the use of deductive reasoning, which gives examiners an advantage in the error rate studies that does not exist in casework.
Currently, studies patterned after  \cite{brundagethesis,Hamby:2018hu} use an answer sheet where examiners are only asked to report which of the 10 knowns the questioned bullet matches. 
Because the study does not ask examiners to report eliminations, there is no way to assess elimination error rates or the overall error rate consisting of both missed identifications and false eliminations.

In addition to the design issues which plague the closed-set studies, the execution of some of these studies makes the data even less reliable. 
Methodological issues, such as those described in  \cite{lyons}, where test sheets were returned to participants who misunderstood instructions, cast even more doubt on the utility of error rates derived from such studies and their ability to generalize to casework. 

Finally, there are some methodological constraints that are common to most studies. 
Of the studies assessed in this paper, only  \citet{bunch2003comprehensive} uses a variable percent of same and different source comparisons; without varying this percentage, it is possible that examiners could compare notes and gain additional information about the study results. 
Many studies (but not all, \cite{Baldwin:2014bb}) pre-screened kit components to ensure that sufficient markings were present to allow for identification - this differs from casework and artifically inflates the rate of successful identifications and eliminations
\footnote{
% \hh{XXX  let's find a different spot for the next paragraph.}

% \svp{XXX Not sure this should go here, but not sure if there's a better place for it to go... XXX}
%Some studies screen for \svp{the} possibility \svp{that there is insufficient information present to make a conclusive assessment}, removing evidence with insufficient identifying marks before the study is conducted.
%In other studies, however, screening was intentionally not performed, so as to better mimic actual casework conditions. 

\svp{
In Baldwin  \cite{Baldwin:2014bb}, the only pre-screening of usability was to ensure cartridge cases were caught with the cartridge catching device. 
Additionally, \hh{for 3234 comparisons, FTEs evaluated how many of the known cartridge cases were usable for an evaluation: all three specimens were used in 3018 cases, two were used in 207 cases, and only one was used in nine cases.}
The Baldwin study is fairly unique in this regard: most studies make some effort to control the quality of the evidence sent out for assessment.
}}. 
\svp{Studies which tightly control the evidence quality and presence of individualizing marks would under-estimate the process error (though the estimates of examiner error are not necessarily affected).}%\svp{XXX not sure this is true - need to think through it...}

In almost all studies, participants use lab rules for determining whether eliminations could be made on individual characteristics; while  \citet{Baldwin:2014bb} instructed participants to use a uniform set of rules, many participants reported that they did not adhere to these guidelines.


<<data-setup, include = F>>=
clopper <- function(alpha, success, trials) {
  lower <- qbeta(alpha/2, success, trials-success+1)
  upper <- qbeta(1-alpha/2, success+1, trials-success)
  c(lower, upper)
}

library(tidyverse) 
studies <- read.csv("data/studies.csv")
studies <- studies %>% mutate(
    Study = factor(Study, 
                    levels=c("Baldwin (2014)", "Keisler (2018)", "Duez (2018)", "Bunch & Murphy (2003)", "Hamby (2019)", "Lyons (2009)"))
  )

cis <- studies %>% group_by(Study, Decision) %>% 
  filter(Type == "Observed") %>%
  summarize(
    pred_ss = sum(Number[Ground.truth=="Same Source"])/sum(Number),
    pred_ss_lower = clopper(0.05, sum(Number[Ground.truth=="Same Source"]), sum(Number))[1],
    pred_ss_upper = clopper(0.05, sum(Number[Ground.truth=="Same Source"]), sum(Number))[2]
  )

source_spec_cis <- studies %>% group_by(Study, Ground.truth) %>% 
  filter(Type == "Observed") %>%
  mutate(
    pred_ss = Number/sum(Number),
    pred_ss_lower = map2_dbl(Number, sum(Number), ~clopper(0.05, .x, .y)[1]),
    pred_ss_upper = map2_dbl(Number, sum(Number), ~clopper(0.05, .x, .y)[2])
  )

exp <- studies %>% 
#  filter(Decision == "Inconclusive") %>%
  group_by(Study, Decision) %>% 
  summarize(
    exp_ss = Number[Type=="Expected"][1] * sum(Number[Type == "Observed"]),
    exp_ss_lower = clopper(0.05, exp_ss, sum(Number[Type == "Observed"]))[1],
    exp_ss_upper = clopper(0.05, exp_ss, sum(Number[Type == "Observed"]))[2],
    exp_prob = exp_ss/sum(Number[Type == "Observed"])
  ) 
  
DontKnow_source_spec <- tibble(
  pred_ss = 0.5, 
  xmin=c(0,0,0,0,0,0), 
  xmax=c(1,1,1,1,1,1), 
  Study = c("Lyons (2009)", "Hamby (2019)", "Lyons (2009)", "Hamby (2019)", "Lyons (2009)", "Hamby (2019)"), 
  Decision=rep(c("Identification", "Inconclusive", "Elimination"), each=2),
  Ground.truth = "Different Source") %>%
  dplyr::mutate(Decision = factor(Decision, levels=c("Identification", "Inconclusive", "Elimination")))

DontKnow <- data.frame(
  pred_ss = 0.5, 
  xmin=c(0,0,0,0), 
  xmax=c(1,1,1,1), 
  Study = c("Lyons (2009)", "Hamby (2019)", "Lyons (2009)", "Hamby (2019)"), 
  Decision=rep(c("Inconclusive", "Elimination"), each=2))

errors <- studies %>% 
  mutate( # note, this is defining CORRECT decisions
    AFTE = (Ground.truth=="Same Source" & Decision != "Elimination") | 
      (Ground.truth=="Different Source" & Decision != "Identification"),
    Process = ((Ground.truth=="Same Source" & Decision == "Identification") | 
      (Ground.truth=="Different Source" & Decision == "Elimination")),
    Elimination = ((Ground.truth=="Same Source" & Decision == "Identification") | 
      (Ground.truth=="Different Source" & Decision %in% c("Inconclusive", "Elimination")))
         ) %>%
  group_by(Study, Ground.truth) %>% 
  filter(Type == "Observed") %>%
  summarize(
    Comparisons = sum(Number),
    AFTE = sum(Number[!AFTE]),
    Process = sum(Number[!Process]),
    Elimination = sum(Number[!Elimination])
  ) %>% 
  mutate(
    Error = c("Missed Elimination", "Missed Identification")[as.numeric(Ground.truth=="Same Source")+1]
  )
@

\subsection{Assessment of Classification Error}

\svp{
\autoref{fig:errors} shows the false positive and false negative rates for the six studies  investigated in this paper. 
Both AFTE errors (which do not consider inconclusives as errors and represent examiner errors) and process errors (which include inconclusives and represent errors attributable to evidence quality and lab policy) are shown; examiner error rates are low for both missed elimination and missed identifications in most studies (though intervals reveal the effect of sample size, which was also noted in e.g.  \citet{pcast-addendum2017}. 
Process errors, however, occur at higher rates and are particularly concerning for missed eliminations. 
There is consistently a much larger discrepancy between the two error rates for eliminations than for identifications; this phenomena is examined in more detail in \autoref{sec:inconclusives}.
Only a portion of the discrepancy between the examiner (AFTE) and process error rates for missed eliminations can be explained by lab policies that only allow eliminations based on mismatches in class characteristics. 
}

The Bunch \& Murphy study, for instance, was conducted at the FBI, where all examiners followed the same lab policy precluding eliminations based on class characteristic mismatches; as a result, the study has the largest difference between the two missed elimination rates.

\svp{Even though  \citet{Baldwin:2014bb} made efforts to control for the effects of lab policy in determination of inconclusive findings, the efforts were apparently not completely successful.}
\svp{This is in spite of the instructions given to participants:} ``A very important aspect of this work that needs to be clearly understood is that the study specifically asked participants not to use their laboratory or agency peer review process... Some [participants] indicated that the design of our study with all cartridges fired from the same model of firearm using the same type of ammunition would prohibit the use of a finding of elimination, while others used a mixture of inconclusive and elimination or did not use inconclusive at all to indicate a finding other than identification."

In  \citet{Duez:2017kha}, the authors report that ``13\% of examiners are not permitted to eliminate on individual characteristics (therefore, their conclusions of inconclusive are perfectly acceptable)." 
\hh{It is interesting to note that 7 of the examiners did not report eliminations due to  lab rules.} 
%\hh{These rules seem to be in contradiction to community accepted AFTE rules and need to be considered during reporting and testimony evaluations. XXX Bias against defense. XXX Needless to say that we do not agree with the authors' assessment that conclusions of inconclusives are perfectly acceptable.}

%These policies 
Policies which forbid elimination based on mismatch of individual characteristics alone \svp{seem to} directly contradict the AFTE Theory of Identification, which allows for eliminations based on ``significant disagreement of discernible class characteristics and/or individual characteristics". 
% \svp{XXX Depending on how you read this -- and if you're using the government legalese translator -- either interpretation is allowable under the rules...} % Added weasel words to fix the issue

% Other studies include examiners from many different labs and thus consist of some evaluations from labs which adhere to the class-characteristic elimination policy and some evaluations from labs which allow eliminations based on mismatch of individualizing characteristics.


The rates reported in this section are calculated directly from information reported in the papers without adjustments accounting for the use of deductive reasoning; as such, they represent the best possible case for error rates in studies which include multiple knowns, such as  \citet{Hamby:2018hu} and  \citet{bunch2003comprehensive}. 
More realistic estimates of error rates that account for the use of deductive reasoning can be found in \ref{sec:sim}.


<<errors, echo=FALSE, fig.width=8, fig.height=4,fig.cap="Percentages of missed eliminations and missed identifications by study. 95\\% Pearson-Clopper confidence intervals are drawn around the error estimates. Missed eliminations cannot be calculated for closed-set studies.", out.width=".8\\textwidth">>=


gg <- errors %>% gather(Type, Number, AFTE, Process, Elimination) %>% 
  mutate(
    Type = factor(Type, levels=c("AFTE", "Process", "Elimination"))
    ) 
levels(gg$Type) <- c(
  "AFTE error (Option 2)",
  "Process error (Option 3)",
  "Inconclusives are Eliminations (Option 4)")

gg %>%
  group_by(Type, Study, Error) %>%
  mutate(
    err_lower = clopper(0.05, Number, Comparisons)[1]*100,
    err_upper = clopper(0.05, Number, Comparisons)[2]*100,
    pos = c("white", "grey")[1 + as.numeric(Study) %% 2]
  ) %>%
  ggplot(aes(x = Number/Comparisons*100, y = Study)) +
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin =1 - 0.3, ymax = 1 + 0.6), fill = "grey90", alpha = .25, inherit.aes = F) + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin =3 - 0.3, ymax = 3 + 0.6), fill = "grey90", alpha = .25, inherit.aes = F) + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin =5 - 0.3, ymax = 5 + 0.6), fill = "grey90", alpha = .25, inherit.aes = F) + 
  geom_point(aes(shape=Type, colour = Type, y = as.numeric(Study)+0.3*(as.numeric(Type)-1.5)) ) +
  facet_grid(Error~.) + #, space="free") +
  theme_bw() +
  xlab("(Source-specific) Error Percentages") +
  xlim(c(0, NA)) +
  geom_errorbarh(aes(xmin=err_lower, xmax=err_upper, colour = Type, y=as.numeric(Study)+0.3*(as.numeric(Type)-1.5)),
                 size=0.5, height = 0.3,  alpha =0.9) +
  scale_y_continuous(breaks=1:6, labels=levels(errors$Study), minor_breaks = NULL) +
  scale_colour_brewer(palette="Dark2") +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(ncol=1))
@



% \svp{Technically these aren't really overall error rates, but they are the most commonly reported.}



\subsection{Source-specific error rates}
\svp{
The false positive rate and false negative rate reported in \autoref{fig:errors} are the error rates typically used to characterize a classification method. 
These rates, however, are functions of a larger set of probabilities which are calculated conditional on the known source of the evidence. 
In \autoref{fig:source-spec-cis}, we see the full set of source-specific conditional probabilities: the probability that an examiner will make an identification, elimination, or inconclusive determination given that the source of the evidence is the same (or different). 
Probabilities are shown with 95\% Pearson-Clopper confidence intervals, which provide a visual indication of the estimate's variability.
It is apparent that examiners are extremely good at working with same-source evidence: there are relatively few inconclusives, almost no missed identifications, and very few false eliminations. 
It is also evident that examiners have much more difficulty with different source evidence. 
Closed-set designs (used in  \cite{Hamby:2018hu} and  \cite{lyons}) do not even allow for the estimation of different-source probabilities due to the fact that it is not possible to estimate the number of different source comparisons which are made; this inherent bias has the unintentional effect of masking the fact that examers are not as accurate when evaluating different-source comparisons. 
While in most studies (with the exception of Bunch \& Murphy), the probability of making an inconclusive determination for different source evidence is below the probability of making an elimination, the probability that examiners will correctly make an elimination is still extremely low relative to the probability of a correct decision in the evaluation of same source evidence.
It may be that examiners are trained to look for similarities, instead of differences, or that it is simply more difficult to classify a difference as opposed to a similarity; some labs also do not allow eliminations to be made unless there is a class characteristic mismatch.
Regardless of the explanation, it is apparent that when presented with a different source comparison, examiners make an elimination much less frequently than an identification when presented with a same source comparison. 
}
<<source-spec-cis, echo=FALSE, fig.width=8, fig.height=5, fig.cap="Pearson-Clopper 95\\% confidence intervals for the probability of same source evidence given an examiner's conclusion. Expected values for each of the probabilities (further discussed in \\autoref{sec:inconclusives}) are shown as grey targets. \\hh{XXX there are no expected values} No assessments can be made about different-source specific probabilities when the total number of comparisons cannot be determined.", out.width=".8\\textwidth">>=

figdata <- source_spec_cis %>% 
  ungroup() %>%
  mutate(
    Decision = factor(Decision, levels=c("Identification", "Inconclusive", "Elimination"))
  ) 

figdata %>%
  ggplot(aes(y = pred_ss, x = Study, colour = Decision, shape = Decision)) +
  facet_grid(Ground.truth~.) +
  ylim(c(0,1)) +
  ylab("Probability of examiner decision given source") +
  theme_bw() +
  geom_hline(yintercept=c(0,1), colour = "grey20", size=0.25) +
  geom_errorbar(position = position_dodge(width = 0.6),
    aes(ymin = pred_ss_lower, ymax=pred_ss_upper, x=Study, group = Decision), 
    size=0.5, width = 0.5, alpha =0.9) +
  geom_point(position = position_dodge(width = 0.6), ) +
  scale_shape(solid = F) + 
  geom_errorbar(aes(ymin=xmin, ymax=xmax, x=Study), 
                 data = DontKnow_source_spec, linetype=2, width = 0.5, colour = "grey20") +
  geom_point(data = DontKnow_source_spec, shape='?', size = 3.5, colour = "grey20") + 
  coord_flip() + 
  theme(legend.position = "bottom") +
  scale_colour_brewer(palette="Dark2") +
   geom_col(aes(x = Study, y = I(1)), width = 1, colour = NA,
          alpha = 0.1, fill = "black", linetype = "solid",
          data = figdata %>%
 filter(Study %in% c("Lyons (2009)", "Bunch & Murphy (2003)", "Keisler (2018)"))) 
@


% \hh{\autoref{fig:inconclusives} are too high for different source comparisons.}
\subsection{Decision-specific error rates}\label{sec:dec-spec-res}

\svp{
In courtroom testimony, we do not know ground truth - we do not know whether the comparison that is presented actually is from the same source or a different source. 
As a result, we cannot use the source-specific error rates or probabilities shown in \autoref{fig:source-spec-cis} to justify the testimony and say something like ``the probability of an identification given that the evidence is from the same source is nearly 1".
In these situations, the information we know is the examiner's decision, so the opposite conditional probability is appropriate: the probability that the evidence is same-source given that the examiner made an identification.
\autoref{fig:cis} shows the probability of same source evidence conditional on the examiner's assessment.
Note that because there are two possible sources for the evidence (same or different), the equivalent probabilities for different-source evidence can be obtained by subtracting the same-source probability from 1.
}

Probabilities are shown with their respective 95\% (Pearson-Clopper) confidence intervals.  
In light grey, expected probabilities are shown based on the distribution of same source/different source comparisons and the number of participants in each study.

\svp{
If inconclusives occur because there are insufficient similarities or differences to draw a specific conclusion of identification or elimination, then they should be equally likely for same-source or different source comparisons. 
As a result, it is possible to calculate the ``target" probability for each decision: the target for identification is 1 - all same source pairs should result in an identification. 
Similarly the target for elimination is 0 - no same-source pairs should result in an elimination. 
Same-source pairs should be assessed as inconclusive at the same frequency as same-source comparisons in the study (\svp{this is explored in much more detail in the next section}).
It is very clear that the probability of same-source evidence given an inconclusive evaluation is much lower than the \svp{calculated} target probability.
The decision-specific probabilities provide indications that the distribution of inconclusives is much different than \svp{the distribution which was} hypothesized in \autoref{fig:shape-overview}. %purposefully understated.
}
<<cis, echo=FALSE, fig.width=8, fig.height=4.5, fig.cap="Pearson-Clopper 95\\% confidence intervals for the probability of same source evidence given an examiner's conclusion. Expected values for each of the probabilities as discussed in \\autoref{sec:inconclusives} are shown as grey targets. Some of the studies do not allow an assessment of all of the predictive values.", out.width=".8\\textwidth">>=

cis %>% 
  ungroup() %>%
  mutate(
    Decision = factor(Decision, levels=c("Identification", "Inconclusive", "Elimination"))
  ) %>%
  ggplot(aes(x = pred_ss, y = Study)) +
  facet_grid(Decision~.) +
  xlim(c(0,1)) +
  xlab("Probability for same source given examiner's decision") +
  theme_bw() +
#  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="red", size=4.5) +
#  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="white", size=3.5) +
  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="grey50", size=2.5) +
  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="white", size = 1.5) +
  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="grey50", size=.5) +
    geom_vline(xintercept=c(0,1), colour = "grey20", size=0.25) +
  geom_errorbarh(
    aes(xmin = pred_ss_lower, xmax=pred_ss_upper, y=Study), 
    size=0.5, height = 0.5, colour = "grey20", alpha =0.9) +
  geom_point() +
  geom_errorbarh(aes(xmin=xmin, xmax=xmax, y=Study), 
                 data = DontKnow, linetype=2) +
  geom_point(data = DontKnow, shape='?', size = 3.5)


@
\autoref{fig:cis} shows an overview  of the conditional probability of same source evidence given an examiner's conclusion for five published studies. 

\section{Inconclusives, Errors, and the Legal System}\label{sec:inconclusives}
% \svp{A lot of this is very repetitive considering all of the stuff that is now ahead of it. I think we can trim down some of the repetition and focus on making the conclusions crystal-clear. So start with why the distribution should be the same, but then show why that's the case for e.g. Baldwin, and what the reality is. Then go through Figure 7, explaining exactly why it's a problem. Survey the actual reasons for this (training focuses on identifications, lab policy, inconclusives are acceptable -- but maybe missing identifications is not?), then follow up with the conclusion: It doesn't matter what the reasons are - this is a fundamentally biased situation that leads to an imbalance in the scales of justice. Possibly introduce the ratio of guilty people freed to innocent people imprisoned. }

% 
% \hh{In the absence of knowing the truth we have to resort to working within a probabilistic framework to assess the likelihood of certain events.
% Some \svp{types of} evidence \svp{rest on assessments} of pattern matching: does the shoe print from the crime scene match a particular shoe? was the bullet found at a crime scene fired from a particular weapon? does the breech face impression on a cartridge case from the crime scene match the breech face of a particular weapon frame?}
% \svp{Other evidence, such as DNA, is based on formal measurement, but produces a certain signature or pattern which can be evaluated using probabilistic methods.}
% 
% In \svp{both of} these situations the fundamental quantity in question is the probability that two pieces of evidence come from the same source or from different sources.
% In a legal setting the opinion provided by the expert guides this assessment.
% \svp{This testimony is particularly important in pattern matching because there is no objective evaluation of the evidence}.
% So given expert testimony, how does or should the evaluation of the above probability change?
% 
% The AFTE theory of firearms and toolmarks identification as shown in \autoref{tab:afte} allows an expert to come to three main conclusions: make an identification, i.e. two pieces of evidence are thought to come from the same source, make an elimination, i.e. two pieces of evidence are tought to come from different sources, or an inconclusive, i.e. the evidence does not allow either an identification or an elimination.

\svp{In \autoref{sec:dec-spec-res}, we refer to the expected distribution of inconclusives, shown as grey targets in \autoref{fig:cis}.
The expected distribution of inconclusives rests on a fundamental premise: inconclusives should be equally likely to occur in the evaluation of same-source and different-source evidence. 
This is shown conceptually in \autoref{fig:shape-overview}: the rate of inconclusives is the same for both same-source and different-source evidence.
That is, under relatively ideal conditions (where there are inconclusives, but no errors according to the AFTE process), the expected conditional probabilities in an experiment would match those shown in \autoref{tab:ideal-probabilities}. 
}
% We are interested \svp{in} the probability of same source and different sources \svp{for each conclusion}, resulting in a total of six ($3 \times 2$) conditional probabilities. 
% \svp{Under ideal conditions?} In a perfect world, we would expect the probabilities shown in \autoref{tab:ideal-probabilities}:

\begin{table}
\caption{Expected conditional probabilities in a world where examiners do not make mistakes, but do make inconclusive decisions. }\label{tab:ideal-probabilities}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & $P(SS|\text{Identification}) = 1 $& $P(SS|\text{Inconclusive}) = P(SS)$ & $P(SS | \text{Elimination}) = 0$ \\
Different source & $P(DS| \text{Identification}) = 0$ & $P(DS| \text{Inconclusive}) = P(DS)$ & $P(DS | \text{Elimination})= 1$ \\\hline
\end{tabu}
}
\end{table}

% Most notably, we would expect inconclusive results not to show any dependence on the type of comparison, unless there is some evidence provided by experts that one type of conclusion is more difficult to make than another.\svp{I'm not even sure that would convince me - fundamentally, it's still important that legally the two are on equal footing, and it's the job of the training process to combat any asymmetry in difficulty level.}

In case work we are not able to assess the probability \svp{of} same source or different source \svp{comparisons}.  
However, those probabilities are easily accessible in formal studies or blinded proficiency testing,\svp{ as $P(SS)$ and $P(DS)$ in these studies are determined by the experimental design.}


% \svp{XXX I think this should go into a different section talking about convention, personally, even possibly citing  \citet{spiegelmanAnalysisExperimentsForensic2013} about standard operating procedures.}
% \hh{reporting of eliminations differs from lab to lab. Some labs do only allow firearm examiners to make eliminations based on differences in class characteristics. XXX quantify: baldwin, }

%The probability to make an identification  could be either reported directly or calculated as a composite of true positive and false positive rates:

% \[
% P(\text{identification made}) = TPR \times P(\text{same source}) + FPR \times P(\text{different source}) 
% \]

% \svp{By examining these studies}, we can use existing data to evaluate the conditional probability of same source versus different sources given an expert's testimony directly.


\svp{As an example, in the Baldwin study  \cite{Baldwin:2014bb}}, we would expect the probabilities for same and different source given an inconclusive result to be: 
\begin{eqnarray*}
P(\text{same source} \mid \text{Inconclusive}) &=& P(\text{same source}) = \frac{1}{3},  \\
P(\text{different source} \mid \text{Inconclusive}) &=& P(\text{different source}) = \frac{2}{3}
\end{eqnarray*}
\svp{This expectation is based on the study's design: 5 of the 15 comparisons were same-source; 10 were different-source. }
\svp{However, in Baldwin, as in the other studies, the probability of same-source evidence given an inconclusive is much lower than the expectation.
}

<<inconclusives, echo=FALSE, fig.width=8, fig.height=3.75,fig.cap="Probability of inconlusive decisions by same-source/different source comparisons. The probability of inconclusives for same source comparisons is generally low (with a 95\\% confidence interval well below 5\\%). Different source comparisons end up in inconclusive results significantly more often. The Bunch \\& Murphy and Baldwin studies have a particularly high percentage of inconclusive results.", out.width=".8\\textwidth", eval = F, include = F>>=

inc <- studies %>% group_by(Study, Ground.truth) %>% 
  filter(Type == "Observed") %>%
  summarize(
    Comparisons = sum(Number),
    Inconclusive = sum(Number[Decision=="Inconclusive"]),
    inc_lower = clopper(0.05, Inconclusive, Comparisons)[1],
    inc_upper = clopper(0.05, Inconclusive, Comparisons)[2]
  )

max_prob <- inc %>% ungroup() %>% filter(Ground.truth == "Different Source") %>% summarize(prop = max(inc_upper, na.rm = T)) %>% as.numeric()

DontKnow <- data.frame(
  x = max_prob/2, 
  xmin=c(0,0,0,0), 
  xmax=max_prob*c(1,1,1,1), 
  Study = c("Lyons (2009)", "Hamby (2019)", "Lyons (2009)", "Hamby (2019)"), 
  Ground.truth="Different Source")

inc %>% 
  ggplot(aes(x = Inconclusive/Comparisons, y = Study)) +
  geom_errorbarh(aes(xmin = inc_lower, xmax=inc_upper, y=Study),
                 size=0.5, height = 0.5, colour = "grey20", alpha =0.9) +
  geom_point() +
  theme_bw() +
  facet_grid(Ground.truth~.) +
  xlab("Conditional probability of inconclusive decisions") +
  geom_errorbarh(aes(x = 0, xmin=xmin, xmax=xmax, y=Study),
                 data = DontKnow, linetype=2,
                 size=0.5, height = 0.5, colour = "grey20", alpha =0.9) +
  geom_point(x = max_prob/2,  data = DontKnow, shape='?', size = 3.5)
@

\svp{
This finding is similar to the implications of \citep{biedermannAreInconclusiveDecisions2019}, though they do not follow their reported results to the logical conclusion. 
Specifically, they show that there is information in the finding of an inconclusive that is useful: inconclusives are much more likely to occur when the evidence is different-source. 
We take this conclusion one step further: if inconclusives are essentially another way to say ``different source", then it makes sense to get rid of the category and declare inconclusives to be equivalent to elimination, at least when evaluating error rates. 
}

% \svp{In the next section, we explore the implications of treating inconclusives as equivalent to elimination for the purposes of evaluating the errors in the firearms examination process. 
% }

\svp{
Calculations are shown below for \citep{Baldwin:2014bb}, which is the only study large enough to have nonzero observed counts for all cells.
}


\begin{eqnarray*}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{rcccr}
P (\text{same source} \mid \text{elimination}) &=& \displaystyle\frac{4}{1425} &=& 0.0028 \\
P (\text{same source} \mid \text{inconclusive or elimination}) &=& \displaystyle\frac{11+4}{748+1425} &=& 0.0069 \\
%P (\text{different source} \mid \text{inconclusive or elimination}) &=& \displaystyle\frac{737+1421}{748+1425} &=& 0.9931 \\ % This isn't mentioned in the text description - I suspect the calc shown below was possibly meant instead?
P (\text{different source} \mid \text{identification}) &=& \displaystyle\frac{22}{1097} &=& 0.0201
\end{array}
\end{eqnarray*}

<<asymmetric-errors, echo = F, fig.width = 8, fig.height = 3, fig.cap = "Decision-specific error rates. XXX needs more text, obviously, but essentially, for all studies errors on identification are higher than errors on eliminations (Same on Keisler). Even if inconclusives are included in eliminations the rates are not significantly higher than for eliminations.  Confidence intervals for wrong identifications do not overlap, indicating that  these errors are significantly higher than wrongful eliminations.">>=
# This was to create a similar chart as the calculations for Baldwin, but Baldwin is the only study with enough data for the calculations to be meaningful...
elimplus <- studies %>%
  dplyr::filter(Decision != "Identification", 
                Type == "Observed") %>%
  group_by(Study, Ground.truth) %>%
  summarize(
    Decision = "Elimination + Inconclusive", 
    Number = sum(Number)
    )


errs <- data.frame(
  Ground.truth = c("Same Source", "Same Source", "Different Source"),
  Decision = c("Elimination", "Elimination + Inconclusive", "Identification"),
  stringsAsFactors = FALSE
)

# Decision-specific
errors2 <- studies %>% 
  filter(Type == "Observed") %>%
  select(-Type) %>%
  bind_rows(elimplus) %>%
  group_by(Study, Decision) %>%
  mutate(Total = sum(Number)) %>%
  right_join(errs) %>%
  mutate(
    rate = Number/Total,
    lower = clopper(0.05, Number, Total)[1],
    upper = clopper(0.05, Number, Total)[2]
    ) %>%
  ungroup() 

errors2 %>%
  arrange(Study) %>%
  mutate(
    Decision = factor(Decision, levels = c("Identification", "Elimination", "Elimination + Inconclusive")),
    ErrorType = factor(c("Identification of DS", "Elimination of SS", "Elimination or Inconclusive of SS")[as.numeric(Decision)]),
    ErrorType = factor(ErrorType, levels = c("Identification of DS", "Elimination of SS", "Elimination or Inconclusive of SS"))
  ) %>%
  filter(!(Study %in% c("Hamby (2019)", "Lyons (2009)", 
                        "Bunch & Murphy (2003)"))) %>%
  ggplot(aes(x = rate*100, y = Study)) +
  geom_hline(yintercept = "Duez (2018)", colour = "black",
             size=24, alpha = 0.1) +
  geom_hline(yintercept = "Baldwin (2014)", colour = "black",
             size=24, alpha = 0.1) +
  geom_point(aes(colour = ErrorType, shape = ErrorType),
             position = position_dodge(width = 0.7),
             size = 2.5) + 
  geom_errorbarh(
    aes(xmin = lower*100, xmax = upper*100, 
        colour = ErrorType), 
    position = position_dodge(width = 0.7)) +
  scale_shape_discrete("Type of Error") +
  scale_colour_manual("Type of Error", values = c("steelblue", "darkorange", "brown")) +
  theme_bw() +
#  theme(legend.position = "bottom",
#        legend.direction = "vertical") + 
  xlab("Error Percentages (decision-specific conditional probabilities) ") 


# # Source-specific
# studies %>% 
#   filter(Type == "Observed") %>%
#   select(-Type) %>%
#   bind_rows(elimplus) %>%
#   group_by(Study, Ground.truth) %>%
#   mutate(Total = sum(Number)) %>%
#   right_join(errs) %>%
#   mutate(rate = Number/Total) %>%
#   ungroup() %>%
#   arrange(Study)
@

\svp{
Examining the experimentally determined error rates for firearms and toolmark identification leads to a disturbing realization. 
There is an order of magnitude difference between the probability of same-source evidence given an elimination and the probability of different-source evidence given an identification.
\autoref{fig:baldwin-shape-overview} provides a visual demonstration of this discrepancy: unlike in \autoref{fig:shape-overview}, there is a difference in the density of points in the inconclusive sections of the chart.
}

<<baldwin-shape-overview, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas) as reported in \\cite{Baldwin:2014bb}. Each dot represents approximately 2 examiner evaluations. Unlike in \\autoref{fig:shape-overview}, there is a significant difference in the probability of an inconclusive evaluation when a pair is different source or same source. ", fig.width = 6, fig.height = 4, dpi = 1000, dev="pdf", out.width = "70%">>=
baldwin <- c(a = 1075, b = 11, c = 4, d = 22, e = 737, f = 1421)

do.call(target_plot, as.list(c(round(baldwin/2), pointsize = .5, bordersize = 0.001)))
# target_plot(1075, 11, 4, 22, 737, 1421, pointsize = .5, bordersize = .001) # too dense
@
<<keisler-shape-overview, eval = T, include = F, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas) as reported in \\cite{Baldwin:2014bb}. Each dot represents approximately 2 examiner evaluations. Unlike in \\autoref{fig:shape-overview}, there is a significant difference in the probability of an inconclusive evaluation when a pair is different source or same source. ", fig.width = 6, fig.height = 4, dpi = 1000, dev="pdf", out.width = "70%">>=
keisler <- c(a = 1508, b = 4, c = 0, d = 0, e = 203, f = 805)

do.call(target_plot, as.list(c(keisler, pointsize = .5, bordersize = 0.001)))
# target_plot(1075, 11, 4, 22, 737, 1421, pointsize = .5, bordersize = .001) # too dense
@
<<duez-shape-overview, eval = T, include = F, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas) as reported in \\cite{Baldwin:2014bb}. Each dot represents approximately 2 examiner evaluations. Unlike in \\autoref{fig:shape-overview}, there is a significant difference in the probability of an inconclusive evaluation when a pair is different source or same source. ", fig.width = 6, fig.height = 4, dpi = 1000, dev="pdf", out.width = "70%">>=
duez <- c(a = 276+59, b = 1, c = 0, d = 2, e = 25, f = 85)

do.call(target_plot, as.list(c(duez, pointsize = .5, bordersize = 0.001)))
# target_plot(1075, 11, 4, 22, 737, 1421, pointsize = .5, bordersize = .001) # too dense
@
<<bunch-shape-overview, eval = T, include = F, fig.cap = "Sketch of the relationship between  ground truth of evidence (dots) and examiners' decisions (shaded areas) as reported in \\cite{Baldwin:2014bb}. Each dot represents approximately 2 examiner evaluations. Unlike in \\autoref{fig:shape-overview}, there is a significant difference in the probability of an inconclusive evaluation when a pair is different source or same source. ", fig.width = 6, fig.height = 4, dpi = 1000, dev="pdf", out.width = "70%">>=
bunch <- c(a = 70, b = 0, c = 0, d = 0, e = 172, f = 118)

do.call(target_plot, as.list(c(bunch, pointsize = .5, bordersize = 0.001)))
# target_plot(1075, 11, 4, 22, 737, 1421, pointsize = .5, bordersize = .001) # too dense
@
\hh{In fact, even if inconclusives and eliminations are combined,} \svp{that is, if we introduce an artificial binary classification scheme,} \hh{the error rate of making a false elimination is 0.0069, only a third of the error of a false identification.}
\svp{Note that while the probability of same source given an elimination is affected by lab policies towards the treatment of inconclusives, these policies do not explain the continued discrepancy in error probabilities when inconclusives and eliminations are considered together.
}

% \subsection{What will examiners bet on?}
% \subsection{Equitable Errors?} % Something that isn't quite as cavalier as "betting" and gets at the heart of the question better - which error is more likely?

\svp{
Under a system with equal consequences for each type of error (false elimination or missed identification), we would ideally want the probability of each type of error to be the same - that is, it should be equally likely to make an error in favor of the prosecution or in favor of the defense. 
In the legal system, however, there are not equal consequences for each type of error. 
If a person is guilty, there are usually multiple pieces of evidence which contribute information used by the judge or jury to make a decision; thus, the consequences of an elimination of same-source evidence are generally relatively low. 
However, there are many examples of convictions which were made on the basis of faulty forensic evidence and led to the unjust imprisonment or execution of individuals who were innocent. 
As a society, we generally adhere to the principle that false convictions are more heinous than allowing a guilty person to go free.  
This principle, known as Blackstone's ratio (``It is better that ten guilty persons escape than that one innocent suffer.") would suggest that equal probability of each type of error is insufficient. 
In accordance with this principle, it would be preferable for $P(\text{SS} | \text{Elimination})$ to be higher than $P(\text{DS} | \text{Identification})$. 
}

\svp{Unfortunately, this is not what we find.}
\svp{In \autoref{fig:asymmetric-errors}, we see that in every experiment with sufficient data, the point estimate of the probability that an identification is from a different-source pair is higher than the probability that an elimination is from a same-source pair.}
\svp{Even when inconclusives are treated as as eliminations, the only study with sufficient sample size to show significant differences between error rates still shows that the probability of different source evidence given an examiner identification is significantly higher than the probability of same-source evidence given an examiner elimination or inconclusive.}
\svp{Note that \citet{Duez:2017kha} and \citet{keisler} have insufficient sample size to distinguish between the probabilities with statistical significance.}
\svp{However, in \citet{Baldwin:2014bb}, it is clear that} examiners are willing to gamble on an indentification much more than they are willing to gamble on an elimination. 
In the courtroom \svp{(or before, in a plea bargin situation)}, this results in a clear bias toward the prosecution (or against the defense).

\svp{There are many ways this bias could arise: there are, of course, well documented motivational and cognitive biases \cite{motivationalcognitivebiases, robertson2016blinding} which may affect examiners. 
In addition, it may be easier to identify similarities than to explicitly identify dissimilarities. 
Examiner training may primarily focus on making identifications, rather than spending equal time on elimination and identification. 
Whatever the source of the bias, however, one thing is clear: the discrepancy between the decision-specific conditional probabilities corresponding to errors is fundamentally unjust, and the bias is in the opposite direction of the ideal expressed by Blackstone.
This problem must be addressed for forensic firearms and toolmark examination to be used equitably. 
}

%Then lead into the discussion, which can bring this back around to a more general PPV/NPV good, FPR/FNR bad, inconclusives are a problem both for error rate calculations and for a fair legal system. System-wide change is needed. 

\section{Recommendations and Conclusions}
% Recap study weaknesses: 
% \begin{itemize}
% \item number of firearms and firearm types very small
% \item all good studies are of cartridge cases
% \item well-designed studies are essential for proper assessment of error rates
% \end{itemize}

\hh{Baldwin study is gold standard for administration of a controlled study: open set, single known source for each comparison, limited information which can be shared between participants}

\hh{downside of Baldwin: treatment of inconclusives and error rate}

\svp{It is clear from our assessment of the currently available studies that there is significant work to be done in examining error rates of different components of firearms and toolmark analysis.
In particular, there is a great need for studies which are both large (many examiners and many evaluations) and well designed (single known source for each comparison, open set, with limited information which can be shared between participants). 
All of the published studies which meet the design criteria analyze cartridge cases; it is imperative that similarly well designed studies examining bullets and toolmarks be conducted and published in order to validate the entire discipline of firearms and toolmark analysis.}

% Recap error rate and inconclusive options, and why these options are suboptimal in source-specific error rate classification. Argue for use of decision-specific probabilities in court because they condition on known information, while using source-specific probabilities to evaluate individual or even lab performance. 

\svp{While published studies vary in their treatment of inconclusive decisions when calculating and reporting error rates or accuracies, in most studies, what are reported as error rates are what we have termed source-specific conditional probabilities. 
We propose that these error rates be calculated for the examiner and the process separately. 
The examiner-specific error rates would not count inconclusives as errors, but the process-specific error rates would count inconclusives as errors because they reflect the failure of the evidence to record sufficient identifying information to make a correct determination.
These two separate summary measures would be used differently - the examiner error rates would be used for evaluation within the forensics lab setting (for e.g. qualification purposes), but the process error rates would be reported in court, because they are germane to the question of interest in court: how does the evidence contribute to information about the suspect's guilt or innocence?}

\svp{We also argue that a simpler alternative to this treatment of error rates in legal settings would be to report the decision-specific conditional error rates: the probability that, given an examiner's conclusion, the conclusion is incorrect.
These rates provide the most relevant characterization of error in a court setting, where ground truth is not known, but the examiner's decision is known information. 
In addition, using decision-specific error rates is advantageous because it side-steps the need to treat inconclusives differently than identification or elimination. 
} 

Specifically, in court, we suggest that when the admissibility of an examiner's testimony is assessed, the examiner is asked to state the rate at which they make inconclusive decisions, along with any relevant error rates specific to the lab. We also suggest that during cross-examination after the testimony was presented, the defense ask about the decision-specific probability of an error (relative to the examiner's decision), which will provide specific information which is relevant to the testimony at hand.

% Discuss the systematic biases which all work against the defendant: 
% \begin{itemize}
% \item error rate studies often have more same-source comparisons than what would be expected. 
% \item Some studies are designed such that it is not possible to determine the error rate for different-source comparisons.
% \item The discipline as a whole is focused on examiner error rates, rather than the treatment of the entire process. \svp{This is the only one which may not be completely against the defense}
% \item Examiners have a lower threshold for identification than for elimination - so when a comparison is different-source, examiners are more likely to say that it is inconclusive than they are when a comparison is same-source
% \item There is a systematically higher probability of different source evidence given an examiner's identification than the probability of same source evidence given an examiner's elimination. This runs against the principles which form the foundation of our legal system.
% \item This bias \svp{ against the defense in inconclusives mirrors biases in admissibility of forensic evidence  \cite{morenoWhatHappensWhen2004, epsteinPreferringWiseMan2014, roachFORENSICSCIENCEMISCARRIAGES2009}}
% \end{itemize}

\svp{In the process of examining these different error rate calculations, we discovered a series of systematic biases in error rate studies and the examination process which all work against the defendant. 
In many studies, there are more same-source comparisons than what would be expected in case work: this ensures that false elimination error rates can be estimated more precisely than false identification error rates. 
In fact, in some common study designs, it is only possible to estimate the false elimination error rate; the rate of false identifications cannot be estimated at all. 
This, too, ensures that there is no way to invalidate the examiner's testimony on the basis that it might contribute to a false conviction of the defendant. 
In addition, examiners clearly have a lower threshold for identification than for elimination; when evidence is different-source, examiners are more likely to say that it is inconclusive than they are when the evidence is same-source. 
As a result, there is a systematically higher probability of different-source evidence given an examiner's identification than the probability of same-source evidence given an examiner's elimination. 
This fundamentally contradicts the principles which form the foundation of our legal system. 
These biases mirror biases in the admissibility of forensic evidence which are also more likely to favor the prosecution\cite{morenoWhatHappensWhen2004, epsteinPreferringWiseMan2014, roachFORENSICSCIENCEMISCARRIAGES2009}.
}

% Future work: Examine the distribution of automated scores (RF score, CCF) and cutoff-based classifications compared to examiner decisions. Are algorithms less biased than examiners? (yes, duh). 

\svp{
The biases identified in this paper increase the importance of the development and implementation of automatic, unbiased assessment algorithms in forensic laboratories. 
It is important that before these algorithms are applied to casework, we examine the distribution of automated scores and cutoff-based classifications compared to examiner decisions to determine whether the algorithms are, in fact, less biased than examiners. 
}
\svp{
XXX need to figure out how to draw this to a close more elegantly.
}



% \svp{Some mentions of quality considerations in  \cite{epsteinPreferringWiseMan2014} -  \cite{UNITEDSTATESAmerica2011} (fingerprints) - need for a precise standard of what makes something high-quality enough for analysis/comparison}
% 
% \svp{Design problems: PCAST Report addendum \cite{pcast-addendum2017}: }
% \begin{quote}\svp{The firearms discipline clearly recognizes the importance of empirical studies. However, most of these studies used flawed designs. As described in the PCAST report, ``set-based" approaches can inflate examiners' performance by allowing them to take advantage of internal dependencies in the data. The most extreme example is the ``closed-set design", in which the correct source of each questioned sample is always present; studies using the closed-set design have under estimated the false-positive and inconclusive rates by more than 100-fold. This striking discrepancy seriously undermines the validity of the results and underscores the need to test methodsunder appropriate conditions. Other set-based designs also involve internal dependencies that provide hints to examiners, although not to the same extent as closed-set designs.}
% 
% \svp{To date, there has been only one appropriately designed black-box study: a 2014 study commissioned by the Defense Forensic Science Center (DFSC) and conducted by the Ames Laboratory, which reported an upper 95\% confidence bound on the false-positive rate of 2.2\%.}
% \end{quote}

%\section{References}

\bibliography{bibfile}
\clearpage
\appendix
\section{Study Summaries and Results}\label{sec:supplement}
\paragraph{Baldwin} \svp{The Baldwin study  \cite{Baldwin:2014bb} was designed such that each test} kit consists of 15 sets of 3 known cartridge cases and 1 questioned cartridge case. In 5 of the 15 sets the questioned cartridge was from the same source as the knowns, while the other 10 questioned cartridges were from different sources as their respective knowns.

25 firearms were used for the study, such that within each kit no firearm was re-used for either knowns or questioned cartridge cases, i.e. no additional information could be gained by comparing any cartridge cases across sets.

Results are summarized in \autoref{tab:baldwin-summary}. 

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked for accuracy on 2020-04-24
\captionsetup{type=table}\captionof{table}{Baldwin study results, conclusion-specific and source-specific probabilities, and reported overall error rates. }\label{tab:baldwin-summary}
{\large\bf A Study of False-Positive and False-Negative Error Rates in Cartridge Case Comparisons}
\emph{ \citeauthor{Baldwin:2014bb} ( \citeyear{Baldwin:2014bb})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# SS Comparisons & \# DS Comparisons & \\\hline
Open set & 5 & 10 & 218 examiners\\
\end{tabularx}

\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same Source & 1075  & 11 & 4 & 1090\\
Different source & 22 & 735+2\footnote{Two comparisons were not reported and are considered to be inconclusives.} & 1421 & 2180\\\hline
\multicolumn{1}{Y}{Conclusion Total} & 1097 & 748 & \multicolumn{1}{r}{1425} & \multicolumn{1}{Y}{3270}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9799 & 0.0147 & 0.0028 & 0.3333\\
Different source & 0.0201 & 0.9853 & 0.9972 & 0.6667\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.3355 & 0.2287 & \multicolumn{1}{r}{0.4358} & \multicolumn{1}{Y}{1.0000}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9862 & 0.0101 & 0.0037 & 0.3333\\
Different source & 0.0101 & 0.3381 & 0.6518 & 0.6667\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.3355 & 0.2287 & \multicolumn{1}{r}{0.4358} & \multicolumn{1}{Y}{1.0000}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0037  & 0.0101 & 0.0080\\
3 & Process error & 0.0138 & 0.3482 & 0.2367 \\
\end{tabularx}
\end{fmpage}

\clearpage
\paragraph{Keisler} \svp{The Keisler Study  \cite{keisler} was designed so that} each test kit consisted of sets of 20 pairs of cartridge cases from Smith \& Wesson pistols, where 12 of the pairs were from the same source and 8 pairs were from different sources. 

Kits were assembled using only 9 Smith \& Wesson pistols (i.e. there is a potential to gain additional information by making comparisons across sets). However, \svp{participants were instructed to only compare single pairs.} 

Results from the study are shown in \autoref{tab:keisler-summary}.

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked for numerical accuracy 2020-04-24
\captionsetup{type=table}\captionof{table}{{\large\bf Isolated Pairs Research Study} \label{tab:keisler-summary}}
\emph{ \citeauthor{keisler} ( \citeyear{keisler})}\\
A study of Smith \& Wesson cartridge cases.
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# SS Comparisons & \# DS Comparisons & \\\hline
Open set & 12 & 8 & 126 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same source & 1508 & 4 & 0 & 1512 \\
Different source & 0 & 203 & 805 & 1008 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 1508 & 207 & \multicolumn{1}{r}{805} & \multicolumn{1}{Y}{2520}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.0000 & 0.0193 & 0.0000 & 0.6000\\
Different source & 0.0000 & 0.9807 & 1.0000 & 0.4000\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.5984 & 0.0821 & \multicolumn{1}{r}{0.3194} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9974 & 0.0026 & 0.0000 & 0.6000\\
Different source & 0.0000 & 0.2014 & 0.7986 & 0.4000 \\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.5984 & 0.0821 & \multicolumn{1}{r}{0.3194} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0000 & 0.0000  & 0.0000 \\
3 & Process error & 0.0026 & 0.2014 & 0.0821 \\
\end{tabularx}
\end{fmpage}

\clearpage
\paragraph{Duez} The Duez study  \cite{Duez:2017kha} used virtual microscopy to evaluate scans of cartridge cases. Each participant was asked to make eight evaluations of breech face impressions. These consisted of two sets: 
\begin{itemize}
\item[CCTS1] set of three knowns, four questioned breech face impressions. All questioned breech face impressions are from the same source as the knowns. 
\item[CCTS2] set of three knowns, four questioned breech face impressions. Two questioned breech face impressions are from the same source, two are from different sources.
\end{itemize}

Both sets were evaluated by 56 participants (46 fully certified examiners and 10 trainees). CCTS1 resulted in 56 x 4 correct identifications.
The design of the experiment does not allow us to quantify all of the quantities to evaluate examiner performance unless we aggregate performance over both sets. \autoref{tab:duez-summary} shows the results of this aggregation.

%Combined, the probability for a  same source pair is 6/8 or 75\% and the probability for  a different source  pair is 2/8 or 25\%.

%The top of  shows results reported by 56 participants (46 examiners, 10 trainees).

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{Duez study results, conclusion-specific and source-specific probablities, and reported overall error rates. For the conditional probabilities, 
the differences in certified examiners and trainees are not large, so only the aggregate results are shown. Certified examiners have a perfect identification rate, only evaluations of different source pairs lead to inconclusives. }\label{tab:duez-summary}
{\large\bf Development and Validation of a Virtual Examination Tool for Firearm Forensics}
\emph{ \citeauthor{Duez:2017kha} ( \citeyear{Duez:2017kha})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# SS Comparisons & \# DS Comparisons & \\\hline
Open set & 6 & 2 & 46+10 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same source & 276+59 & 0+1 & 0+0 & 276+60 \\
Different source & 0+2 & 12+13 & 80+5 & 92+20 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 276+61 & 12+14 & \multicolumn{1}{r}{80+5} & \multicolumn{1}{Y}{449}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9941 & 0.0385 & 0.0000 & 0.7500 \\
Different source & 0.0059 & 0.9615 & 1.0000 & 0.2500 \\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.7522 & 0.0580 & \multicolumn{1}{r}{0.1897} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9970 & 0.0030 & 0.0000 & 0.7500\\
Different source & 0.0179 & 0.2232 & 0.7589 & 0.2500\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.7522 & 0.0809 & \multicolumn{1}{r}{0.1897} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0000/0.0000  & 0.0000/0.1000 & 0.0000/0.0303\\
3 & Process error & 0.0000/0.0167 & 0.1304/0.7500 & 0.0326/0.2000\\ \hline
\end{tabularx}
\end{fmpage}

\clearpage
\paragraph{Brundage-Hamby} The Brundage-Hamby study consists of sets of 20 test fires from known barrels and 15 questioned bullets. The 20 known test fires are 2 bullets from each of ten consecutively manufactured barrels. The Brundage-Hamby study is a closed set study, i.e. the 15 questioned bullets are known to be fired from one of these ten barrels. Participants (firearm examiners) are asked to identify which of the knowns a questioned bullet matches. 

The study was originally reported on by Brundage in 1998  \cite{brundage}. Updates on the study with increasing number of responses have been published several times since   \cite{hamby, Hamby:2018hu}. 
The design of the Brundage-Hamby is well known in the forensics community and has been copied in  \citet{fadul} for a study of cartridge cases of the same firearms. Slight modifications of the study design are also common  \cite{lyons}. 

Results reported in the 2019 paper are shown in \autoref{tab:hamby-summary}.

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{Evaluations, conditional error rates, and overall error rates from the combined Brundage-Hamby studies. Note the focus on identifications.}\label{tab:hamby-summary}
{\large\bf A Worldwide Study of Bullets Fired From 10 Consecutively Rifled 9MM RUGER Pistol Barrels - Analysis of Examiner Error Rate}
\emph{ \citeauthor{Hamby:2018hu} ( \citeyear{Hamby:2018hu})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Closed set & 10 & 15 & 507 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same Source & 10447 & 8 & 0 & 10455 \\
Different source & 0 & ? & ? & 47047.5\footnote{This number is imputed based on the average number of pairwise comparisons an examiner would have to do to complete a Hamby study. Details can be found in the supplemental material.} \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 10447 & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{57502.5}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.0000 & ? & 0.0000 & 0.1818\\
Different source & 0.0000 & ? & 1\footnote{The elimination-specific conditional probabilities are only identifiable in this case because there were not any false eliminations. In any other situation, we would not be able to calculate these probabilities because we do not have the total number of eliminations.} & 0.8282\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.1817 & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9992 & 0.0008 & 0.0000 & 0.1818\\
Different source & 0.0000 & ? & ? & 0.8282\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.1817 & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0000 & ? & ?\\
3 & Process error & ? & ? & ? \\\hline
\end{tabularx}
\end{fmpage}

\clearpage
\paragraph{Bunch \& Murphy Study}  \cite{bunch2003comprehensive}
\svp{XXX description introduction to study?}

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{Results and summary tables for the Bunch and Murphy study. Values in this table are as reported in the study; however, due to the study's structure, it is possible to perform fewer comparisons, because not all comparisons are independent. Results for independent comparisons are shown in \autoref{tab:bunch-indep}}\label{tab:bunch-summary}
{\large\bf A Comprehensive Validity Study for the Forensic Examination of Cartridge Cases}\emph{ \citeauthor{bunch2003comprehensive} ( \citeyear{bunch2003comprehensive})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Open set & variable & variable & 8 FBI examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Reported (Nominal) Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same source & 70 & 0 & 0 & 70 \\
Diff source & 0 & 172 & 118 & 290 \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 70 & 172 & \multicolumn{1}{r}{ 118} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf (Nominal) Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1 & 0 & 0 & 0.2414\\
Diff source & 0 & 1 & 1 & 0.7586\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.1944 & 0.4778 & \multicolumn{1}{r}{0.3278} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf (Nominal) Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.0000 & 0.0000 & 0.0000 & 0.2414\\
Diff source & 0.0000 & 0.5931 & 0.4069 & 0.7586\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.1944 & 0.4778 & \multicolumn{1}{r}{0.3278} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf (Nominal) Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0000 & 0.0000 & 0.0000\\
3 & Process error & 0.0000 & 0.5931 & 0.4778 \\
\end{tabularx}
\end{fmpage}
\clearpage
\paragraph{Lyons} The Lyons study  \cite{lyons} examined marks made by 10 consecutively manufactured extractors (manufactured by Caspian Arms Ltd).

Kits were assembled from 32 cartridge cases: 20 known cartridge cases from pairs of 2 cartridges from each of the 10 extractors (the knowns) and 12 questioned cartridges, such that each known corresponded to at least one questioned, with some replication in most of the kits (one kit accidentally only had ten questioned cartridges). Thus, the setup of this study is similar to the Brundage-Hamby study. This study suffers from the same problems as the Brundage-Hamby study: it is a closed set study with multiple knowns and asks for identifications only. We can therefore only estimate a fraction of the relevant error rates. 
It is also not possible to determine the total number of independent different source comparisons. 
\svp{The study results are shown in \autoref{tab:lyons-summary}, along with computed conclusion-specific and source-specific probabilities and error rates.}

\noindent\begin{fmpage}{.985\textwidth}\centering % Checked numbers 2020-04-24
\captionsetup{type=table}\captionof{table}{}\label{tab:lyons-summary}
{\large\bf The Identification of Consecutively Manufactured Extractors} \emph{ \citeauthor{lyons} ( \citeyear{lyons})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Closed set & 10 & 10~or~12 & 15 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same Source & 174\footnote{ \citet{lyons} reports 175 correct identifications, but it is clear from the discussion that one of those same source identifications was actually an inconclusive. 12 answer sheets with 12 correct identifications, one with 10 (out of 10) correct identifications, one with 9 correct identifications and 3 errors, and one with 11 correct identifications and one inconclusive. So 12$\cdot$12 + 10 + 9 + 11 = 174. } &  1 & 3 & 178 \\
Different source & 3 & ? & ? & ? \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 177 & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{?}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9831 & ? & ? & ?\\
Different source & 0.0169 & ? & ? & ?\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & ? & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 0.9775 & 0.0056 & 0.0169 & ?\\
Different source & ? & ? & ? & ?\\\hline
\multicolumn{1}{Y}{Conclusion Probability} & ? & ? & \multicolumn{1}{r}{?} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Overall Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0169 & ? & ?\\
3 & Process error & 0.0225 & ? & ? \\\hline
\end{tabularx}
\end{fmpage}


\section{Simulation of comparisons in studies with multiple knowns}\label{sec:sim}

\subsection{Bunch and Murphy simulation}\label{sec:bunchmurphysim}
\svp{In \citet{bunch2003comprehensive}, there are two sources of unknown information: the composition of the study test kits, and potential examiner use of deductive logic. 
While it is possible to assess the effect of deductive logic by eliminating redundant comparisons systematically, because the composition of the test kits was random, there is not a single value for the reduction in comparisons due to the use of deduction.
As a result, we must first simulate the composition of a test kit, and then evaluate the minimal number of comparisons which must be completed, if deductive reasoning is used.}

\svp{The process for assembling the test kits provides sufficient information to simulate the composition of the eight test kits used in the study.}
\svp{Using the simulation method, we created 500,000 sets of 8 test kits; from these simulated sets, we identified any which match the reported values of 70 same-source and 290 possible different-source comparisons (287,536). We considered adding the restriction of 45 consecutively manufactured comparisons, but found that this reduced the number of simulations to 6,416; given that we do not assess the consecutively manufactured comparisons separately, adding this restriction was deemed unnecessary. Using these sets, we can then estimate the minimal number of comparisons which are necessary using deductive reasoning in addition to examination. As an independent comparison is one in which the examiner has no relevant prior information about either of the cartridges, the minimal set of comparisons necessary to evaluate all cartridges would also be the set of independent comparisons. }

\svp{R code to reproduce this simulation (and the deductive reasoning algorithm) can be found at \url{https://gist.github.com/srvanderplas/9cb0268df99e97a9ce327bb1489f7046}. }

\noindent\begin{fmpage}{.985\textwidth}\centering
\captionsetup{type=table}\captionof{table}{Results and summary tables for the Bunch and Murphy study, with only independent pairwise comparisons included. 95\% bootstrap intervals are provided for quantities  estimated via simulation. To allocate the inconclusives and eliminations, we used the rule stated in  \cite{bunch2003comprehensive}: FBI examiners can exclude only on class characteristic mismatches. It should be noted that in none of the 287,536 simulations which matched the other set criteria did we find a set which had more than 109 class characteristic mismatches, suggesting that there may have been some eliminations made on individual characteristics, but as this cannot be confirmed, estimates of eliminations only include class characteristic mismatches. }\label{tab:bunch-indep}
{\large\bf A Comprehensive Validity Study for the Forensic Examination of Cartridge Cases}\emph{ \citeauthor{bunch2003comprehensive} ( \citeyear{bunch2003comprehensive})}\\
\begin{tabularx}{\textwidth}{XrrX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Open set & variable & variable & 8 FBI examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|rrr|Y}
\multicolumn{5}{c}{\bf Independent Comparisons}\\
\multicolumn{5}{c}{Mean (95\% CI)}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Total\\\hline
Same source & 28.7 [26, 31] & 0 & 0 & 28.7 [26, 31] \\
Diff source & 0 & 156.5 [134, 181] & 42.5 [22, 64] & 199.1 [184, 217] \\\hline
\multicolumn{1}{Y}{Conclusion Total} & 28.7 [26, 31] & 156.5 [134, 181] & \multicolumn{1}{r}{42.5 [22, 64] } & \multicolumn{1}{Y}{}\\\hline\hline
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Independent Comparisons: Conclusion-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.000 & 0.000 & 0.000 & 0.126 [0.110, 0.142]\\
Diff source & 0.000 & 1.000 & 1.000 & 0.874 [0.858, 0.890] \\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.126 [0.110, 0.142] & 0.687 [0.597, 0.779] & \multicolumn{1}{r}{0.187 [0.095, 0.278]} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Independent Comparisons: Source-Specific Probabilities}\\
\multicolumn{1}{r}{} & Identification & Inconclusive & \multicolumn{1}{r}{Elimination} & Source Probability\\\hline
Same source & 1.000 & 0.000 & 0.000 & 0.126 [0.110, 0.142]\\
Diff source & 0.000 & 0.787 [0.682, 0.891] & 0.214 [0.109, 0.318] &  0.874 [0.858, 0.890] \\\hline
\multicolumn{1}{Y}{Conclusion Probability} & 0.126 [0.110, 0.142] & 0.687 [0.597, 0.779] & \multicolumn{1}{r}{0.187 [0.095, 0.278]} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|rr|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Independent Comparisons: Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{r}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.000 & 0.000 & 0.000\\
3 & Process error & 0.000 & 0.787 [0.682, 0.891] & 0.687 [0.587, 0.779] \\
\end{tabularx}
\end{fmpage}


\subsection{Enumeration of all possible Brundage-Hamby set comparisons}\label{sec:brundage}
\svp{XXX Describe why this section exists as the lead-in to the results}
Brundage-Hamby sets consist of a set of 20 known bullets (two each from the ten consecutively manufactured barrels) and 15 questioned bullets.
 \citet{brundage} outlines the construction of sets of questioned bullets in detail as follows: ten bullets are chosen, one from each of the ten barrels. The remaining five bullets are picked at random from the barrels, such that at most three questioned bullets are from the same barrel. 

Using this strategy, a total of 1452 different sets can be constructed. These sets have questioned bullets of three main forms, listed in \autoref{tab:sets} from the perspective of the number of barrels with one, two, or three matching questioned bullets. 

\begin{table}
\caption{\label{tab:sets}Number of all possible sets of questioned bullets for Brundage-Hamby sets.}
\centering
\begin{tabular}{|r|ccc|l|} \hline
 &  one questioned & two questioned & three questioned  & number of \\
\# barrels & bullet matches & bullets match & bullets match & possibilities\\ \hline
I & 5 & 5 & 0 & 252 = $\binom{10}{5}\binom{5}{5}$\\
II & 6 & 3 & 1 & 840\\
III & 7 & 1 & 2 & 360\\ \hline
\end{tabular}
\end{table}

\autoref{fig:histogram} shows the histogram of the total number of different source comparisons before and identification is made.

<<histogram, fig.width=8, fig.height = 3, outwidth="\\textwidth", warning=FALSE, message=FALSE, fig.cap="Histogram of the total number of different source comparisons before an identification is made.">>=
library(tidyverse)
sets <- read.csv("data/allpossible-hambysets.csv")
sets$ds <- rowSums(sets-1)

sets %>% 
  ggplot(aes(x = ds)) + geom_histogram(binwidth=1, colour="grey50") +
  theme_bw() +
  theme(axis.title.x = element_blank()) + 
  xlab("Number of different source comparisons in a set before an identification is made") +
  ylab("Number of sets") +
  scale_x_continuous(breaks=c(49, 54.5, 61,  67.5, 74, 80.5, 86), labels=c("49", "54.5", "61", "67.5", "74", "80.5",  "86"))
@



\section{Extra stuff that doesn't have a home}


% <!--include marginal distributions of training data and compare to test data-->

<< echo=FALSE, eval=FALSE>>=
training <- read.csv("data/features-hamby.csv")
gtrain <- training %>% 
  select(profile1_id, profile2_id, study1, study2, match, ccf:sum_peaks) %>%
  gather(feature, value, ccf:sum_peaks)
# now compare to test data
@


from section 2.4

\hh{XXX We need to say more about the marginal distributions and how to calulate PPV, if we want to keep the next paragraph. But we could also think about moving this into the discussion. Essentially, this is a descriptive version of Bayes theorem without mentioning Bayes or the fact that given the marginal distribution we can move from one conditional distribution to the other. We should make this explicit or leave out. At the moment I am leaning towards out.}

This difference is fairly critical when generalizing from designed studies to casework: in a designed study, the proportion of same-source and different-source comparisons is determined by the experimental design and is fixed, which makes the FPR and FNR (and the combined accuracy rate) reasonable metrics for comparison. 
However, we have no idea what the prevalence of same-source comparisons and different-source comparisons might be in casework. 
Thus, it is not particularly sensible to evaluate the accuracy of examiner performance on case work using metrics developed in designed experiments with fixed ratios of same-source and different-source comparisons.

If we instead evaluate FTE accuracy using PPV and NPV, we avoid the need to know the prevalence of same-source and different-source comparisons; instead, we only need to know the examiner's conclusion from the evidence in the case. 
That is, in legal settings, it is the state of reality that is under debate; the examiner's opinion is testimony and is information we already have.

\hh{Given the new paragraphs at the beginning of 2.4 - do we still need the above discussion? XXX }

\hh{XXX Would it be fair to say that FPR and FNR allow us to assess examiners or the firearms matching process, while the predictive values allow us to evaluate the strength of the testimony? Maybe we do not want to go into detail on strength of evidence when the testimony is categorical. }



% \paragraph{Baldwin Study} \cite{Baldwin:2014bb}
% \begin{itemize}
% \item
% \hh{The fixed number of same source and different source sets in each kit is a weakness of the design in the Baldwin study. If a participant becomes aware of this fact, the additional information would help in reducing uncertainty about comparisons and might contribute to a reduced error rate. To mitigate this risk, participants were instructed ``to refrain from sharing or discussing the contents or results of their sample sets" (Baldwin et al), but in practice this is out of the hand of the tester. }
% \item It seems that firearms examiners are more willing to bet on identifications than on eliminations. 
% \hh{useful information, but not sure where to put it - probably for evaluating the error in the process}
% Only cartridge cases caught with the cartridge catching device (XXX name?) were used to assemble kits. No other pre-screening of usability for comparisons was done. 
% 
% For 3234 comparisons, FTEs evaluated how many of the known cartridge cases were usable for an evaluation: all three specimens were used in 3018 cases, two were used in 207 cases, and only one was used in nine cases.





% ``In this report we present our findings on these variations and suggest that this is an area for further study and discussion within the community."


\paragraph{Keisler Study}  \cite{keisler}
\begin{itemize}
% \item \hh{XXX there are two (minor) flaws: margins fixed and additional information. }
\item Again, inconclusive results are much more indicative of different source results than they should be  based on the setup of the study; we would expect same source results with a probability of 0.60 given an inconclusive result. 
% \item \svp{The design of the Keisler study is ideal - it allows for the calculation of all of the error and success rates, all conditional probabilities, and precise identification of the number of same-source and different-source comparisons conducted by each examiner. }
% \item \svp{One major flaw in the Keisler study (that is by no means unique to this study) is the acknowledgement that
% ``on the response sheet participants were asked if their laboratory had a policy which does not allow examiners to exclude based on individual characteristics. ... many participants answered `yes' to the laboratory policy question and answered `inconclusive' on all within-class comparisons that could have been correctly excluded." }
\end{itemize}

\paragraph{Duez Study} \cite{Duez:2017kha}

\begin{itemize}
\item The error rate of certified examiners in making eliminations is much lower than that of the ten trainees. This is reflected in the error rates of the overall process. The process errors are much smaller for certified firearms examiners than for trainees. \svp{XXX add to the conclusion?}
\item However, the probability to miss eliminations is again high, lab rules contribute to this error (see discussion below).
\item The set of conditional probabilities are shown at the bottom of \autoref{tab:duez-summary}. As in other studies, the probability of same source pairs given examiners reported an inconclusive result is much lower than the expect 75\% based on the setup of the study.
% \item \svp{The problems with ... ``The study was blind in that examiners did not know the true source of the cartridge cases."}
\end{itemize}

% Treatment of the Inconclusives:
% 
% The authors report that
% ``13\% of examiners are not permitted to eliminate on individual characteristics
% (therefore, their conclusions of inconclusive are perfectly acceptable)."
% 
% \hh{It is interesting to note that 7 of the examiners did not report eliminations due to  lab rules. These rules seem to be in contradiction to community accepted AFTE rules and need to be considered during reporting and testimony evaluations. XXX Bias against defense. XXX Needless to say that we do not agree with the authors' assessment that conclusions of inconclusives are perfectly acceptable.}





\paragraph{Brundage-Hamby Study } \cite{Hamby:2018hu}
% \begin{itemize}
% \item \hh{The study is a closed set study. Closed set study designs go hand-in-hand with an unfortunate emphasis of identifications over eliminations. }
% \item \hh{The design of the study does not allow for an assessment of all of the error and accuracy rates discussed in \autoref{sec:rates}, but shows a strong focus on evaluating identifications. }
% In particular, the answer sheets for Brundage-Hamby sets only asks for reports of identifications for each of the questioned bullets. 

% While the number of same source comparison is obviously one for each questioned bullet, i.e.\ each participant made exactly fifteen same source comparisons, the number of different source comparisons that were made  is an open question.

Technically, there are nine different source comparisons for each questioned bullet for a total of 135 (nominal) different source comparisons.  

However, these evaluations are not independent of each other: once a bullet is successfully identified to a pair of knowns, none of the other sets of knowns can possibly be a match. 
Rather than having to make a call on a comparison, the evaluation shifts to a confirmation of an elimination. We will assume that an examiner will need to make an average of 4.5 different source comparisons before making an identification, resulting in an average of 67.5 different source comparisons for the whole set, and an approximate total of 47047.5 different source comparisons overall. See \ref{sec:brundage} for mathematical details.
% \item \hh{Given that only identifications had to be reported, we are also} missing  any information on the number of eliminations versus inconclusive results for different source comparisons. 
% \end{itemize}





% \paragraph{Lyons Study} \cite{lyons}
% \begin{itemize}
% \item First, the researchers established that there were sufficient individualizing characteristics to identify cartridges ejected by the same tool, and that there were sufficient individualizing characteristics to exclude cartridges ejected by sequentially manufactured tools. 
% \item It should be noted that there are some methodological problems with the Lyons study - examiners who did not initially include all 12 unknown evaluations were given the chance to correct results; in addition, not every examiner evaluated the same number of unknowns. Excluding these results might provide different estimates, but we have chosen to use the numbers as reported in the paper for this calculation.
% \end{itemize}


% \paragraph{Bunch \& Murphy Study}  \cite{bunch2003comprehensive}

% Issues - assume the full number of possible pairwise comparisons (360) was completed; simulations show that in practice, if examiners used logic along with pairwise comparisons, the total number of required comparisons is between 186 and 289 (by simulation - should probably figure out if it's possible to get a hard floor without completely enumerating the sample space). This has pros (no possible information gained from examiners talking to each other), but also has problems - we can only provide bounded estimates for the number of comparisons done.



\end{document}