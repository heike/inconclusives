\documentclass{elsarticle}


\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\usepackage{amsmath,amsthm,amstext}
\usepackage{graphicx,psfrag,epsf,wrapfig,subfig,float,capt-of}
\usepackage[dvipsnames]{xcolor}
\usepackage{booktabs,longtable,multirow,multicol,tabu,tabularx,threeparttable,colortbl,array}
\usepackage{adjustbox}
\usepackage{pdflscape,fullpage}
\usepackage{pbox}
\usepackage[numbers]{natbib}
\usepackage{enumerate}

\usepackage{csquotes}% display/block quotes

\setlength{\parindent}{0cm} % remove paragraph indent
\usepackage{parskip}

\newenvironment{definition}[1]% environment name 
{% begin code 
  \par\vspace{.75\baselineskip}\noindent 
  \textbf{Definition (#1)}\begin{itshape}% 
  \par\vspace{.5\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Necessary commands for study-wise summary
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\renewcommand\tabularxcolumn[1]{m{#1}}% for vertical centering text in X column

\newsavebox{\fmbox}
\newenvironment{fmpage}[1]
{\begin{lrbox}{\fmbox}\begin{minipage}[t]{#1}}
{\end{minipage}\end{lrbox}\fbox{\usebox{\fmbox}}}

\journal{Forensic Science International}
%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
% \bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
% \bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num-names}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\noindent

\begin{frontmatter}

\title{Treatment of Inconclusives in the AFTE Range of Identifications\tnoteref{t1}}

\tnotetext[t1]{This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement \#70NANB15H176 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, University of California Irvine, and University of Virginia.}


%% Group authors per affiliation:
% \author{Elsevier\fnref{myfootnote}}
% \address{Radarweg 29, Amsterdam}
% \fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
% \author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
% \ead[url]{www.elsevier.com}
% 
% \author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
% \cortext[mycorrespondingauthor]{Corresponding author}
% \ead{support@elsevier.com}
% 
% \address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
% \address[mysecondaryaddress]{360 Park Avenue South, New York}

\author[isu,csafe]{Heike Hofmann}
\author[isu,csafe]{Alicia Carriquiry\corref{corauthor}}
\cortext[corauthor]{Corresponding author}\ead{alicia@iastate.edu}
\author[unl]{Susan Vanderplas}
\address[isu]{Statistics Department, Iowa State University\\2438 Osborne Dr, Ames, IA 50011}
\address[csafe]{Center for Statistical Applications in Forensic Evidence, Iowa State University\\613 Morrill Rd, Ames, IA 50011}
\address[unl]{Statistics Department, University of Nebraska Lincoln\\340 Hardin Hall North Wing, Lincoln, NE 68583-0963}   

\begin{abstract}
The treatment of inconclusives in the assessment of errors has been a long-standing issue with far-reaching implications in the legal system. 
\end{abstract}

\begin{keyword}
forensic science, black box studies, proficiency testing
\end{keyword}

\end{frontmatter}



\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\svp}[1]{{\textcolor{teal}{#1}}}
% boxes for illustrations

\noindent


<<echo = FALSE, message = FALSE, warning = FALSE>>=
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  dpi = 600,
  fig.align = "center",
  out.width = "\\textwidth",
  cache = TRUE,
  fig.path = "figures/",
  echo = FALSE
)
options(knitr.table.format = "latex")

library(stringr)

library(tidyverse)
library(scales)
library(multidplyr) # install_github("hadley/multidplyr")
library(bulletxtrctr) # install_github("csafe-isu/bulletxtrctr")
library(gridExtra)
library(kableExtra)
@


Examiners visually classify similarity of toolmark and firearm evidence according to the AFTE theory of identification \citep{identification} as one of identification, inconclusive or elimination. Exact guidelines for this classification vary from lab to lab; some labs will exclude only on the basis of non-matching class characteristics, such as direction of the twist in rifling, land length or number of lands, or type of rifling. In other labs, CMS (consecutively matching striae) as defined by \citeauthor{biasotti} \citep{biasotti} is used as a measure to quantify the similarity of two lands. In virtually all labs, individual characteristics used to identify matching bullets are derived from visual assessment; some class characteristics may be directly measured, but these are not sufficient for individualization. 
% Identification using 3D Scanning Technology


\begin{table}[hbt]
\noindent\fbox{%
\parbox{.98\textwidth}{%
\begin{enumerate}
\item Identification\hfill\newline
Agreement of a combination of individual characteristics and all discernible class characteristics where the extent of agreement exceeds that which can occur in the comparison of toolmarks made by different tools and is consistent with the agreement demonstrated by toolmarks known to have been produced by the same tool.


\item Inconclusive 
\begin{enumerate}
\item Some agreement of individual characteristics and all discernible class characteristics, but insufficient for an identification.
\item Agreement of all discernible class characteristics without agreement or disagreement of individual characteristics due to an absence, insufficiency, or lack of reproducibility.
\item Agreement of all discernible class characteristics and disagreement of individual characteristics, but insufficient for an elimination.
\end{enumerate}

\item Elimination \hfill\newline
Significant disagreement of discernible class characteristics and/or individual characteristics.

\item Unsuitable \hfill\newline
Unsuitable for examination.
\end{enumerate}
    }%
}
\caption{\label{tab:afte} AFTE Rules of Toolmark Identifications \citep{identification}.}
\end{table}
% \section{Background} % Identification

The identification process -- i.e. the assessment of whether two samples come from the same source (were made by the same tool, the same shoe, the same finger, shot through the same barrel) or from different sources -- is quite complex. 
In order to assess how well the identification process is functioning, studies with known ground truth have to be employed \svp{as} casework does not allow for an assessment of correctness. 
Ground truth, that is, the knowledge of whether two samples come from the same source or two different sources, is only available to the individual setting up the test; as casework is not so strictly controlled, we do not have information about ground truth and thus cannot use casework for error rate assessment.

\section{Study Design Considerations}

\svp{Studies must control for or systematically vary the type of implement making the pattern, manufacturing order (in many cases), maintenance procedures, ammunition or marking surface, and many other factors (see \citet{spiegelmanAnalysisExperimentsForensic2013} for a thorough review of all of the physical factors which might be controlled for or varied in a comprehensive study). In addition to the physical evidence-generation process, experimenters must carefully control the amount of information participants have about the study design and structure. When this information is not strictly controlled, examiners may use deductive reasoning and other strategies that are useful for the test but that do not correspond to casework.}
As a result, studies which attempt to gain insight into the identification process must also navigate these complexities, ideally without becoming too complicated to reasonably execute.

There are several considerations when designing a study of the identification process:
\begin{description}
\item [Closed vs. Open Set] A closed set study is one in which all questioned samples share the same source as one set of  knowns in the test set. Conversely, an open set study is one in which questioned samples may originate from sources which are not part of the known samples provided in the test set.
\item [White box vs. Black Box] A white-box study is one which makes an attempt to understand \emph{why} an examiner came to the decision, in contrast to a black box study, which only evaluates the correctness of the decision (and not the reasoning behind it). 
\item [Blind testing] A blind (or blinded) study is one in which the participant (in this case the examiner) does not know that they are being tested; that is, a study which appears to be part of a case, rather than research. 
\citet{pcast2016} recommended that the use of blind testing be increased, as error rates derived from blind studies better generalize to casework. 
\svp{\citet{spiegelmanAnalysisExperimentsForensic2013} also suggest blind testing as a method to increase the external validity of studies beyond assessing the proficiency of individual examiners.} 
\svp{Note that this is different from the definition of a blind study used in \citet{bunch2003comprehensive}; in that paper, the use of the terms blind and double blind do not conform to the generally accepted definitions.}
\item [Number of knowns from different sources] Some studies \citep{keisler} provide only one known exemplar (or a set of multiple exemplars from the same source). Other studies, such as \citep{brundage, hamby,Hamby:2018hu} and the Houston FSC and Phoenix studies from \citet{case-validation}, include multiple different sources as knowns. %Typically, these studies can be completed with the assumption that once an unknown matches one known, it will not match any other knowns. \svp{Add in the issues with this later}
\item [Study length] Most crime labs are understaffed and maintain a fairly large backlog of cases; as a consequence, examiner time is limited. While examiners should participate in these studies, because they benefit the discipline as a whole, they must balance the competing demands of a large and consequential workload. As such, studies which require examiners to make a large number of comparisons may be less likely to have sufficient participants to generalize to the wider discipline.
\end{description}

%% Transition to error rates
Once a study is \svp{designed} and test samples have been assessed by forensic examiners, error rates can be calculated. Differences between reported results and ground truth are considered to be errors in the identification process. %A detailed discussion on the the exact procedure on how to deal with inconclusive results in calculating error rates is provided in section XXX.



\section{Error Rate Calculation}

The need for scientific validation and experimentally-determined error rates has been identified in several reports evaluating the discipline of forensic science in the United States as critical for addressing problems within the field and improving the overall performance of the justice system.
National Research Council 2009:

\begin{displayquote} much forensic evidence—including, for example, bitemarks and firearm and toolmark identifications—is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline.\end{displayquote}

PCAST (2015) identified two important gaps:

\begin{displayquote} (1) the need for clarity on the scientific meaning of "reliable principles and methods" and "scientific validity" in the context of certain forensic disciplines, and (2) the need to evaluate specific forensic methods to determine whether they have been scientifically established to be valid and reliable.\end{displayquote}

Indeed, when jurors are left to form their own conclusions about the error rates of forensic pattern disciplines, they often come up with estimates which are far lower than empirical studies, suggesting that jurors consider forensic evidence (in the absence of error rates determined from scientific studies) as of more determinative value than is warranted by the evidence. The PCAST Report (2016) summarizes the effect:

\begin{displayquote} In an online experiment, researchers asked mock jurors to estimate the frequency that a qualified, experienced forensic scientist would mistakenly conclude that two samples of specified types came from the same person when they actually came from two different people. The mock jurors believed such errors are likely to occur about 1 in 5.5 million for fingerprint analysis comparison; 1 in 1 million for bitemark comparison; 1 in 1 million for hair comparison; and 1 in 100 thousand for handwriting comparison.\end{displayquote}

Unfortunately, the way in which error rates are computed and reported in the (relatively sparse) literature involving estimating these error rates experimentally is highly variable. 
\citet{Dror:2018fp} \svp{suggested treating} inconclusive results as an examiner's conscious decision to not decide on an identification or elimination. 
This view has drawn some criticism \citep{Biedermann:2018hr,biedermannAreInconclusiveDecisions2019}. 
Inconclusives can be excluded from result calculations, included as correct results, or included as incorrect results; each of these decisions has an impact on the interpretation of the resulting error rates.
In this paper, we \svp{discuss} the treatment of inconclusives in the assessment of error rates from a statistical point of view, % We believe that this -- rather than further dividing the different opinions -- provides a unifying framework.
examining previous error rate studies to assess the technical accuracy and practical utility of the reported error rates. 
Before approaching the actual studies, it may be useful to \svp{explore} the logic behind the \svp{numerical} calculations and define some basic notation to facilitate comparison of studies with different experimental designs.

\subsection{Evaluating Classification Decisions}\label{sec:evaluating-classification-decisions}
When an examiner assesses a set of evidence (generally, a pair of items, one of known provenance and one of unknown provenance) and decides whether the evidence is an identification, inconclusive, or an elimination, they provide a \emph{classification} of the evidence in a set of given categories.  
We \hh{XXX who is "we"? I don't think it's a good idea to have a them and we kind of description. } assess the accuracy of a classification method using \emph{success rates} and \emph{error rates}, that is, with numerical quantities indicating how often the process produces correct and incorrect results.



\subsection{Source-specific assessment}

\hh{When assessing the accuracy of a classification \emph{success rates} and \emph{error rates} are of interest, i.e.\ the interest shifts from the classification to the numerical quantity indicating how often the process produces correct and incorrect results.}

The most common way to assess the accuracy of a classification method is \hh{to calculate its \emph{sensitivity} and \emph{specificity}. Sensitivity, also called the true positive rate, is an examiner's ability to make an identification from same source evidence (SS). This can be expressed as the conditional probability $P (\text{identification}) \mid SS)$. Conversely, specificity, or the true negative rate, is an examiner's ability to make an elimination given evidence from different sources (DS), $P (\text{elimination}) \mid DS)$.}
Another, often used, way of assessing accuracy of a classification method is to calculate the false positive rate and the false negative rate. The \emph{false positive rate} is the probability that an examiner makes an identification from evidence which is from different sources(DS), $P(Identification | DS)$.
Mathematically, we calculate the false positive rate as $$ FPR = P(Identification | DS) = \frac{P(Identification \text{ and } DS)}{P(DS)}.$$

The \emph{false negative rate} is the probability that an examiner makes an elimination from evidence which originates from the same source (SS), $P(Elimination | SS)$. In all  of these assessments, the  rates are calculated by conditioning on the origin of the evidence. \hh{XXX The figure includes inconclusives - which we haven't talked about at this point. } \autoref{fig:FPR-specific-tree} shows a decision tree describing the possible comparisons which can be made conditioned on ground truth. \hh{XXX sorry for taking out the next paragraph - I'm not sure it helps in clarifying the math behind the tree. }
%The terminal nodes \hh{XXX not defined} describe the joint probability of examiner classification and ground truth; the ground truth nodes describe \hh{XXX not sure how the nodes describe probabilities} the probability of same-source or different-source comparisons, which is determined by the experimental design. \hh{XXX No hypotheticals. Either we do or we don't.} If we wanted to calculate the false positive rate, we would take the far-right terminal node, and compare the probability of that node to the probability of the different source node which is its parent. 

\begin{figure}
\centering
%\includegraphics[width=\textwidth]{figures/Source_Specific_Tree_Diagram.pdf}
\includegraphics[width=\textwidth]{figures/Source-specific-tree-diagram.pdf}
\caption{Tree Diagram illustrating the logic of source-specific error rates. We must first decide whether we are interested in same-source or different-source rates, and then examine the probabilities for each of the possible labels: Identification, Inconclusive, and Elimination.}\label{fig:FPR-specific-tree}
\end{figure}

\subsection{Decision-specific assessment}

The use of false positive and false negative rates to characterize a classification task is extremely common, however, there are alternate ways to evaluate a classification method. 
% In medicine, in particular, it is common to evaluate tests based on the positive and negative predictive value. 
The \emph{positive predictive value} (PPV) is the probability that, if an examiner makes an identification, the evidence is from the same source, that is, $P(SS | Identification)$. 
\svp{Its complement, the \emph{false discovery rate} (FDR), is the probability that if an examiner makes an identification, the evidence is from different sources.}
The \emph{negative predictive value}(NPV) is the probability that, if an examiner makes an exclusion, the evidence is from different sources, or $P(DS | Exclusion)$. \svp{Its complement, the \emph{false omission rate} (FOR) is the probability that, if an examiner makes an exclusion, the evidence is actually from the same source.}
\svp{In \citet{songEstimatingErrorRates2018}, the FDR is referred to as the false identification error rate, and the FOR is called the false exclusion error rate.}
As before, we can map out the conditional hierarchy using a decision tree, shown in \autoref{fig:PPV-specific-tree}. 

\hh{XXX I think we have too many rates in there with too close naming. Personally, I would not distinguish between False discovery and False positive rates. But they are defined differently. Help XXX - I have added names to the figures. This might help a bit with the problem. But I think it still is a problem.}


Predictive value assessments are extremely useful in understanding the implications of a particular classification label; they inform us about the likely state of the physical evidence. 
% \svp{In designed studies, this is known information; however, the PPV, NPV, FDR, and FOR are also useful for casework, where the true state of the evidence is unknown, as they are not conditioned on unknown information.} 



\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/Conclusion-specific-tree-diagram.pdf}
%\includegraphics[width=\textwidth]{figures/Conclusion_Specific_Tree_Diagram.pdf}
\caption{
Tree Diagram illustrating the logic of positive and negative predictive value calculations. 
We must first select which conclusion we are interested in, and then examine the probability of each state of reality (same source or different sources) given that conclusion.
}\label{fig:PPV-specific-tree}
\end{figure}

\subsection{Practical impact of error rates}


\hh{Mathematically,} the primary difference between the decision tree diagrams in Figure \ref{fig:FPR-specific-tree} and \ref{fig:PPV-specific-tree} is the conditioned variable. 

\hh{From a practical point of view, the question of whether source-specific or decision-specific rates are of interest, is where in the overall (legal) decision process we are: 
\begin{itemize}
\item as a firearms and toolmark examiner or a lab director, we are interested in assessing accuracy based on ground truth, i.e. we are interested in source-specific rates to answer questions of the form: given this evidence, are  trained firearms examiner  more likely than trainees to come to the correct conclusion? (Case studies later show that yes, that is the case.)
\item as part of courtroom proceedings, we -- the jury, lawyers, and judges -- are interested in assessing accuracy of testimonies, i.e. we are interested in conclusion-specific rates to answer questions of the form: given an expert testified to the evidence as being an identification, what is the probability that the evidence actually does come from the same source?
\end{itemize}
These two forms of questions look deceptively similar, but lead to different mathematical equations. 
This difficulty is not exclusive to evaluations of firearms evidence -- it is a known and pervasive pitfall in other application areas, such as medical diagnostics \cite{Casscells:1978ba, Manrai:2014fp}. }
\hh{The distinction between these two sources of error rates }
\svp{ is perhaps a bit more approachable in the setting of a medical context: a given test will have a sensitivity (true positive rate, TPR) and a specificity (true negative rate, TNR) that combined would yield the accuracy rate. 
However, if you as a patient are sent for a test which yields a positive result, what you care about is 
%not the accuracy rate (because when testing for rare diseases the true negatives are much more common than true positives); it 
is the probability that, given your positive result, you actually have the disease. 
}  \hh{are sent for? that looks weird to my German eyes}

\hh{Mathematically, the source-specific error rates, }
%When calculating the
FPR and FNR, \hh{are calculated by conditioning on the source of of the evidence (same-source/different source) and give the probability for an examiner's conclusion.} 
%we condition on the source of the evidence and predict the examiner's classification, while 
\hh{PPV and NPV, are calculated by conditioning on an examiner's conclusion and give the probability of }  the state of reality. \hh{XXX I'm not quite sure what the state of reality means, but I can't think of how to re-phrase just now.} 

%The implications of the two sets of conditional probabilities are somewhat different as well. 

\hh{XXX next few paragraphs earlier?}
The medical context is perhaps a more approachable setting: a given test will have a sensitivity (true positive rate, TPR) and a specificity (true negative rate, TNR) that combined would yield the accuracy rate. 
However, if you as a patient are sent for a test which yields a positive result, what you care about is not the accuracy rate (because when testing for rare diseases the true negatives are much more common than true positives); it is the probability that, given your positive result, you actually have the disease. 
In tests for rare diseases which are highly accurate, the positive predictive value can be fairly small, because false positives are more common than the incidence rate of the disease.
\hh{XXX moved the previous paragraphs up, but left out the rare probabilities.}

\hh{XXX We need to say more about the marginal distributions and how to calulate PPV, if we want to keep the next paragraph. But we could also think about moving this into the discussion. Essentially, this is a descriptive version of Bayes theorem without mentioning Bayes or the fact that given the marginal distribution we can move from one conditional distribution to the other. We should make this explicit or leave out. At the moment I am leaning towards out.}

This difference is fairly critical when generalizing from designed studies to casework: in a designed study, the proportion of same-source and different-source comparisons is determined by the experimental design and is fixed, which makes the FPR and FNR (and the combined accuracy rate) reasonable metrics for comparison. 
However, we have no idea what the prevalence of same-source comparisons and different-source comparisons might be in casework. 
Thus, it is not particularly sensible to evaluate the accuracy of examiner performance on case work using metrics developed in designed experiments with fixed ratios of same-source and different-source comparisons.

If we instead evaluate FTE accuracy using PPV and NPV, we avoid the need to know the prevalence of same-source and different-source comparisons; instead, we only need to know the examiner's conclusion from the evidence in the case. 
That is, in legal settings, it is the state of reality that is under debate; the examiner's opinion is testimony and is information we already have.

\hh{Given the new paragraphs at the beginning of 2.4 - do we still need the above discussion? XXX }

\hh{XXX Would it be fair to say that FPR and FNR allow us to assess examiners or the firearms matching process, while the predictive values allow us to evaluate the strength of the testimony? Maybe we do not want to go into detail on strength of evidence when the testimony is categorical. }


\hh{XXX A sentence on why we need case studies to evaluate and assess all of these error rates. }

\subsection{Assessment of error rates in case studies}

In order to calculate these different probabilities and rates, let us assume that we have a (hypothetical)  \hh{XXX why hypothetical? XXX} experiment, where participants are asked to complete a total of $N$ comparisons, consisting of $S$ same-source comparisons and $D$ different source comparisons.

\hh{Why do you write this so hypothetically? I thought we wanted to make this as real as possible.}
\hh{A summary of these evaluations can then be reported}
%The study data would consist of evaluations reported 
in the form of a %table \svp{which can be arranged in a form like the} 
generic table as shown in \autoref{tab:generic-results}.

\begin{table}
\centering
\caption{
An example results table for a generic experiment where comparisons can be broken down as either from the same source or from different sources, and examiners classify each comparison as an identification, an inconclusive, or an elimination, as  specifiedin the AFTE rules of identification. 
Define $S = a + b + c$ as the total number of same source comparisons, $D = d + e + f$ as the total number of different-source comparisons, and $N = S + D$ as the total number of comparisons. 
Then the probability of the occurrence of any cell in the interior of the table (a, b, c, d, e, or f) can be found by dividing the respective letter by $N$, the total number of comparisons.
}\label{tab:generic-results}
\begin{tabularx}{\textwidth}{Y|ccc|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\\hline
\multicolumn{1}{c}{} & Identification & Inconclusive & \multicolumn{1}{c}{Elimination} & Source Total\\\hline
Same Source & $a$ & $b$ & $c$ & $S = a + b + c$\\
Diff Source & $d$ & $e$ & $f$ & $D = e + f + g$\\\hline
\multicolumn{1}{Y}{Conclusion Total} & $a + d$ & $b + e$ & \multicolumn{1}{c}{$c + f$} & \multicolumn{1}{Y}{$N = S + D$}\\\hline\hline
\end{tabularx}
\end{table}

The \emph{joint probability} of any source condition and any examiner conclusion can be found by taking the corresponding cell in \autoref{tab:generic-results} and dividing by $N$, the total number of comparisons evaluated in the study. 
Thus, the joint probability of a same-source comparison and examiner identification would be $a/N$, while the joint probability of a different-source comparison and examiner identification would be $d/N$. 
Then, using the quantities in \autoref{tab:generic-results}, we can calculate the conclusion-specific probabilities as shown in \autoref{tab:fpr-fnr}.

\begin{table}\centering
\caption{
Conclusion-specific probabilities, calculated using the quantities introduced in \autoref{tab:generic-results}. 
In each cell in the main body of the table, probabilities shown are the probability of source $X$, if the examiner's conclusion (one of Identification, Inconclusive, or Elimination) is $Y$. 
In the last row in the table, the marginal probabilities $P(Y)$ are shown, that is, the unconditional probability that the examiner's decision is $Y$. 
}\label{tab:fpr-fnr}
\begin{tabularx}{\textwidth}{Y|ccc|Y}
\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\\hline
\multicolumn{1}{c}{} & Identification & Inconclusive & \multicolumn{1}{c}{Elimination} & Source Probability\\\hline
Same source & $a/(a+d)$ & $b/(b+e)$ & $c/(c+f)$ & $S/N$\\
Diff source & $d/(a+d)$ & $e/(b+e)$ & $f/(c+f)$ & $D/N$\\\hline
\multicolumn{1}{Y}{Conclusion probability} & $(a + d)/N$ & $(b + e)/N$ & \multicolumn{1}{c}{$(c + f)/N$} & \multicolumn{1}{Y}{1.0}\\\hline\hline
\end{tabularx}
\end{table}

Using the same data from \autoref{tab:generic-results}, we could also calculate predictive value probabilities as shown in \autoref{tab:pv-prob}.

\begin{table}\centering
\caption{
Predictive value probabilities, calculated using the quantities introduced in \autoref{tab:generic-results}. 
In each cell in the main body of the table, probabilities shown are the probability of conclusion $Y$ (one of Identification, Inconclusive, or Elimination) if the source is $X$ (same, different). 
In the last row in the table, the marginal probabilities $P(Y)$ are shown, that is, the unconditional probability that the examiner's decision is $Y$. 
In the last column of the table, the marginal probabilities $P(X)$ are shown; these are determined by the experimental design.
}\label{tab:pv-prob}
\begin{tabularx}{\textwidth}{Y|ccc|Y}
\multicolumn{5}{c}{\bf Predictive Value Probabilities}\\
\multicolumn{1}{c}{} & Identification & Inconclusive & \multicolumn{1}{c}{Elimination} & Source Prop.\\\hline
Same source & $a/S$ & $b/S$ & $c/S$ & $S/N$\\
Diff source & $d/D$ & $e/D$ & $f/D$ & $D/N$\\\hline
\multicolumn{1}{Y}{Concl. prop.} & $(a + d)/N$ & $(b + e)/N$ & \multicolumn{1}{c}{$(c + f)/N$} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}
\end{table}
A google sheets workbook for performing these calculations is available at \url{http://bit.ly/FTE-error-rate}.

Now that we have defined several different metrics for evaluating the errors made in a study, we will consider the role of inconclusives in pattern evidence examination. 

\subsection{What makes an error?}
The sets of conditional probabilities in \autoref{tab:fpr-fnr} and \autoref{tab:pv-prob} are important in the assessment of the process of evidence examination. 
However, standard evaluations of classification algorithms (or human decision making), such as false positive and false negative rates, or sensitivity and specificity \svp{XXX cite...?} generally involve classification states which mirror reality. 
More precisely, in most classification problems, the number of real classes is the same as the number of possible decisions. 
\hh{The AFTE Rules of Identification introduce inconclusives as a  third decision into the process. Inconclusives do not correspond to a real state, we therefore} 
have to decide how to handle them \hh{when assessing error rates}. 
This difference between forensic evaluation and classification problems which are more commonly evaluated provides an unfortunate opportunity for confusion when using the same methods for assessing forensic decision making. 

\autoref{tab:overall-err-rate} shows several different  error rate calculations, using the general structure of \autoref{tab:generic-results}. 
Each of these calculations has a  different meaning and interpretation; as a result, it is important to consider which error rate best suits the purpose of a  study \hh{and its interpretation in the larger framework of pattern evidence}. 
In \autoref{tab:overall-err-rate}, option 0 corresponds to a definition of the overall error rate of a binary, symmetric decision process consisting of two actual states and two possible decisions. 
For  simplicity we will assume that the two actual states are SS (same source) and DS (different source) and the two decisions are Identification and Elimination. 
In this case, the overall error rate is the sum of the false positives and the false negatives, divided by the number of overall comparisons. 




\newcommand{\redsq}[2][1.15em]{\protect\fcolorbox{black}{red!50}{\makebox[#1][c]{#2\vphantom{dp}}}}
\newcommand{\graysq}[2][1.15em]{\protect\fcolorbox{black}{black!30}{\makebox[#1][c]{#2\vphantom{dp}}}}
\newcommand{\whitesq}[2][1.15em]{\protect\fcolorbox{black}{white!100}{\makebox[#1][c]{#2\vphantom{dp}}}}
% \newcommand{\blacksq}{\fcolorbox{black}{black!100}{\phantom{x}}}
\newcommand{\xsq}{\protect\fcolorbox{black}{white!100}{\makebox[1.25em][c]{x\vphantom{dp}}}}

\begin{fmpage}{\textwidth}\protect\captionof{table}{Different ways to calculate the overall error rate.}
Reading this table: 
An arrangement of two rows of three boxes then resembles the interior cells of \autoref{tab:generic-results}, where \pbox{10em}{\mbox{\whitesq{a}\whitesq{b}\whitesq{c}}\\\mbox{\whitesq{d}\whitesq{e}\whitesq{f}}} are the six conditional quantities under discussion. 
Let \pbox{1.65em}{\whitesq{}} ~correspond to a correct decision, indicating that the box is to be included in the denominator of the calculation, \pbox{1.65em}{\graysq{}} indicate that the box is to be conditionally excluded from the calculation, and \pbox{1.65em}{\redsq{}} correspond to an incorrect decision, indicating that the box is to be included in the numerator and denominator of the calculation. 

\begin{tabularx}{\linewidth}{llcl}
\# & Type & Illustration & $P(Error) = $\\
0 & No Inconcl. & \pbox{5em}{\whitesq{a}\redsq{c}\\\redsq{d}\whitesq{f}} & $P(DS\text{ and }Ident.) + P(SS\text{ and }Excl.) = \displaystyle\frac{c + d}{a + c + d + f}$\\\\
1 & \pbox[l]{8em}{Condition on\\Not Inconcl} & \pbox{5em}{\whitesq{a}\graysq{b}\redsq{c}\\\redsq{d}\graysq{e}\whitesq{f}} & $\displaystyle\frac{P(DS\text{ and }Ident.) + P(SS \text{ and } Excl.)}{P(Ident.) + P(Excl.)} = \displaystyle\frac{c + d}{a + d + c + f}$\\\\
2 & \pbox[l]{8em}{Inconclusives\\as Correct} & \pbox{5em}{\whitesq{a}\whitesq{b}\redsq{c}\\\redsq{d}\whitesq{e}\whitesq{f}} & $P(DS\text{ and }Ident.) + P(SS\text{ and }Excl.) = \displaystyle\frac{c+d}{N}$\\\\
3 & \pbox[l]{8em}{Inconclusives\\as Incorrect} & \pbox{5em}{\whitesq{a}\redsq{b}\redsq{c}\\\redsq{d}\redsq{e}\whitesq{f}} & $P(DS\text{ and }Ident.) + P(SS\text{ and }Excl.) + P(Inconcl.) = \displaystyle\frac{b + c + d + e}{N}$\\
\end{tabularx}\label{tab:overall-err-rate}
\end{fmpage}



One reasonable solution to the inconclusive problem would be to calculate the probability of an error given that the classification was not inconclusive, as in option 1 of \autoref{tab:overall-err-rate}. 
Note that if we condition on the decision being either identification or elimination, the probability calculations for scenarios 0 and 1 in \autoref{tab:overall-err-rate} are identical; the only difference between the two is the setup of the scenario: 
In scenario 1, we allow for inconclusives as a decision category but restrict them from the calculation of the error rate entirely; in scenario 0, inconclusives are not a valid conclusion. 
This solution does entirely ignore inconclusives, though, and as they are part of the process and can be included in examiner testimony (usually phrased as ``could not be excluded"), this elimination from consideration is not appropriate if the goal of a study is to assess the error rate of the entire evaluation process. 
\svp{\citet{koehlerProficiencyTestsEstimate2013} recommends using the option 1 approach for calculating error rates while  also tracking rates of inconclusive identifications ``for other purposes", perhaps including reporting during testimony.}\svp{The last clause needs work here XXX}

Under AFTE's Theory of Identification, however, inconclusive results are acceptable outcomes and not considered errors; to give a very extreme example, an examiner could report inconclusives for the rest of their career and never make an error. Option 2 shows this way of treating inconclusives as correct regardless of the actual state.
What this means in real terms, is that inconclusives are effectively counted as identifications if they are actually from the same source, but counted as eliminations if they are actually from a different source. 
When the goal is to assess the error rate of the examiner under the prevailing guidelines of the AFTE Theory of Identification, this is a reasonable option for assessing error rates of firearm and toolmark examiners, with the slightly odd side-effect that an examiner who marks every comparison as inconclusive does not provide any useful actionable information, but effectively has a zero error rate. \svp{XXXthis is repeating something said above, though I'm not sure that's a bad thing XXX}

When viewed within the framework of the legal system, error rates calculated using option 2  are  not considering any error introduced by the evidence itself. 
If e.g. a firearm does not mark well, a firearms examiner might not be able to make an identification or an elimination through no fault of theirs. 
However, from the wider perspective of the \emph{system}, it is  important to look at the overall error rate of the entire process.
This requires that we consider the errors from the perspective of the real-world state: two pieces of evidence are  either from the same source or originate from different sources; an inability to distinguish between those options is an error in the \emph{process} even it if is not an error made on the part of the examiner. 

Option 3 of \autoref{tab:overall-err-rate} reflects this global view by counting  inconclusives as errors. 
Practically, option 3 reflects the error of the examination process rather than the examiner as an individual. 
To illustrate why this is the case, consider that an examiner may, upon inspection of a fired bullet, find that there are insufficient individualizing characteristics to make an identification. 
This does not reflective negatively on the examiner's skill; it is merely a consequence of the contact made between the bullet and the barrel during the firing process. 
Nonetheless, even though a pair of evidence originated from the same physical source, it is not possible to evaluate the evidence and make an identification. The inconclusives are representative of the errors involved in the process of evidence recording, rather than the evaluation. 

In the next section, we will examine the results from a number of studies of the error rate of firearms examinations, identifying the results which are reported, converting these results (where possible) into the format used in \autoref{tab:generic-results}, and calculating the conditional probabilities in the format of \autoref{tab:fpr-fnr} and \autoref{tab:pv-prob}. 
Prior to that, however, it may be useful to develop one more idea: in a perfect world with examiners who are all-knowing, what should the results of these studies look like?\svp{XXX This could be a bit better transition XXX}


\subsection{Ideal study probabilities}

\hh{In the absence of knowing the truth we have to resort to working within a probabilistic framework to assess the likelihood of certain events. 
Some (XXX? quantify?) evidence is based on questions of pattern matching: does the shoe print from the crime scene match a particular shoe? was the bullet found at a crime scene fired from a particular weapon? does the breech face impression on a cartridge case from the crime scene match the breech face of a particular weapon frame?}
\svp{Other evidence, such as DNA, is based on formal measurement, but produces a certain signature or pattern which can also be evaluated using probabilistic methods.}

In these situations the fundamental quantity in question is the probability that two pieces of evidence come from the same source or from different sources. 
In a legal setting the opinion provided by the expert guides this assessment. 
So given expert testimony, how does or should the evaluation of the above probability change?  

The AFTE theory of firearms and toolmarks identification as shown in \autoref{tab:afte} allows an expert to come to three main conclusions: make an identification, i.e. two pieces of evidence are thought to come from the same source, make an elimination, i.e. two pieces of evidence are tought to come from different sources, or an inconclusive, i.e. the evidence does not allow either an identification or an elimination. 

Given these three outcomes, we are interested for each the probability of same source and different sources, resulting in a total of six ($3 \times 2$) conditional probabilities. 
In a perfect world, we would expect the probabilities shown in \autoref{tab:ideal-probabilities}:

\begin{table}
\caption{Expected conditional probabilities in a perfect world.}\label{tab:ideal-probabilities}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & $P(SS|Identification) = 1 $& $P(SS|Inconclusive) = P(SS)$ & $P(SS | Elimination) = 0$ \\
Different source & $P(DS|Identification) = 0$ & $P(DS|Inconclusive) = P(DS)$ & $P(DS | Elimination)= 1$ \\\hline
\end{tabu}
}
\end{table}

Most notably, we would expect inconclusive results to not show any dependence on the type of comparison unless there is some evidence provided by experts that one type of conclusion is more difficult to make than another.

In actual case work we are not able to assess the probabilities for same source or different source.  
However, those probabilities are easily accessible in formal studies or blinded proficiency testing,\svp{ as $P(SS)$ and $P(DS)$ in these studies are determined by the experimental design.}


\svp{XXX I think this should go into a different section talking about convention, personally, even possibly citing \citet{spiegelmanAnalysisExperimentsForensic2013} about standard operating procedures.}
\hh{reporting of eliminations differs from lab to lab. Some labs do only allow firearm examiners to make eliminations based on differences in class characteristics. XXX quantify: baldwin, }

%The probability to make an identification  could be either reported directly or calculated as a composite of true positive and false positive rates:

% \[
% P(\text{identification made}) = TPR \times P(\text{same source}) + FPR \times P(\text{different source}) 
% \]

\svp{By examining these studies}, we can use existing data to evaluate the conditional probability of same source versus different sources given an expert's testimony directly.



\section{Studies}

Baldwin , Keisler , Lyons \cite{lyons}, Hare, Fadul \cite{fadul}, Brundage-Hamby, Duez , Chumbley - can't find any examiner evaluations for Chumbley

\autoref{fig:cis} shows an overview  of the conditional probability of same source evidence given an examiner's conclusion for five published studies. 
Probabilities are shown with their respective 95\% (Pearson-Clopper) confidence intervals.  
In light grey, expected probabilities are shown based on the distribution of same source/different source comparisons and the number of participants in each study.

<<cis, echo=FALSE, fig.width=8, fig.height=4.5, fig.cap="Pearson-Clopper 95\\% confidence intervals for the probability of same source given examiner's conclusion.", out.width=".8\\textwidth">>=
clopper <- function(alpha, success, trials) {
  lower <- qbeta(alpha/2, success, trials-success+1)
  upper <- qbeta(1-alpha/2, success+1, trials-success)
  c(lower, upper)
}

library(tidyverse)
studies <- read.csv("data/studies.csv")
cis <- studies %>% group_by(Study, Decision) %>% 
  filter(Type == "Observed") %>%
  summarize(
    pred_ss = sum(Number[Ground.truth=="Same Source"])/sum(Number),
    pred_ss_lower = clopper(0.05, sum(Number[Ground.truth=="Same Source"]), sum(Number))[1],
    pred_ss_upper = clopper(0.05, sum(Number[Ground.truth=="Same Source"]), sum(Number))[2]
  )

exp <- studies %>% 
  filter(Decision == "Inconclusive") %>%
  group_by(Study, Decision) %>% 
  summarize(
    exp_ss = Number[Type=="Expected"][1] * sum(Number[Type == "Observed"]),
    exp_ss_lower = clopper(0.05, exp_ss, sum(Number[Type == "Observed"]))[1],
    exp_ss_upper = clopper(0.05, exp_ss, sum(Number[Type == "Observed"]))[2],
    exp_prob = exp_ss/sum(Number[Type == "Observed"])
  )
  

cis %>% 
  ungroup() %>%
  mutate(
    Decision = factor(Decision, levels=c("Identification", "Inconclusive", "Elimination")),
    Study = factor(Study, levels=c("Baldwin", "Keisler", "Duez", "Hamby", "Lyons"))
  ) %>%
  ggplot(aes(x = pred_ss, y = Study)) +
  facet_grid(Decision~.) +
  geom_vline(xintercept=c(0,1), colour = "grey20", size=0.25) +
  geom_errorbarh(
    aes(xmin = pred_ss_lower, xmax=pred_ss_upper, y=Study), 
    size=0.5, height = 0.5, colour = "grey20") +
  geom_point() +
  xlim(c(0,1)) +
  xlab("Probability for same source given examiner's decision") +
  theme_bw() +
  geom_point(aes(x = exp_prob, y = Study), data = exp, colour="grey80") +
  geom_errorbarh(
    aes(xmin = exp_ss_lower, xmax=exp_ss_upper, y=Study, x = exp_ss_lower),
    size=0.5, height = 0.5, colour = "grey80", data = exp)
@


\subsection{Baldwin Study \cite{Baldwin:2014bb}}

Each kit consists of 15 sets of 3 known cartridge cases and 1 questioned. 
For all participants 5 of the sets were from same-source and 10 of the sets were from different sources. 
{\bf ``We instructed all participants to refrain from sharing or discussing the contents or results of their sample sets and answers to minimize the risk of revealing the design".} 
\hh{this is a weakness of the design,  but it also fixes the frequencies of same source and different source occurrences and with it the probabilities for the events:}

A total of 25 firearms was used for the study, such that within each kit no firearm was re-used for either knowns or questioned cartridge cases, i.e. no additional information could be gained by comparing any cartridge cases across sets.

218 participants (self-reporting as active forensic firearm examiners) provided evaluations of the kits, evaluation results are shown in \autoref{tab:baldwin-totals}. 
Each of these evaluations is  based on a comparison of one questioned cartridge case to three known same source  cartridge cases. \svp{This wording is slightly confusing - I understand it now, but had to puzzle over whether there was a single known source (with 3 replicates) or 3 knowns. }


\begin{table}[hbt]
\centering
\caption{Reported number of evaluations  from Baldwin Study.}\label{tab:baldwin-totals}
{\tabulinesep=1.2mm
\begin{tabu}{|c|rrr|r|}\hline
 & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
 & Identification & Inconclusive  & Elimination &  \\
$X = $Reality&&or no response&&Totals\\\hline
Same source & 1075  & 11 & 4 & 1090\\
Different source & 22 & 735+2 & 1421 & 2180\\\hline
Percent & 33.5\% & 22.9\% & 43.6\% & 100.0\\\hline
\end{tabu}
}
\end{table}




\begin{table}[hbt]
\centering
\caption{Error rate assessment for Baldwin Study.}\label{tab:baldwin-errors}
{\tabulinesep=1.2mm
\begin{tabu}{|l|rr|}\hline
 & Missed Identification & Missed Elimination  \\ \hline
Option 2 (FTE error) & 0.0037  & 0.0101\\
Option 3 (Process error) & 0.0138 & 0.3482\\ \hline
\end{tabu}
}
\end{table}


The resulting set of conditional probabilities is shown in \autoref{tab:baldwin}

\begin{table}[hbt]
\centering
\caption{Predictive probabilities from Baldwin Study.}\label{tab:baldwin}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & 0.9799 & 0.0147 & 0.0028 \\
Different source & 0.0201 & 0.9853 & 0.9972 \\\hline
\end{tabu}
}
\end{table}



It seems that firearms examiners are more willing to bet on identifications than on eliminations. 
Based on the set up of the study, we would expect the probabilities for same and different source given an inconclusive result is reported to be: 
\begin{eqnarray*}
P(\text{same source} \mid \text{Inconclusive}) &=& P(\text{same source}) = \frac{1}{3},  \\
P(\text{different source} \mid \text{Inconclusive}) &=& P(\text{different source}) = \frac{2}{3}
\end{eqnarray*}

In fact, even if inconclusives and eliminations are combined, the error rate of making a false elimination is 0.0069, only a third of the error of a false identification:

\begin{eqnarray*}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{rcccr}
P (\text{same source} \mid \text{inconclusive or elimination}) &=& \frac{11+4}{748+1425} &=& 0.0069 \\
P (\text{different source} \mid \text{inconclusive or elimination}) &=& \frac{737+1421}{748+1425} &=& 0.9931 
\end{array}
\end{eqnarray*}

\hh{useful information, but not sure where to put it - probably for evaluating the error in the process}
Only cartridge cases caught with the cartridge catching device (XXX name?) were used to assemble kits. No other pre-screening of usability for comparisons was done. 

For 3234 comparisons, FTEs evaluated how many of the known cartridge cases were usable for an evaluation: all three specimens were used in 3018 cases, two were used in 207 cases, and only one was used in nine cases.

``A very important aspect of this work that needs to be clearly understood is that the study specifically asked participants not to use their laboratory or agency peer review process."
`` Some [participants] indicated that the design of our study with all cartridges fired from the same model of firearm using the same type of ammunition would prohibit the use of a finding of elimination"

\subsection{Keisler Study \cite{keisler}}

Each test kit consisted of sets of 20 pairs of cartridge cases,  12 of the pairs were from the same source and 8 pairs were from different sources.

126 participants reported results as shown in \autoref{tab:keisler-totals}.


\begin{table}[hbt]
\centering
\caption{Evaluation results from Keisler Study.}\label{tab:keisler-totals}
{\tabulinesep=1.2mm
\begin{tabu}{|c|rrr|r|}\hline
Evaluations & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination & Totals \\\hline
Same source & 1508 & 4 & 0 & 1512 \\
Different source & 0 & 203 & 805 & 1008 \\\hline
Percent & 59.8\% & 8.2\% & 31.9\% & 100.0 \\\hline
\end{tabu}
}
\end{table}

\begin{table}[hbt]
\centering
\caption{Error rate assessment for Keisler Study.}\label{tab:keisler-errors}
{\tabulinesep=1.2mm
\begin{tabu}{|l|rr|}\hline
 & Missed Identification & Missed Elimination  \\ \hline
Option 2 (FTE error) & 0.0000  & 0.0000\\
Option 3 (Process error) & 0.0026 & 0.2014\\ \hline
\end{tabu}
}
\end{table}


The resulting set of conditional probabilities is given in \autoref{tab:keisler}.

\begin{table}[hbt]
\centering
\caption{Conditional probabilities from Keisler Study.}\label{tab:keisler}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & 1.0000 & 0.0193 & 0.0000 \\
Different source & 0.0000 & 0.9807 & 1.0000 \\\hline
\end{tabu}
}
\end{table}

Again, inconclusive results are much more indicative of different source results than they should be  based on the setup of the study; we would expect same source results with a probability of 0.60 given an inconclusive result. 

``on the response sheet participants were asked if their laboratory had a policy which does not allow examiners to exclude based on individual characteristics. ... many participants answered `yes' to the laboratory policy question and answered `inconclusive' on all within-class comparisons that could have been correctly excluded."

\subsection{Duez Study \cite{Duez:2017kha}}

using virtual microscopy, each participant is asked to make eight evaluations of breech face impressions. 

CCTS1: set of three knowns, four questioned breech face impressions. All questioned breech face impressions are from the same source as the knowns. 

CCTS2: set of three knowns, four questioned breech face impressions. Two questioned breech face impressions are from the same source, two are from different sources.

Both sets were evaluated 46 times each (10 trainees, 36 fully certified examiners). CCTS1 resulted in 46 x 4 correct identifications.

The design of the experiment does not allow us to quantify all of the quantities to evaluate examiner performance unless we aggregate performance over both sets.

Combined, the probability for a  same source pair is 6/8 or 75\% and the probability for  a different source  pair is 2/8 or 25\%.

\autoref{tab:duez-totals} shows results reported by 56 participants (46 examiners, 10 trainees).

\begin{table}[hbt]
\centering
\caption{Evaluation results from Duez Study.}\label{tab:duez-totals}
{\tabulinesep=1.2mm
\begin{tabu}{|c|rrr|r|}\hline
Evaluations (Examiner+Trainee) & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination & Totals \\\hline
Same source & 276+59 & 0+1 & 0+0 & 276+60 \\
Different source & 0+2 & 12+13 & 80+5 & 92+20 \\\hline
Percent & 75.2\% & 5.8\% & 19.0\% & 100.0\\\hline
\end{tabu}
}
\end{table}

\begin{table}[hbt]
\centering
\caption{Error rate assessment for Duez Study for certified examiners/trainees.}\label{tab:duez-errors}
{\tabulinesep=1.2mm
\begin{tabu}{|l|rr|}\hline
 & Missed Identification & Missed Elimination  \\ \hline
Option 2 (FTE error) & 0.0000/0.0000  & 0.0000/0.1000\\
Option 3 (Process error) & 0.0000/0.0167 & 0.1304/0.7500\\ \hline
\end{tabu}
}
\end{table}

\autoref{tab:duez-errors} shows the two sets of error rates for the Duez study. The error rate of certified examiners in making eliminations is much lower than that of the ten trainees. This is reflected in the error rates of the overall process. The process errors are much smaller for certified firearms examiners than for trainees. However, the probability to miss eliminations is again high, lab rules contribute to this error (see discussion below).

The set of conditional probabilities are shown in \autoref{tab:duez}. 
The differences in certified examiners and trainees are not large, so we are only showing the aggregates. Certified examiners have a perfect identification rate, only evaluations of different source pairs lead to inconclusives. 

\begin{table}[hbt]
\centering
\caption{Conditional probabilities from Duez Study.}\label{tab:duez}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & 0.9941 & 0.0385 & 0.0000 \\
Different source & 0.0059 & 0.9615 & 1.0000 \\\hline
\end{tabu}
}
\end{table}
Again, the probability of same source pairs given examiners reported an inconclusive result is much lower than the expect 75\% based on the setup of the study.

The authors report that
``13\% of examiners are not permitted to eliminate on individual characteristics
(therefore, their conclusions of inconclusive are perfectly acceptable)."

\hh{It is interesting to note that 7 of the examiners did not report eliminations due to  lab rules. These rules seem to be in contradiction to community accepted AFTE rules and need to be considered during reporting and testimony evaluations. XXX Bias against defense. XXX Needless to say that we do not agree with the authors' assessment that conclusions of inconclusives are perfectly acceptable.}






\subsection{Brundage-Hamby Study \cite{Hamby:2018hu}}

The Brundage-Hamby study consists of sets of 20 known samples and 15 questioned samples. The 20 known samples are 2 bullets from each of ten consecutively manufactured barrels. The Brundage-Hamby study is a closed set study, i.e. the 15 questioned bullets are known to be fired from one of these ten barrels, and examiners are asked to identify which of the knowns a questioned bullet matches. 
The study was originally reported on by Brundage in 1998 \cite{brundage} and updates on the study with increasing number of responses have been published several times since  \cite{hamby, Hamby:2018hu}. 
The design has also been copied for a study of cartridge cases of the same firearms in \citet{fadul}.

Results reported in the 2018 paper are shown in \autoref{tab:hamby-totals}.

\begin{table}
% \begin{tabular}{lrrrr}\hline
% Test Series	& \#Examiners &	\#Examiners 	& \#Inconclusively 	& \#Incorrectly  \\
% & & Reporting Inconclusives & Identified Bullets & Identified Bullets \\ \hline
% Brundage &	67 &	1 &	1 &	0\\
% Hamby &	630 &	4 &	7 &	0\\
% Totals: &	697 &	5 &	8 &	0\\ \hline
% \end{tabular}
\centering
\caption{\label{tab:hamby-totals} Evaluations reported from the combined Brundage-Hamby studies. Note the focus on identifications. }
{\tabulinesep=1.2mm
\begin{tabu}{|c|rrr|r|}\hline
Evaluations & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination & Totals \\\hline
Same source & 10447 & 8 & 0 & 10455 \\
Different source & 0 & ? & ? & 47047.5 \\\hline
\end{tabu}
}
\end{table}

\begin{table}[hbt]
\centering
\caption{Error rate assessment for Brundage-Hamby Study.}\label{tab:hamby-errors}
{\tabulinesep=1.2mm
\begin{tabu}{|l|rr|}\hline
 & Missed Identification & Missed Elimination  \\ \hline
Option 2 (FTE error) & 0.0000  & ? \\
Option 3 (Process error) & 0.0008 & ?\\ \hline
\end{tabu}
}
\end{table}

\begin{table}[hbt]
\centering
\caption{Conditional probabilities from Brundage-Hamby Study.}\label{tab:hamby}
{\tabulinesep=1.2mm
\begin{tabu}{|c|ccc|}\hline
$P(X | Y)$ & \multicolumn{3}{c|}{$Y = $Examiner Conclusion} \\\hline
$X = $Reality & Identification & Inconclusive & Elimination \\\hline
Same source & 1.000 & ? & ? \\
Different source & 0.000 & ? & ? \\\hline
\end{tabu}
}
\end{table}
The answer sheets for Brundage-Hamby sets only asks for reports of identifications for each of the questioned bullets. While the number of same source comparison is obviously one for each questioned bullets, i.e.\ each participant made exactly fifteen same source comparisons. However, the number of different source comparisons that were made  is an open question. Technically, there are nine different source comparisons for each questioned bullet for a total of 135 different source comparisons.  
However, these evaluations are not independent of each other: once a bullet is successfully identified to a set of knowns, none of the other sets of knowns can possibly be a match. Rather than having to make a call on a match, the evaluation shifts to a confirmation of an elimination. We will assume that an examiner will need to make an average of 4.5 different source comparisons before making an identification, resulting in an average of 67.5 different source comparisons for the whole set, and an approximate 47047.5 different source comparisons, in which no identification was reported (see \autoref{app:brundage} for mathematical details).
What is still missing is any information on eliminations versus inconclusive results for different source comparisons.



\subsection{Lyons Study}
This study involved the use of 10 consecutively manufactured extractors. First, the researchers established that there were sufficient individualizing characteristics to identify cartridges ejected by the same tool, and that there were sufficient individualizing characteristics to exclude cartridges ejected by sequentially manufactured tools. Then, they assembled 20 kits with pairs of 2 cartridges from each of the 10 extractors (the knowns) and 10 or 12 unknown cartridges, such that each known corresponded to an unknown, with some replication in most of the kits. Thus, the setup of this study is similar to the Hamby studies, which had 35 bullets each; 20 knowns from 10 barrels and 15 unknowns. As with the Hamby studies, as a closed set study with multiple knowns, it is not possible to determine the total number of different source comparisons. \svp{The study results are shown in \autoref{tab:lyons-summary}, along with computed conclusion-specific and source-specific probabilities and error rates.}

It should be noted that there are some methodological problems with the Lyons study - examiners who did not initially include all 12 unknown evaluations were given the chance to correct results; in addition, not every examiner evaluated the same number of unknowns. Excluding these results might provide different estimates, but we have chosen to use the numbers as reported in the paper for this calculation.
\noindent\begin{fmpage}{\textwidth}\centering
\captionof{table}{{\large\bf The Identification of Consecutively Manufactured Extractors}}\label{tab:lyons-summary}
\emph{\citeauthor{lyons} (\citeyear{lyons})}\\
\begin{tabularx}{\textwidth}{XllX}\hline
\multirow{2}{*}{\bf Study Type} & \multicolumn{2}{c}{\bf Test Set} & \multirow{2}{*}{\bf Participants}\\
& \# Knowns & \# Unknowns & \\\hline
Closed set & 10 & 10~or~12 & 15 examiners\\
\end{tabularx}
\begin{tabularx}{\textwidth}{Y|ccc|Y}
\multicolumn{5}{c}{\bf Experiment Count Data}\\
\multicolumn{1}{c}{} & Identification & Inconclusive & \multicolumn{1}{c}{Elimination} & Source Total\\\hline
Same Source & 174\footnotemark &  1 & 3 & 178 \\
Diff Source & 3 & ? & ? & ? \\\hline
\multicolumn{1}{Y}{Concl. Total} & 177 & ? & \multicolumn{1}{c}{?} & \multicolumn{1}{Y}{?}\\\hline\hline

\multicolumn{5}{c}{}\\

\multicolumn{5}{c}{\bf Conclusion-Specific Probabilities}\\
\multicolumn{1}{c}{} & Identification & Inconclusive & \multicolumn{1}{c}{Elimination} & Source Prop.\\\hline
Same source & 0.9831 & ? & ? & ?\\
Diff source & 0.0169 & ? & ? & ?\\\hline
\multicolumn{1}{Y}{Concl. prop.} & ? & ? & \multicolumn{1}{c}{?} & \multicolumn{1}{Y}{}\\\hline\hline

\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Source-Specific Probabilities}\\
\multicolumn{1}{c}{} & Identification & Inconclusive & \multicolumn{1}{c}{Elimination} & Source Prop.\\\hline
Same source & 0.9775 & 0.0056 & 0.1685 & ?\\
Diff source & ? & ? & ? & ?\\\hline
\multicolumn{1}{Y}{Concl. prop.} & ? & ? & \multicolumn{1}{c}{?} & \multicolumn{1}{Y}{}\\\hline\hline
\end{tabularx}

\begin{tabularx}{\textwidth}{lX|cc|Y}
\multicolumn{5}{c}{}\\
\multicolumn{5}{c}{\bf Reported Error Rates}\\
Opt. & \multicolumn{1}{c}{Meaning} & Missed Identification & \multicolumn{1}{c}{Missed Elimination} & Total \\\hline
2 & FTE error & 0.0169 & ? & ?\\
3 & Process error & 0.0225 & ? & ? \\
\end{tabularx}
\end{fmpage}

\footnotetext{\citet{lyons} reports 175 correct identifications, but it is clear from the discussion that one of those same source identifications was actually an inconclusive. 12 answer sheets with 12 correct identifications, one with 10 (out of 10) correct identifications, one with 9 correct identifications and 3 errors, and one with 11 correct identifications and one inconclusive. So 12*12 + 10 + 9 + 11 = 174. }


\subsection{Bunch \& Murphy Study \cite{bunch2003comprehensive}}


\section{Discussion and Conclusions}

number of firearms very small 

positive predictive value (control group) vs. post-test rate (for an individual case)

Implications for court process - examiner states error rate, defense asks for inconclusive rate (established during controlled testing?) and elimination rate

<<output-all-figs, eval = F, include = F>>=
thisdoc <- readLines("index.Rnw")

# First, get figures that aren't in code chunks
# Haven't really tested this part - regex may need work
raw_figures <- tibble(
  line_start = which(str_detect(thisdoc, "\\begin\\{figure\\}")), 
  line_end = which(str_detect(thisdoc, "\\end\\{figure\\}")),
  subfloats = purrr::map2(line_start, line_end, 
                          ~str_extract(thisdoc[.x:.y], "\\includegraphics(?:\\[.*\\])?\\{(.*)\\}") %>%
                            str_replace("\\includegraphics(?:\\[.*\\])?\\{(.*)\\}", "\\1"))
)

# Code chunk figures
chunk_figures <- tibble(
  line_start = which(str_detect(thisdoc, "^<<.*>>=$")),
  line_end = which(str_detect(thisdoc, "^@$")),
  chunk_name = str_replace(thisdoc[line_start], "<<([0-9A-z-]*?),?((?:.*=.*){1,})>>=", "\\1")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, chunk_name, ~thisdoc[.x] %>%
                              str_extract("fig\\.subcap ?= ?.*?\\),") %>%
                              str_replace("fig\\.subcap ?= ?", "") %>%
                              str_replace(",$", "") %>%
                              parse(text=.) %>% eval %>% length() %>% 
                              seq(1, ., by = 1) %>%
                              sprintf("figures/%s-%d.pdf", .y, .))
  )

# Knitr included graphics figures
chunk_incl_figures <- tibble(
  line_start = which(str_detect(thisdoc, "^<<.*>>=$")),
  line_end = which(str_detect(thisdoc, "^@$")),
  chunk_name = str_replace(thisdoc[line_start], "<<([0-9A-z-]*?),?((?:.*=.*){1,})>>=", "\\1")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, line_end, function(.x, .y) {
      thisdoc[.x:.y] %>%
        paste(collapse = " ") %>%
        str_extract(., "include_graphics\\(c?\\(?.*?\\)?\\)") %>%
        str_replace("include_graphics\\(", "") %>%
        str_extract_all('\\"[\\.A-z0-9 /-]*?\\"') %>%
        parse(text = .) %>% eval() %>%
        str_remove_all("\\\\|\"")
    })
  )

# All figures
figs <- bind_rows(raw_figures, chunk_figures, chunk_incl_figures) %>%
  arrange(line_start) %>%
  mutate(subfloats = purrr::map(subfloats, function(x) data_frame(file = x, exists = file.exists(x)))) %>%
  tidyr::unnest() %>%
  filter(exists) %>%
  mutate(fig_num = group_indices(., line_start, line_end)) %>%
  group_by(fig_num) %>%
  mutate(rn = row_number(),
         maxrn = n(),
         fig_label = letters[rn],
         fig_label = ifelse(maxrn > 1, fig_label, ""),
         fileext = tools::file_ext(file),
         fig_label = sprintf("Figure_%d%s.%s", fig_num, fig_label, fileext))

if (!dir.exists("separate_fig_files")) dir.create("separate_fig_files")
file.copy(figs$file, file.path("separate_fig_files", figs$fig_label), overwrite = T)

@
\section{References}

\bibliography{bibfile}

\section{Supplemental Material}
\subsection{All possible Brundage-Hamby Sets}\label{app:brundage}
Brundage-Hamby sets consist of a set of 20 known bullets (two each from the ten consecutively manufactured barrels) and 15 questioned bullets.
\citet{brundage} outlines the construction of sets of questioned bullets in detail as follows: ten bullets are chosen, one from each of the ten barrels. The remaining five bullets are picked at random from the barrels, such that at most three questioned bullets are from the same barrel. 

Using this strategy, a total of 1452 different sets can be constructed. These sets have questioned bullets of three main forms, listed in \autoref{tab:sets} from the perspective of the number of barrels with one, two, or three matching questioned bullets. 

\begin{table}
\caption{\label{tab:sets}Number of all possible sets of questioned bullets for Brundage-Hamby sets.}
\centering
\begin{tabular}{|r|ccc|l|} \hline
 &  one questioned & two questioned & three questioned  & number of \\
\# barrels & bullet matches & bullets match & bullets match & possibilities\\ \hline
I & 5 & 5 & 0 & 252 = ${10 \choose 5}{5 \choose 5}$\\
II & 6 & 3 & 1 & 840\\
III & 7 & 1 & 2 & 360\\ \hline
\end{tabular}
\end{table}

\autoref{fig:histogram} shows the histogram of the total number of different source comparisons before and identification is made.

<<histogram, fig.width=8, fig.height = 4, outwidth=".5\\textwidth", warning=FALSE, message=FALSE, fig.caption="Histogram of the total number of different source comparisons before an identification is made.", fig.label="fig:histogram">>=
library(tidyverse)
sets <- read.csv("data/allpossible-hambysets.csv")
sets$ds <- rowSums(sets-1)

sets %>% 
  ggplot(aes(x = ds)) + geom_histogram(binwidth=1, colour="grey50") +
  theme_bw() +
  xlab("Number of different source comparisons in a set before an identification is made") +
  ylab("Number of sets") +
  scale_x_continuous(breaks=c(49, 54.5, 61,  67.5, 74, 80.5, 86), labels=c("49", "54.5", "61", "67.5", "74", "80.5",  "86"))
@

\end{document}

% <!--include marginal distributions of training data and compare to test data-->

<< echo=FALSE, eval=FALSE>>=
training <- read.csv("data/features-hamby.csv")
gtrain <- training %>% 
  select(profile1_id, profile2_id, study1, study2, match, ccf:sum_peaks) %>%
  gather(feature, value, ccf:sum_peaks)
# now compare to test data
@
